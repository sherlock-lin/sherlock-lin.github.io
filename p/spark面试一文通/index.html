<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="高频 说说RDD、DataFrame、DataSet三者的区别与联系 RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)\nSpark最开始只提供了RDD，RDD它是Spark中最基本的数据处理模型，代表了一个弹性、不可变、可分区、里面的元素可进行计算的集合。但它的缺点也很明显，就是无法获取到所存元素的具体结构，因此Spark引入了DataFrame，DataFrame除了记录数据还记录了数据的结构信息schema，可以把它当做数据库中的一张表来对待。相比RDD来说性能更高、API使用更加友好，门槛更低。DataFrame可以基于已有的信息做RBO层面的优化，例如谓词下推也就是filter前置、列裁剪、分区裁剪等，并且数据可以以二进制的方式存在非堆内存，摆脱GC的限制提升性能。但DataFrame只能表示一个表对象，而在业务中会有各种各样的对象，因此Spark又增加了DataSet的支持，DataSet支持任意对象并且具有DataFrame的所有优势，可以把DataFrame看做DataSet的一个特例，RDD、Dataframe、Dataset三者之间可以互相转换。\n">
<title>Spark面试一文通</title>

<link rel='canonical' href='https://sherlock-lin.github.io/p/spark%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/'>

<link rel="stylesheet" href="/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css"><meta property='og:title' content="Spark面试一文通">
<meta property='og:description' content="高频 说说RDD、DataFrame、DataSet三者的区别与联系 RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)\nSpark最开始只提供了RDD，RDD它是Spark中最基本的数据处理模型，代表了一个弹性、不可变、可分区、里面的元素可进行计算的集合。但它的缺点也很明显，就是无法获取到所存元素的具体结构，因此Spark引入了DataFrame，DataFrame除了记录数据还记录了数据的结构信息schema，可以把它当做数据库中的一张表来对待。相比RDD来说性能更高、API使用更加友好，门槛更低。DataFrame可以基于已有的信息做RBO层面的优化，例如谓词下推也就是filter前置、列裁剪、分区裁剪等，并且数据可以以二进制的方式存在非堆内存，摆脱GC的限制提升性能。但DataFrame只能表示一个表对象，而在业务中会有各种各样的对象，因此Spark又增加了DataSet的支持，DataSet支持任意对象并且具有DataFrame的所有优势，可以把DataFrame看做DataSet的一个特例，RDD、Dataframe、Dataset三者之间可以互相转换。\n">
<meta property='og:url' content='https://sherlock-lin.github.io/p/spark%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/'>
<meta property='og:site_name' content='夏洛克-林'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2025-03-12T11:53:11&#43;08:00'/><meta property='article:modified_time' content='2025-03-12T11:53:11&#43;08:00'/><meta property='og:image' content='https://sherlock-lin.github.io/p/spark%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/nevada-8338929_1280.jpg' />
<meta name="twitter:title" content="Spark面试一文通">
<meta name="twitter:description" content="高频 说说RDD、DataFrame、DataSet三者的区别与联系 RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)\nSpark最开始只提供了RDD，RDD它是Spark中最基本的数据处理模型，代表了一个弹性、不可变、可分区、里面的元素可进行计算的集合。但它的缺点也很明显，就是无法获取到所存元素的具体结构，因此Spark引入了DataFrame，DataFrame除了记录数据还记录了数据的结构信息schema，可以把它当做数据库中的一张表来对待。相比RDD来说性能更高、API使用更加友好，门槛更低。DataFrame可以基于已有的信息做RBO层面的优化，例如谓词下推也就是filter前置、列裁剪、分区裁剪等，并且数据可以以二进制的方式存在非堆内存，摆脱GC的限制提升性能。但DataFrame只能表示一个表对象，而在业务中会有各种各样的对象，因此Spark又增加了DataSet的支持，DataSet支持任意对象并且具有DataFrame的所有优势，可以把DataFrame看做DataSet的一个特例，RDD、Dataframe、Dataset三者之间可以互相转换。\n"><meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content='https://sherlock-lin.github.io/p/spark%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/nevada-8338929_1280.jpg' />
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/cs_hu10005534405353342465.jpg" width="300"
                            height="294" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">🥳</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">夏洛克-林</a></h1>
            <h2 class="site-description">欢迎来到一个读书狂魔、偏执狂的博客~</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/CaiJimmy/hugo-theme-stack'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://twitter.com'
                        target="_blank"
                        title="Twitter"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M22 4.01c-1 .49 -1.98 .689 -3 .99c-1.121 -1.265 -2.783 -1.335 -4.38 -.737s-2.643 2.06 -2.62 3.737v1c-3.245 .083 -6.135 -1.395 -8 -4c0 0 -4.182 7.433 4 11c-1.872 1.247 -3.739 2.088 -6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58 -1.04 6.522 -3.723 7.651 -7.742a13.84 13.84 0 0 0 .497 -3.753c-.002 -.249 1.51 -2.772 1.818 -4.013z" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>归档</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>搜索</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>友情链接</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#高频">高频</a>
      <ol>
        <li><a href="#说说rdddataframedataset三者的区别与联系">说说RDD、DataFrame、DataSet三者的区别与联系</a></li>
        <li><a href="#spark的stage划分">Spark的Stage划分</a></li>
        <li><a href="#什么情况下会产生spark-shuffle">什么情况下会产生Spark Shuffle</a></li>
        <li><a href="#spark中lineage的基本原理">Spark中Lineage的基本原理</a></li>
        <li><a href="#介绍一下spark-sql解析过程">介绍一下Spark SQL解析过程</a></li>
        <li><a href="#raft算法">Raft算法</a></li>
        <li><a href="#lsm">LSM</a></li>
        <li><a href="#spark-为什么比hive快">Spark 为什么比hive快</a></li>
        <li><a href="#缓存机制与checkpoint机制的区别">缓存机制与checkpoint机制的区别</a></li>
        <li><a href="#说下spark-join的分类">说下Spark join的分类</a></li>
        <li><a href="#spark处理数据的具体流程">Spark处理数据的具体流程</a></li>
        <li><a href="#spark-map-join的实现原理">Spark map join的实现原理</a></li>
        <li><a href="#spark应用程序执行流程">Spark应用程序执行流程</a></li>
        <li><a href="#介绍下spark-shuffle以及其优缺点">介绍下Spark Shuffle以及其优缺点</a></li>
        <li><a href="#为什么要spark-shuffle">为什么要Spark Shuffle</a></li>
        <li><a href="#说明yarn-cluster模式作业提交流程yarn-cluster与yarn-client的区别">说明yarn-cluster模式作业提交流程，yarn-cluster与yarn-client的区别</a></li>
        <li><a href="#spark的内存模型">Spark的内存模型</a></li>
        <li><a href="#说说spark的动态资源分配">说说Spark的动态资源分配</a></li>
        <li><a href="#简述广播变量和累加器的基本原理和用途">简述广播变量和累加器的基本原理和用途</a></li>
        <li><a href="#spark的优化怎么做spark做过哪些优化原理是什么">Spark的优化怎么做？Spark做过哪些优化？原理是什么</a></li>
        <li><a href="#谈一谈spark中的容错机制">谈一谈Spark中的容错机制</a></li>
        <li><a href="#说说spark数据倾斜">说说Spark数据倾斜</a></li>
      </ol>
    </li>
    <li><a href="#基本">基本</a>
      <ol>
        <li><a href="#在源码中是怎么判断属于shufflemap-stage或result-stage的">在源码中是怎么判断属于ShuffleMap Stage或Result Stage的</a></li>
        <li><a href="#spark-join在什么情况下会变成窄依赖">Spark join在什么情况下会变成窄依赖</a></li>
        <li><a href="#数据本地性是在哪个环节确定的">数据本地性是在哪个环节确定的</a></li>
        <li><a href="#spark的数据本地性有哪几种">Spark的数据本地性有哪几种</a></li>
        <li><a href="#说说spark提交作业参数">说说Spark提交作业参数</a></li>
        <li><a href="#spark的宽窄依赖以及spark如何划分stage如何确定每个stage中task个数">Spark的宽窄依赖，以及Spark如何划分Stage，如何确定每个Stage中Task个数</a></li>
        <li><a href="#aqe是什么">AQE是什么</a></li>
        <li><a href="#spark累加器有哪些特点">Spark累加器有哪些特点</a></li>
        <li><a href="#spark使用parquet文件存储格式能带来哪些好处">Spark使用parquet文件存储格式能带来哪些好处</a></li>
        <li><a href="#sparksql常用哪些算子">SparkSQL常用哪些算子</a></li>
        <li><a href="#spark有哪两种算子">Spark有哪两种算子</a></li>
        <li><a href="#对于spark中的数据倾斜问题你有什么好的方案">对于Spark中的数据倾斜问题你有什么好的方案</a></li>
        <li><a href="#spark技术栈有哪些组件每个组件都有什么功能适合什么应用场景">Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景</a></li>
        <li><a href="#描述以下算子的区别与联系">描述以下算子的区别与联系</a></li>
        <li><a href="#spark算子可分为两类算子区别是什么分别举例">Spark算子可分为两类算子，区别是什么，分别举例</a></li>
        <li><a href="#说说你对spark分区器的理解">说说你对Spark分区器的理解</a></li>
        <li><a href="#spark-streaming-中有哪些消费kafka数据的方式它们之间的区别是什么">Spark Streaming 中有哪些消费Kafka数据的方式，它们之间的区别是什么</a></li>
        <li><a href="#说说对spark-shuffle的理解">说说对Spark Shuffle的理解</a></li>
        <li><a href="#说说spark的优化">说说Spark的优化</a></li>
        <li><a href="#说说spark工作机制">说说Spark工作机制</a></li>
        <li><a href="#说说spark-job执行流程spark的执行流程简要描述spark写数据的流程">说说Spark Job执行流程？spark的执行流程？简要描述Spark写数据的流程？</a></li>
        <li><a href="#说说spark的运行模式简单描述wc">说说Spark的运行模式，简单描述WC</a></li>
        <li><a href="#如何理解standalone模式下spark资源分配是粗粒度的">如何理解Standalone模式下，Spark资源分配是粗粒度的？</a></li>
        <li><a href="#spark中standalone模式特点有哪些优点和缺点">Spark中standalone模式特点，有哪些优点和缺点？</a></li>
        <li><a href="#简要描述spark分布式集群搭建的步骤">简要描述Spark分布式集群搭建的步骤</a></li>
        <li><a href="#spark-streaming中如何实现精准一次消费">Spark Streaming中如何实现精准一次消费</a></li>
        <li><a href="#spark中master-实现ha有哪些方式">Spark中Master 实现HA有哪些方式</a></li>
        <li><a href="#spark-master使用zookeeper进行ha有哪些元数据保存在zookeeper">Spark master使用zookeeper进行HA，有哪些元数据保存在Zookeeper?</a></li>
        <li><a href="#spark-master-ha-主从切换过程不会影响集群已有的作业运行为什么">Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么？</a></li>
        <li><a href="#spark-master如何通过zookeeper做ha">Spark master如何通过Zookeeper做HA？</a></li>
        <li><a href="#说说spark-hashparitioner的弊端是什么">说说spark hashParitioner的弊端是什么</a></li>
        <li><a href="#spark读取数据是几个partition呢">Spark读取数据，是几个Partition呢</a></li>
        <li><a href="#rangepartitioner分区的原理">RangePartitioner分区的原理</a></li>
        <li><a href="#rangepartioner分区器特点">RangePartioner分区器特点</a></li>
        <li><a href="#介绍parition和block有什么关联关系">介绍parition和block有什么关联关系</a></li>
        <li><a href="#什么是二次排序你是如何用spark实现二次排序的互联网公司常面">什么是二次排序，你是如何用spark实现二次排序的？（互联网公司常面）</a></li>
        <li><a href="#如何使用spark解决topn问题互联网公司常面">如何使用Spark解决TopN问题？（互联网公司常面）</a></li>
        <li><a href="#如何使用spark解决分组排序问题互联网公司常面">如何使用Spark解决分组排序问题？（互联网公司常面）</a></li>
        <li><a href="#hadoop中mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子">Hadoop中，Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？</a></li>
        <li><a href="#spark-shell启动时会启动derby">Spark shell启动时会启动derby</a></li>
        <li><a href="#介绍一下你对unified-memory-management内存管理模型的理解">介绍一下你对Unified Memory Management内存管理模型的理解</a></li>
        <li><a href="#hbase预分区个数和spark过程中的reduce个数相同么">HBase预分区个数和Spark过程中的reduce个数相同么</a></li>
        <li><a href="#简要介绍下spark的内存管理">简要介绍下Spark的内存管理</a></li>
        <li><a href="#spark读取hdfs上的文件然后count有多少行的操作你可以说说过程吗那这个count是在内存中还是磁盘中计算的呢">Spark读取hdfs上的文件，然后count有多少行的操作，你可以说说过程吗。那这个count是在内存中，还是磁盘中计算的呢</a></li>
        <li><a href="#hbase-region多大会分区spark读取hbase数据是如何划分partition的">hbase region多大会分区，spark读取hbase数据是如何划分partition的</a></li>
        <li><a href="#说说blockmanager怎么管理硬盘和内存的">说说BlockManager怎么管理硬盘和内存的</a></li>
        <li><a href="#列举spark中常见的端口分别有什么功能">列举Spark中常见的端口，分别有什么功能</a></li>
        <li><a href="#集群上nodemanager和resourcemanager的数量关系">集群上nodemanager和ResourceManager的数量关系</a></li>
        <li><a href="#spark-如何防止内存溢出">Spark 如何防止内存溢出</a></li>
        <li><a href="#spark的通信方式">Spark的通信方式</a></li>
        <li><a href="#spark如何处理结构化数据spark如何处理非结构话数据">Spark如何处理结构化数据，Spark如何处理非结构话数据？</a></li>
        <li><a href="#对于spark你觉得他对于现有大数据的现状的优势和劣势在哪里">对于Spark你觉得他对于现有大数据的现状的优势和劣势在哪里</a></li>
        <li><a href="#简要描述你了解的一些数据挖掘算法与内容">简要描述你了解的一些数据挖掘算法与内容</a></li>
        <li><a href="#spark-streaming中对接的socket的缓存策略是什么">Spark Streaming中对接的socket的缓存策略是什么</a></li>
        <li><a href="#mapreduce和spark的相同和区别">Mapreduce和Spark的相同和区别</a></li>
        <li><a href="#对rdddag和task的理解">对RDD、DAG和Task的理解</a></li>
        <li><a href="#dag为什么适合spark">DAG为什么适合Spark</a></li>
        <li><a href="#介绍下spark的dag以及它的生成过程">介绍下Spark的DAG以及它的生成过程</a></li>
        <li><a href="#dagscheduler如何划分干了什么活">DAGScheduler如何划分？干了什么活</a></li>
        <li><a href="#spark的容错机制">Spark的容错机制</a></li>
        <li><a href="#rdd的容错">RDD的容错</a></li>
        <li><a href="#executor内存分配">Executor内存分配</a></li>
        <li><a href="#spark的batchsize怎么解决小文件合并问题">Spark的batchsize，怎么解决小文件合并问题</a></li>
        <li><a href="#spark参数性能调优">Spark参数(性能)调优</a></li>
        <li><a href="#介绍一下spark是怎么基于内存计算的">介绍一下Spark是怎么基于内存计算的</a></li>
        <li><a href="#说下什么是rdd对rdd的理解rdd有哪些特点说下知道的rdd算子">说下什么是RDD(对RDD的理解)，RDD有哪些特点？说下知道的RDD算子</a></li>
        <li><a href="#rdd底层原理">RDD底层原理</a></li>
        <li><a href="#rdd属性">RDD属性</a></li>
        <li><a href="#rdd的缓存级别">RDD的缓存级别</a></li>
        <li><a href="#spark广播变量的实现和原理">Spark广播变量的实现和原理</a></li>
        <li><a href="#reducebykey和groupbykey的区别和作用">reduceByKey和groupByKey的区别和作用</a></li>
        <li><a href="#reducebykey和reduce的区别">reduceByKey和reduce的区别</a></li>
        <li><a href="#使用reducebykey出现数据倾斜怎么办">使用reduceByKey出现数据倾斜怎么办</a></li>
        <li><a href="#sparksql的执行原理">SparkSQL的执行原理</a></li>
        <li><a href="#sparksql的优化">SparkSQL的优化</a></li>
        <li><a href="#说下spark-checkpoint">说下Spark checkpoint</a></li>
        <li><a href="#sparksql自定义函数怎么创建dataframe">Sparksql自定义函数？怎么创建DataFrame</a></li>
        <li><a href="#hashpartitioner和rangepartitioner的实现">HashPartitioner和RangePartitioner的实现</a></li>
        <li><a href="#dagschedulertaskschedulerschedulerbackend实现原理">DAGScheduler、TaskScheduler、SchedulerBackend实现原理</a></li>
        <li><a href="#driver怎么管理executor">Driver怎么管理executor</a></li>
        <li><a href="#spark的map和flatmap的区别">Spark的map和flatmap的区别</a></li>
        <li><a href="#map和mappartition的区别">map和mapPartition的区别</a></li>
        <li><a href="#spark的cache和persist的区别它们是transformation算子还是action算子">Spark的cache和persist的区别？它们是transformation算子还是action算子？</a></li>
        <li><a href="#sparkstreaming的工作原理">SparkStreaming的工作原理</a></li>
        <li><a href="#spark-streaming的dstream和dstreamgraph的区别">Spark Streaming的DStream和DStreamGraph的区别</a></li>
        <li><a href="#spark输出文件的个数如何合并小文件">Spark输出文件的个数，如何合并小文件</a></li>
        <li><a href="#spark的driver是怎么驱动作业流程的">Spark的driver是怎么驱动作业流程的</a></li>
        <li><a href="#sparksql的劣势">SparkSQL的劣势</a></li>
        <li><a href="#dag划分spark源码实现">DAG划分Spark源码实现</a></li>
        <li><a href="#spark-steaming的双流join的过程怎么做的">Spark Steaming的双流join的过程，怎么做的</a></li>
        <li><a href="#spark的block管理">Spark的Block管理</a></li>
        <li><a href="#spark怎么保证数据不丢失">Spark怎么保证数据不丢失</a></li>
        <li><a href="#sparksql如何使用udf">SparkSQL如何使用UDF</a></li>
        <li><a href="#sparksql读取文件内存不够使用如何处理">SparkSQL读取文件，内存不够使用，如何处理</a></li>
        <li><a href="#spark的lazy提现在哪里">Spark的lazy提现在哪里</a></li>
        <li><a href="#spark中的并行度等于什么">Spark中的并行度等于什么</a></li>
        <li><a href="#spark运行时并行度的设置">Spark运行时并行度的设置</a></li>
        <li><a href="#sparksql的数据倾斜">SparkSQL的数据倾斜</a></li>
        <li><a href="#spark的exactly-one">Spark的exactly-one</a></li>
        <li><a href="#spark的rdd和partition的联系">Spark的RDD和partition的联系</a></li>
        <li><a href="#spark3的特性">Spark3的特性</a></li>
        <li><a href="#spakr计算的灵活性提现在哪里">Spakr计算的灵活性提现在哪里</a></li>
      </ol>
    </li>
    <li><a href="#实战">实战</a>
      <ol>
        <li><a href="#spark数据倾斜问题如何定位解决方案">Spark数据倾斜问题，如何定位，解决方案</a></li>
        <li><a href="#spark提交你的jar包时所用的命令是什么">Spark提交你的jar包时所用的命令是什么</a></li>
        <li><a href="#spark-submit的时候如何引入外部jar包">spark-submit的时候如何引入外部jar包</a></li>
        <li><a href="#你如何从kafka中获取数据">你如何从Kafka中获取数据</a></li>
        <li><a href="#spark-streaming对接kafka两种整合方式的区别">Spark Streaming对接Kafka两种整合方式的区别</a></li>
        <li><a href="#如何配置spark-master的ha">如何配置Spark master的HA</a></li>
        <li><a href="#在一个不确定的数据规模的范围内进行排序可以采用以下几种方法">在一个不确定的数据规模的范围内进行排序可以采用以下几种方法</a></li>
        <li><a href="#spark如何自定义partitioner分区器">Spark如何自定义partitioner分区器</a></li>
        <li><a href="#使用shell和scala代码实现wordcount">使用shell和scala代码实现WordCount</a></li>
        <li><a href="#怎么用spark做数据清洗">怎么用Spark做数据清洗</a></li>
        <li><a href="#说说spark怎么整合hive">说说Spark怎么整合hive</a></li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="has-image main-article">
    <header class="article-header">
        <div class="article-image">
            <a href="/p/spark%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/">
                <img src="/p/spark%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/nevada-8338929_1280_hu12942515254112204139.jpg"
                        srcset="/p/spark%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/nevada-8338929_1280_hu12942515254112204139.jpg 800w, /p/spark%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/nevada-8338929_1280_hu2070711175736809333.jpg 1600w"
                        width="800" 
                        height="533" 
                        loading="lazy"
                        alt="Featured image of post Spark面试一文通" />
                
            </a>
        </div>
    

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/%E9%9D%A2%E8%AF%95%E4%B8%93%E9%A2%98/" style="background-color: #2a9d8f; color: #fff;">
                面试专题
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/spark%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/">Spark面试一文通</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025-03-12</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    阅读时长: 92 分钟
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="高频">高频
</h2><h3 id="说说rdddataframedataset三者的区别与联系">说说RDD、DataFrame、DataSet三者的区别与联系
</h3><p>RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)</p>
<p>Spark最开始只提供了RDD，RDD它是Spark中最基本的数据处理模型，代表了一个弹性、不可变、可分区、里面的元素可进行计算的集合。但它的缺点也很明显，就是无法获取到所存元素的具体结构，因此Spark引入了DataFrame，DataFrame除了记录数据还记录了数据的结构信息schema，可以把它当做数据库中的一张表来对待。相比RDD来说性能更高、API使用更加友好，门槛更低。DataFrame可以基于已有的信息做RBO层面的优化，例如谓词下推也就是filter前置、列裁剪、分区裁剪等，并且数据可以以二进制的方式存在非堆内存，摆脱GC的限制提升性能。但DataFrame只能表示一个表对象，而在业务中会有各种各样的对象，因此Spark又增加了DataSet的支持，DataSet支持任意对象并且具有DataFrame的所有优势，可以把DataFrame看做DataSet的一个特例，RDD、Dataframe、Dataset三者之间可以互相转换。</p>
<p>DataFrame 只能做到运行时类型检查, DataSet 能做到编译和运行时都有类型检查。</p>
<h3 id="spark的stage划分">Spark的Stage划分
</h3><p>Spark会将Job作业根据宽依赖划分为多个Stage，每个Stage包含一组可以并行执行的Task任务。</p>
<p>它的划分流程是这样，首先会从最终的RDD开始，根据血缘关系(Lineage)逆向遍历RDD，遇到窄依赖时就继续向上遍历，遇到宽依赖时，将当前已经遍历过的RDD划分为一个Stage，并创建一个新的Stage重复上诉过程，直到遍历完所有的RDD。</p>
<p>尽量避免使用 groupByKey 等宽依赖操作，改用 reduceByKey 等窄依赖操作。同时可以通过 Spark UI 查看作业的阶段划分和任务执行情况，以及查看 Driver 和 Executor 的日志，分析阶段划分和任务执行的性能瓶颈。</p>
<h3 id="什么情况下会产生spark-shuffle">什么情况下会产生Spark Shuffle
</h3><p>宽依赖操作会导致 Shuffle，因为数据需要重新分布到不同的分区，例如groupByKey、reduceByKey、join、cogroup、distinct、repartition、sortByKey。</p>
<p>我们应当尽量避免不必要的shuffle或者要对shuffle做些调优，例如使用广播JOIN、使用 reduceByKey 代替 groupByKey(reduceByKey 在 Map 端进行局部聚合，减少了 Shuffle 数据量)、优化分区数、使用缓存等。避免不必要的 Shuffle，是优化 Spark 作业性能的关键，通过合理设计数据处理逻辑和优化配置，可以显著减少 Shuffle 的开销。</p>
<h3 id="spark中lineage的基本原理">Spark中Lineage的基本原理
</h3><p>在Spark中，Lineage（血统）是一种记录RDD之间依赖关系的机制。每一个 RDD 都是一个不可变的分布式可重算的数据集，它记录着确定性的操作以及继承关系，任意一个 RDD 的分区出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的，它是Spark实现容错性和数据恢复的关键。</p>
<p>Lineage的基本原理如下：</p>
<ol>
<li><strong>RDD的创建</strong> ：当我们创建一个RDD时，Spark会将其初始数据划分为一系列的分区，并将这些分区的元数据保存在Lineage中。</li>
<li><strong>转换操作</strong> ：当我们对RDD进行转换操作（如map、filter、reduce等），Spark会根据转换操作的逻辑生成一个新的RDD，并将这个转换操作添加到当前RDD的Lineage中。</li>
<li><strong>宽依赖和窄依赖</strong> ：当一个RDD依赖于多个父RDD时，称为宽依赖；当一个RDD只依赖于一个父RDD时，称为窄依赖。Spark通过这种依赖关系来构建RDD之间的有向无环图（DAG）。</li>
<li><strong>容错性和数据恢复</strong> ：当某个RDD分区数据丢失或节点失败时，Spark可以根据Lineage重新计算丢失的分区。它会遍历RDD的Lineage，根据依赖关系逐级计算缺失的分区，直到达到原始数据的来源。</li>
</ol>
<p>通过使用Lineage，Spark可以实现容错性和数据恢复。当节点失败或数据丢失时，Spark可以根据Lineage重新计算丢失的数据，而无需重新计算整个RDD，从而提高了计算效率和可靠性。</p>
<p>如果一个分区出错了：对于窄依赖，则只要把丢失的父 RDD 分区重算即可，不依赖于其他分区；对于宽依赖，则父 RDD 的所有分区都需要重算，代价昂贵；所以在长“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点。</p>
<h3 id="介绍一下spark-sql解析过程">介绍一下Spark SQL解析过程
</h3><p>Spark SQL是Apache Spark中的一个模块，用于处理结构化数据。它提供了一个用于查询数据的统一编程接口，并支持SQL查询和DataFrame API。Spark SQL的解析过程包括以下几个步骤：</p>
<ol>
<li><strong>输入SQL查询或DataFrame操作</strong> ：用户可以使用SQL语句或DataFrame操作来描述他们想要执行的查询或转换操作。</li>
<li><strong>解析器（Parser）</strong> ：Spark SQL首先使用解析器将输入的SQL查询或DataFrame操作转换为逻辑计划。解析器负责将输入的查询语句或操作转换为一个抽象语法树（AST）表示。</li>
<li><strong>语义分析器（Semantic Analyzer）</strong> ：在解析完成后，Spark SQL使用语义分析器对AST进行处理。语义分析器负责验证查询语句的语法和语义的正确性，包括检查表和列的存在性、解析函数和表达式，并进行类型检查等。</li>
<li><strong>逻辑优化器（Logical Optimizer）</strong> ：在语义分析完成后，Spark SQL使用逻辑优化器对逻辑计划进行优化。逻辑优化器通过应用一系列的规则和转换来优化查询计划，以提高查询性能。例如，它可以进行谓词下推、投影消除、常量折叠等优化操作。</li>
<li><strong>物理优化器（Physical Optimizer）</strong> ：在逻辑优化完成后，Spark SQL使用物理优化器对逻辑计划进行进一步的优化。物理优化器负责将逻辑计划转换为物理执行计划，选择最优的物理算子和执行策略。它考虑了数据的分布、数据大小、可用的计算资源等因素来选择最佳的执行计划。</li>
<li><strong>代码生成器（Code Generator）</strong> ：在物理优化完成后，Spark SQL使用代码生成器将优化后的物理执行计划转换为可执行的Java字节码。代码生成器负责生成高效的执行代码，以提高查询的执行速度。</li>
<li><strong>执行器（Executor）</strong> ：最后，Spark SQL将生成的Java字节码交给执行器执行。执行器负责将查询计划分解为一系列的任务，并在集群上执行这些任务。执行器还负责处理数据的读取、计算和输出等操作，以完成整个查询过程。</li>
</ol>
<p>总结起来，Spark SQL的解析过程包括解析、语义分析、逻辑优化、物理优化、代码生成和执行等阶段。这些阶段的目标是将用户输入的查询或操作转换为高效的执行计划，并在集群上执行这些计划来处理结构化数据。</p>
<h3 id="raft算法">Raft算法
</h3><p>分布式系统通常由异步网络连接的多个节点构成，每个节点有独立的计算和存储，节点之间通过网络通信进行协作，分布式一致性是指多个节点对某一变量的取值达成一致。常见的一致性算法包括Paxos算法，Raft算法，ZAB算法等。</p>
<p>Raft算法内部采用复制状态机模型，在这个模型中会利用多台服务器构成一个集群，并通过这个集群对外提供一个相对稳定的分布式一致性保证。Raft算法在具体实现中，将分布式一致性问题分解为了Leader选举、日志同步和安全性保证三大子问题。</p>
<p>Raft里面有Leader、Follower和Candidate三种角色。集群启动时，所有节点都是Follower角色并且都有一个计时器，如果计时器超时说明当前Follower有一段时间内没有收到 Leader 的消息，那么就可以认为 Leader 已死或者不存在，那么该结点就会转变成 Candidate，意思为准备竞争成为 Leader。成为 Candidate 后结点会向所有其他结点发送请求投票的请求，其他结点在收到请求后会判断是否可以投给他并返回结果。Candidate 如果收到了半数以上的投票就可以成为 Leader，成为Leader之后会立即并在任期内定期发送一个心跳信息通知其他所有结点新的 Leader 信息，并用来重置定时器，避免其他结点再次成为 Candidate。</p>
<p>如果 Candidate 在一定时间内没有获得足够的投票，那么就会进行一轮新的选举，直到其成为 Leader,或者其他结点成为了新的 Leader，自己变成 Follower。</p>
<p>Raft算法将时间分为一个个的任期（term），每一个term的开始都是Leader选举。每一个任期以一次选举作为起点，所以当一个结点成为 Candidate 并向其他结点请求投票时，会将自己的 Term 加 1，表明新一轮的开始以及旧 Leader 的任期结束。所有结点在收到比自己更新的 Term 之后就会更新自己的 Term 并转成 Follower，而收到过时的消息则拒绝该请求。在成功选举Leader之后，Leader会在整个term内管理整个集群。如果Leader选举失败，该term就会因为没有Leader而结束。</p>
<p>在投票时候，所有服务器采用先来先得的原则，在一个任期内只可以投票给一个结点，得到超过半数的投票才可成为 Leader，从而保证了一个任期内只会有一个 Leader 产生（Election Safety）。</p>
<p>工作流程：</p>
<p>Leader选出后，就开始接收客户端的请求。Leader把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 AppendEntries RPC复制日志条目。当这条日志被复制到大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。日志由有序编号（log index）的日志条目组成。每个日志条目包含它被创建时的任期号（term），和用于状态机执行的命令。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。</p>
<h3 id="lsm">LSM
</h3><p>目前HBase,LevelDB,RocksDB、Paimon这些存储都是采用的LSM树。LSM树的核心特点是利用顺序写来提高写性能，但因为分层(此处分层是指的分为内存和文件两部分)的设计会稍微降低读性能，但是通过牺牲小部分读性能换来高性能写，使得LSM树成为非常流行的存储结构。</p>
<h3 id="spark-为什么比hive快">Spark 为什么比hive快
</h3><p>Spark 相对于 Hive 具有以下几个方面的优势：</p>
<ol>
<li><strong>DAG计算模型</strong>：Spark 使用自己的DAG执行引擎，通过有向无环图（DAG）来表示任务，优化了任务的调度和执行，避免不必要的中间结果写入磁盘，也就是数据都存在内存中，直接通过算子处理，中间结果不落盘，从而大幅减少磁盘IO开销，而Hive默认使用 MapReduce 作为执行引擎，MapReduce计算模型比较简单，每个算子都会将中间结果写入磁盘，导致大量的磁盘 I/O 操作，所以速度较慢。</li>
<li><strong>延迟计算</strong>: Spark 使用延迟计算机制，只有在遇到action操作如 collect、count时才会真正执行计算。这种机制允许 Spark 进行更多的优化，例如合并多个操作、减少数据 shuffle等并且内存分配更科学；而Hive 的查询通常是即时执行的，缺乏类似的优化机制。</li>
<li><strong>数据分区和并行处理</strong>：Spark 支持更细粒度的数据分区和并行处理，可以更好地利用集群资源。Spark 的 RDD（弹性分布式数据集）和 DataFrame/Dataset API 提供了灵活的分区和并行处理机制。而Hive 的并行处理能力依赖于 MapReduce，MapReduce 的并行度受限于 Map 和 Reduce 任务的数量，且任务调度开销较大。</li>
<li><strong>优化器</strong>：Spark 提供了 Catalyst 优化器，用于优化 SQL 查询的执行计划。Catalyst 优化器可以进行谓词下推、列剪裁、常量折叠等多种优化，显著提高查询性能。Hive 也有优化器，但其优化能力相对较弱，尤其是在处理复杂查询时，优化效果不如 Spark。</li>
<li><strong>JVM 的优化</strong> : Hive 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM，基于进程的操作。而 Spark 每次操作是基于线程的，只在启动 Executor 时候启动一次 JVM，内存的 Task 操作是线程复用的。每次启动 JVM 的时间可能就需要几秒甚至十几秒，那么当 Task 多了，mapreduce 不知道比 Spark 慢了多少。</li>
</ol>
<p>总的来说Spark 之所以比 Hive 快，主要是因为 Spark 采用了DAG计算模型或者说内存计算、延迟计算、Catalyst 优化器等先进技术，减少了磁盘 I/O 和任务调度开销，提高了数据处理效率，因此Spark非常适合像批处理、机器学习这样的场景，当然对于一些传统的批处理场景，Mapreduce仍然是一种可靠的选择。</p>
<h3 id="缓存机制与checkpoint机制的区别">缓存机制与checkpoint机制的区别
</h3><p>两者都是做RDD持久化的，RDD通过cache方法或者persist方法可以将前面的计算结果缓存，这个RDD会被缓存在计算节点的内存中，并供后面使用(但并不是立即缓存，而是在调用Action算子的时候)。缓存既不是transformation也不是action类的算子，并且缓存结束后不会产生新的RDD。</p>
<p>缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p>
<p>使用缓存的条件：（或者说什么时候进行缓存）</p>
<ol>
<li>要求的计算速度快，对效率要求高的时候</li>
<li>集群的资源要足够大，能容得下要被缓存的数据</li>
<li>被缓存的数据会多次的触发Action（多次调用Action类的算子）</li>
<li>先进行过滤，然后将缩小范围后的数据缓存到内存中</li>
</ol>
<p>在使用完数据之后，要释放缓存，否则会一直在内存中占用资源</p>
<p>RDD的缓存容错机制能保证数据丢失也能正常的运行，是因为在每一个RDD中，都存有上一个RDD的信息，当RDD丢失以后，可以追溯到元数据，再进行计算。</p>
<p>CheckPoint(本质是通过将RDD写入高可用的地方（例如 hdfs）做检查点）是为了通过lineage（血统）做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。</p>
<p>设置checkpoint的目录，可以是本地的文件夹、也可以是HDFS、S3。一般是在具有容错能力，高可靠的文件系统上设置一个检查点路径，用于保存检查点数据。</p>
<p>在设置检查点之后，该RDD之前的有依赖关系的父RDD都会被销毁，下次调用的时候直接从检查点开始计算。checkPoint和cache一样，都是通过调用一个Action类的算子才能运行。checkPoint降低运行时间的原因：第一次调用检查点的时候，会产生两个executor，两个进程分别是从hdfs读文件和计算（调用的Action类的算子），在第二次调用的时候会发现，运行的时间大大减少，是由于第二次调用算子的时候，不会再从hdfs读源文件，而是直接从hdfs读取缓存到的数据</p>
<h3 id="说下spark-join的分类">说下Spark join的分类
</h3><p>根据数据分布的方式，可以分为Shuffle Join、Broadcast Join和Bucket Join。</p>
<ul>
<li>Shuffle Join：数据需要通过网络Shuffle到不同的节点，网络传输开销大，性能差。</li>
<li>Broadcast Join：将小表广播到所有Executor节点，避免Shuffle，适用于一个大表和小表的JOIN，避免了Shuffle，性能较高。</li>
<li>Bucket Join：两个表都按照JOIN Key进行分桶，且分桶数相同，避免了Shuffle，数据可以直接在本地JOIN。但需要两张表都进行分桶，且分桶规则匹配。</li>
</ul>
<p>根据执行的方式，可以分为SortMergeJoin、Hash Join和Nested Loop Join。</p>
<ul>
<li>Sort Merge Join：对JOIN Key进行排序后合并，是Spark中最常用的JOIN方式，适合大规模数据且性能稳定。</li>
<li>Hash Join：对JOIN Key进行哈希分区后JOIN，适用于内存足够的情况也就是一张表可以完全放入内存，性能较高，适合小表JOIN大表。</li>
<li>Nested Loop Join：通过嵌套循环的方式逐条匹配数据，性能较差，通常不推荐使用，适合两个小数据集的JOIN</li>
</ul>
<p>根据JOIN类型分类，有Inner Join、Left Join、Right Join、Full Outer Join、Cross Join、Left Semi Join和Left Anti Join</p>
<ul>
<li>
<p>Inner Join：只返回两个表中匹配的记录。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">table1</span><span class="w"> </span><span class="n">INNER</span><span class="w"> </span><span class="n">JOIN</span><span class="w"> </span><span class="n">table2</span><span class="w"> </span><span class="n">ON</span><span class="w"> </span><span class="n">table1</span><span class="p">.</span><span class="na">key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">table2</span><span class="p">.</span><span class="na">key</span><span class="p">;</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>Left Join：返回左表的所有记录，以及右表中匹配的记录（不匹配的字段为 NULL）。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">table1</span><span class="w"> </span><span class="n">LEFT</span><span class="w"> </span><span class="n">JOIN</span><span class="w"> </span><span class="n">table2</span><span class="w"> </span><span class="n">ON</span><span class="w"> </span><span class="n">table1</span><span class="p">.</span><span class="na">key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">table2</span><span class="p">.</span><span class="na">key</span><span class="p">;</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>Right Join：返回右表的所有记录，以及左表中匹配的记录（不匹配的字段为 NULL）。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">table1</span><span class="w"> </span><span class="n">RIGHT</span><span class="w"> </span><span class="n">JOIN</span><span class="w"> </span><span class="n">table2</span><span class="w"> </span><span class="n">ON</span><span class="w"> </span><span class="n">table1</span><span class="p">.</span><span class="na">key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">table2</span><span class="p">.</span><span class="na">key</span><span class="p">;</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>Full Outer Join：返回两个表的所有记录，不匹配的字段为 NULL。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">table1</span><span class="w"> </span><span class="n">FULL</span><span class="w"> </span><span class="n">OUTER</span><span class="w"> </span><span class="n">JOIN</span><span class="w"> </span><span class="n">table2</span><span class="w"> </span><span class="n">ON</span><span class="w"> </span><span class="n">table1</span><span class="p">.</span><span class="na">key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">table2</span><span class="p">.</span><span class="na">key</span><span class="p">;</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>Cross Join：返回两个表的笛卡尔积。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">table1</span><span class="w"> </span><span class="n">CROSS</span><span class="w"> </span><span class="n">JOIN</span><span class="w"> </span><span class="n">table2</span><span class="p">;</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>Left Semi Join：只返回左表中与右表匹配的记录。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">table1</span><span class="w"> </span><span class="n">LEFT</span><span class="w"> </span><span class="n">SEMI</span><span class="w"> </span><span class="n">JOIN</span><span class="w"> </span><span class="n">table2</span><span class="w"> </span><span class="n">ON</span><span class="w"> </span><span class="n">table1</span><span class="p">.</span><span class="na">key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">table2</span><span class="p">.</span><span class="na">key</span><span class="p">;</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>Left Anti Join：只返回左表中与右表不匹配的记录。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-java" data-lang="java"><span class="line"><span class="cl"><span class="n">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FROM</span><span class="w"> </span><span class="n">table1</span><span class="w"> </span><span class="n">LEFT</span><span class="w"> </span><span class="n">ANTI</span><span class="w"> </span><span class="n">JOIN</span><span class="w"> </span><span class="n">table2</span><span class="w"> </span><span class="n">ON</span><span class="w"> </span><span class="n">table1</span><span class="p">.</span><span class="na">key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">table2</span><span class="p">.</span><span class="na">key</span><span class="p">;</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="spark处理数据的具体流程">Spark处理数据的具体流程
</h3><p>Spark处理数据的流程包括数据加载、任务调度、数据计算、结果输出，这个过程也涉及到资源管理以及容错等操作。</p>
<p>首先第一步是数据加载，Spark支持多种数据源像HDFS、S3、本地文件系统、Kafka、JDBC等，可以通过textFile、read、parallelize等方法来加载数据，数据加载后会被划分为多个分区，每个分区都是一个独立的数据块。</p>
<p>第二步是任务的调度，Spark 根据 RDD 的血缘关系（Lineage）生成有向无环图（DAG），DAG 描述了数据处理的逻辑流程，包括转换操作（Transformation）和行动操作（Action）。然后Spark 会将 DAG 划分为多个阶段（Stage），每个Stage包含一组可以并行执行的Task任务。Stage划分的依据是宽依赖（Shuffle Dependency）和窄依赖（Narrow Dependency）。然后Spark 将Task任务分配给 Executor 执行，任务调度由 Driver 程序负责。</p>
<p>第三步是任务计算，每个Task任务处理一个分区的数据，执行转换操作如map、filter等，任务在Executor中执行，Executor是运行在集群节点上的进程，在遇到宽依赖比如groupByKey、join的时候，Spark会进行Shuffle操作，Shuffle 操作涉及数据的跨节点传输和重新分区。如果数据需要多次使用，可以将其缓存到内存或磁盘中（如 persist、cache）。</p>
<p>最后一步就是结果输出，对于行动操作（如 collect、count），Spark 会将结果返回给 Driver 程序，对于输出操作（如 saveAsTextFile、saveAsTable），Spark 会将结果存储到外部存储系统中。</p>
<h3 id="spark-map-join的实现原理">Spark map join的实现原理
</h3><p>在 Spark 中，Map Join（也称为 Broadcast Join）是一种用于优化小表与大表 JOIN 操作的技术。它的核心思想是将小表的数据广播到所有 Executor 节点，从而避免 Shuffle 操作，显著提升 JOIN 的性能。在常规的 Shuffle Join 中，大表和小表的数据都需要通过网络传输到 Reduce 阶段进行 JOIN，这会导致大量的网络和磁盘 I/O 开销，而Map Join 通过将小表的数据广播到所有 Executor 节点，使得每个节点都可以在本地完成 JOIN 操作，从而避免 Shuffle。</p>
<p>广播Join的原理是，Spark 使用广播变量将小表的数据序列化并分发到所有 Executor 节点，Executor 节点将接收到的数据反序列化并存储在内存中，然后每个 Executor 节点在本地将大表的分区数据与广播的小表数据进行 JOIN。Join的时候Spark会对大表的分区数据进行遍历，对于每条记录，根据JOIN Key在小表中查找匹配的记录并将匹配的结果输出，最后每个Executor 节点将 JOIN 结果输出到本地或外部存储系统。</p>
<p>广播Join出发的条件是，小表的大小必须小于spark.sql.autoBroadcastJoinThreshold参数的值，默认10M，也就是小表的数据需要能够完全放入 Executor 的内存中，广播Join适用于Inner Join、Left Join 等 JOIN 类型。但Spark也支持用户手动使用broadcast函数触发。</p>
<p>广播Join的优势在于避免了Shuffle操作，减少网络和磁盘的IO开销，同时JOIN操作在本地完成，显著提升了执行效率。但它也有自己的局限性，那就是小表的数据必须能够完全放入Executor 的内存中，并且广播Join不适用于Full Outer Join 等复杂的 JOIN 类型。</p>
<h3 id="spark应用程序执行流程">Spark应用程序执行流程
</h3><p>Spark应用程序的执行过程可以分为以下几个步骤：</p>
<ol>
<li><strong>创建SparkContext</strong> ：应用程序首先需要创建一个SparkContext对象，它是与Spark集群通信的入口点。SparkContext负责与集群管理器通信，并为应用程序分配资源。</li>
<li><strong>创建RDD</strong> ：应用程序需要将数据加载到弹性分布式数据集（RDD）中。RDD是Spark的核心数据结构，它代表了分布在集群中的数据集合。RDD可以通过读取文件、从内存中的数据集创建、从其他RDD转换等方式来创建。</li>
<li><strong>转换操作</strong> ：一旦RDD被创建，应用程序可以对RDD进行一系列的转换操作，例如map、filter、reduce等。这些转换操作会生成新的RDD，而不会立即执行计算。</li>
<li><strong>行动操作</strong> ：当应用程序需要获取转换操作的结果时，需要执行行动操作。行动操作会触发Spark执行计划的生成和执行。常见的行动操作包括collect、count、save等，它们会返回计算结果给应用程序。</li>
<li><strong>作业划分</strong> ：Spark将应用程序的行动操作划分为一系列的作业（jobs），每个作业由一组相关的转换操作组成。作业划分是为了提高并行度和数据本地性，以便更好地利用集群资源。</li>
<li><strong>任务划分</strong> ：每个作业被划分为一系列的任务（tasks），每个任务处理数据的一部分。任务的划分是根据数据分区和可用资源来完成的。</li>
<li><strong>任务调度和执行</strong> ：Spark将任务分发到集群中的执行器节点上执行。任务调度器负责将任务分配给可用的执行器节点，并考虑数据本地性以提高性能。执行器节点会加载数据到内存中，并执行任务操作。</li>
<li><strong>结果返回</strong> ：一旦任务执行完成，执行器将结果返回给驱动程序。驱动程序可以将结果保存到内存、磁盘或其他外部存储中，也可以将结果返回给应用程序。</li>
<li><strong>容错和恢复</strong> ：Spark具有容错机制，可以在节点故障时自动恢复。如果某个节点失败，Spark可以重新调度任务并在其他节点上执行。</li>
</ol>
<p>总的来说，Spark应用程序的执行过程包括创建SparkContext、创建RDD、转换操作、行动操作、作业划分、任务划分、任务调度和执行、结果返回以及容错和恢复等步骤。通过这些步骤，Spark能够以高效、可靠的方式处理大规模数据和复杂计算任务。</p>
<h3 id="介绍下spark-shuffle以及其优缺点">介绍下Spark Shuffle以及其优缺点
</h3><p>在 Apache Spark 中，Shuffle 是一个关键的操作，它负责在分布式计算中重新分布数据。Shuffle 通常发生在宽依赖操作（如 groupByKey、reduceByKey、join 等）中，是 Spark 作业中最昂贵的操作之一。Shuffle 是 Spark 中数据重新分布的过程。当需要将数据按照某种规则（如 Key）重新分组或聚合时，Spark 会将数据从上游任务（Map 阶段）的输出分区重新分区并传输到下游任务（Reduce 阶段）的输入分区。</p>
<p>Shuffle 分为两个主要阶段，Shuffle Write：Map 任务将输出数据写入本地磁盘，并按照分区规则将数据分组，数据会被写入多个临时文件，每个文件对应一个 Reduce 分区。Shuffle Read：Reduce 任务从各个 Map 任务的输出文件中读取数据，并进行聚合或计算，数据读取过程可能涉及网络传输。</p>
<p>Spark 提供了两种 Shuffle 实现方式，Hash Shuffle：每个 Map 任务为每个 Reduce 任务生成一个文件，导致文件数过多，适用于小规模数据，但会产生大量小文件，影响性能。Sort Shuffle：Map 任务将数据排序后写入单个文件，并生成索引文件，减少了文件数，适用于大规模数据，是 Spark 1.2 之后的默认实现。</p>
<p>Shuffle 的优点：Shuffle 使得 Spark 能够支持复杂的宽依赖操作，如 groupByKey、reduceByKey、join 等；Shuffle 可以根据业务需求重新分布数据，满足计算需求；灵活性：Shuffle 支持自定义分区器（Partitioner），用户可以根据业务需求定义分区规则。</p>
<p>Shuffle 的缺点</p>
<p>性能开销大：磁盘 I/O：Shuffle Write 和 Shuffle Read 都涉及大量磁盘读写操作；网络传输：数据需要在节点之间传输，网络带宽可能成为瓶颈；序列化/反序列化：数据在传输前后需要序列化和反序列化，消耗 CPU 资源。</p>
<p>数据倾斜：某些分区的数据量远大于其他分区，导致任务负载不均衡。</p>
<p>小文件问题：Hash Shuffle 会产生大量小文件，影响存储和查询性能。</p>
<p>Shuffle 的优化</p>
<p>减少 Shuffle 数据量：在 Shuffle 前过滤掉不必要的数据，使用 reduceByKey 代替 groupByKey，减少数据传输量。</p>
<p>优化分区数：合理设置分区数，避免过多或过少的分区。</p>
<p>使用高效的序列化库：使用 Kryo 序列化库，减少序列化开销。</p>
<p>启用 Shuffle 压缩：通过 spark.shuffle.compress 参数启用压缩，减少网络传输量。</p>
<p>数据倾斜处理：对倾斜的 Key 添加随机前缀，分散数据，使用自定义分区器，优化数据分布。</p>
<h3 id="为什么要spark-shuffle">为什么要Spark Shuffle
</h3><p>在分布式计算中，数据通常分布在不同的节点上。要对数据进行聚合或连接操作，需要将具有相同 Key 的数据重新分布到同一个节点上进行处理。</p>
<h3 id="说明yarn-cluster模式作业提交流程yarn-cluster与yarn-client的区别">说明yarn-cluster模式作业提交流程，yarn-cluster与yarn-client的区别
</h3><p>在Spark中，YARN是一种用于集群资源管理的框架，可以将Spark应用程序提交到YARN集群上进行执行。YARN提供了两种模式供Spark应用程序提交：yarn-cluster和yarn-client。</p>
<ol>
<li>yarn-cluster模式作业提交流程：
<ul>
<li>当你在使用 <code>spark-submit</code>命令提交Spark应用程序时，指定了 <code>--deploy-mode cluster</code>参数，即使用yarn-cluster模式。</li>
<li>提交命令发送给YARN的ResourceManager，ResourceManager会为应用程序分配一个ApplicationMaster。</li>
<li>ApplicationMaster是一个专门负责管理应用程序执行的组件，它会向ResourceManager请求资源。</li>
<li>ResourceManager会为ApplicationMaster分配一定数量的资源，这些资源将用于启动Executor。</li>
<li>ApplicationMaster会与各个NodeManager通信，启动Executor，并将应用程序的代码和依赖文件分发到各个Executor所在的节点上。</li>
<li>Executor会根据指定的任务分配策略执行具体的任务，任务的执行结果会返回给ApplicationMaster。</li>
<li>ApplicationMaster会将任务的执行结果返回给Driver程序，Driver程序可以根据需要进行后续处理。</li>
</ul>
</li>
<li>yarn-cluster与yarn-client的区别：
<ul>
<li>yarn-cluster模式下，Driver程序运行在集群中的一个Executor上，并且Driver程序与ApplicationMaster是独立的进程。这意味着Driver程序的执行不会受到本地环境的限制，可以充分利用集群资源。</li>
<li>yarn-client模式下，Driver程序运行在提交Spark应用程序的本地机器上，而不是集群中的一个Executor上。Driver程序与ApplicationMaster是同一个进程。这意味着Driver程序的执行受到本地环境的限制，例如可用的内存和CPU资源。</li>
<li>在yarn-cluster模式下，提交的应用程序可以在集群中长时间运行，即使与提交应用程序的客户端断开连接，应用程序仍然可以继续执行。而在yarn-client模式下，如果与客户端断开连接，应用程序会被终止。</li>
<li>yarn-cluster模式适用于长时间运行的应用程序，例如批处理作业。yarn-client模式适用于交互式应用程序，例如Spark Shell。</li>
</ul>
</li>
</ol>
<p>Yarn Client模式(一般用于测试)</p>
<ul>
<li>Driver在任务提交的本地机器上运行</li>
<li>Driver启动后会向ResourceManager通讯申请启动ApplicationMaster</li>
<li>ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，负责向ResourceManager申请Executor内存</li>
<li>ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分配指定的NodeManager上驱动Executor进程</li>
<li>Executor进程启动后会向Driver反向注册，Executor全部注册完成之后，Driver开始执行Main函数</li>
<li>之后执行到Action算子，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行</li>
</ul>
<p>Yarn Cluster模式</p>
<ul>
<li>任务提交后，Client会先和ResourceManager通讯，申请启动ApplicationMaster</li>
<li>ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver</li>
<li>Driver启动后会向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后在合适的NodeManager上启动Executor进程</li>
<li>Executor进程启动后会向Driver反向注册，Executor全部注册完成之后，Driver开始执行Main函数</li>
<li>之后执行到Action算子，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行</li>
</ul>
<p>总结：yarn-cluster模式下，Driver程序运行在集群中的一个Executor上，与ApplicationMaster是独立的进程；yarn-client模式下，Driver程序运行在本地机器上，与ApplicationMaster是同一个进程。</p>
<h3 id="spark的内存模型">Spark的内存模型
</h3><p>Spark中的内存使用分为两部分：执行（execution）与存储（storage）。执行内存主要用于shuffles、joins、sorts和aggregations，存储内存则用于缓存或者跨节点的内部数据传输。1.6之前，对于一个Executor，内存都由以下部分构成：
  1）ExecutionMemory。这片内存区域是为了解决 shuffles,joins, sorts and aggregations 过程中为了避免频繁IO需要的buffer。 通过spark.shuffle.memoryFraction(默认 0.2) 配置。
  2）StorageMemory。这片内存区域是为了解决 block cache(就是你显示调用rdd.cache, rdd.persist等方法), 还有就是broadcasts,以及task results的存储。可以通过参数 spark.storage.memoryFraction(默认0.6)设置。
  3）OtherMemory。给系统预留的，因为程序本身运行也是需要内存的(默认为0.2)。   传统内存管理的不足：
  1）Shuffle占用内存0.2*0.8，内存分配这么少，可能会将数据spill到磁盘，频繁的磁盘IO是很大的负担，Storage内存占用0.6，主要是为了迭代处理。传统的Spark内存分配对操作人的要求非常高。（Shuffle分配内存：ShuffleMemoryManager, TaskMemoryManager, ExecutorMemoryManager）一个Task获得全部的Execution的Memory，其他Task过来就没有内存了，只能等待；
  2）默认情况下，Task在线程中可能会占满整个内存，分片数据</p>
<p><a class="link" href="https://zhuanlan.zhihu.com/p/497268882"  target="_blank" rel="noopener"
    >https://zhuanlan.zhihu.com/p/497268882</a></p>
<h3 id="说说spark的动态资源分配">说说Spark的动态资源分配
</h3><p>对于Spark应用来说，资源是影响Spark应用执行效率的一个重要因素。当一个长期运行的服务，若分配给它多个Executor，可是却没有任何任务分配给它，而此时有其他的应用却资源紧张，这就造成了很大的资源浪费和资源不合理的调度。</p>
<h3 id="简述广播变量和累加器的基本原理和用途">简述广播变量和累加器的基本原理和用途
</h3><p>通常情况下，一个传递给 RDD 操作（如map、reduceByKey）的 func 是在远程节点上执行的。函数 func 在多个节点执行过程中使用的变量，是Driver上同一个变量的多个副本。这些变量以副本的方式拷贝到每个task中，并且各task中变量的更新并不会返回 Driver</p>
<p>为了解决以上问题，Spark 提供了两种特定类型的共享变量 : 广播变量 和 累加器。<strong>广播变量主要用于高效分发较大的数据对象，累加器主要用于对信息进行聚合</strong></p>
<p>广播变量的好处，不需要每个task带上一份变量副本，而是变成每个节点的executor的一份副本。这样的话， 就可以让变量产生的副本大大减少。而且 Spark 使用高效广播算法（BT协议）分发广播变量以降低通信成本</p>
<p>累加器是 Spark 中提供的一种分布式的变量机制，在Driver端进行初始化，task中对变量进行累加操作</p>
<p>广播变量典型的使用案例是Map Side Join；累加器经典的应用场景是用来在 Spark 应用中记录某些事件/信息的数量</p>
<h3 id="spark的优化怎么做spark做过哪些优化原理是什么">Spark的优化怎么做？Spark做过哪些优化？原理是什么
</h3><p>Spark的优化可以从多个方面进行，包括数据存储和压缩、数据分区和分桶、Shuffle操作的优化、内存管理和缓存、并行度和资源配置等。以下是一些常见的Spark优化技术和原理：</p>
<ol>
<li>
<p><strong>数据存储和压缩</strong> ：使用列式存储格式（如Parquet、ORC）可以减少磁盘IO和内存占用，提高查询性能。同时，使用压缩算法（如Snappy、Gzip）可以减小数据的存储空间，减少磁盘IO和网络传输开销。</p>
</li>
<li>
<p><strong>数据分区和分桶</strong> ：合理的数据分区和分桶可以提高数据的局部性，减少Shuffle操作的数据传输量。通过将相同键的数据分配到同一个分区或桶中，可以减少数据的移动和网络传输。</p>
</li>
<li>
<p><strong>Shuffle操作的优化</strong> ：Shuffle是Spark中常见的开销较大的操作，可以通过使用合适的Shuffle操作算子、调整分区数、合理设置缓存等手段来优化Shuffle的性能。</p>
<ul>
<li>调整分区数：合理设置分区数，避免数据倾斜和资源浪费。</li>
<li>使用本地化Shuffle：尽可能将Shuffle数据放置在与计算节点相同的节点上，减少网络传输开销。</li>
<li>使用累加器和广播变量：避免将大量数据通过Shuffle传输，而是通过累加器和广播变量在节点间共享数据。</li>
</ul>
</li>
<li>
<p><strong>内存管理和缓存</strong> ：Spark使用内存来加速数据处理，合理配置Spark的内存分配和缓存策略，可以充分利用内存资源，减少磁盘IO，提高查询速度。</p>
<ul>
<li>合理设置内存分配比例，如Executor内存和Storage内存的比例。</li>
<li>使用内存序列化：将数据以序列化的方式存储在内存中，减少内存占用和GC开销。</li>
<li>合理使用缓存：将频繁使用的数据缓存到内存中，避免重复计算和IO开销。</li>
</ul>
</li>
<li>
<p><strong>并行度和资源配置</strong> ：合理设置并行度和资源配置可以充分利用集群资源，提高作业的执行效率。可以根据数据量、计算复杂度和集群规模等因素来调整并行度和资源分配。</p>
</li>
<li>
<p><strong>使用合适的算子和函数</strong> ：选择合适的Spark算子和函数，避免使用性能较差的操作，如全量数据的操作和大量的shuffle操作。</p>
</li>
<li>
<p><strong>使用缓存和持久化</strong> ：对于经常被重复使用的数据集，可以使用缓存或持久化技术将其保存在内存或磁盘中，避免重复计算，提高性能。</p>
</li>
<li>
<p><strong>使用广播变量</strong> ：对于小规模的数据集，可以将其广播到每个Executor上，减少数据传输开销。</p>
</li>
<li>
<p><strong>资源预测和动态资源分配</strong> ：通过资源预测和动态资源分配技术，根据作业的实际需求和集群的资源情况，动态调整资源的分配，提高资源的利用率。</p>
</li>
</ol>
<p>Spark做出这些优化的目的是为了提高作业的执行效率和性能，减少资源的浪费和开销。这些优化的原理主要是通过减少磁盘IO、网络传输和计算开销，提高数据的局部性和并行度，充分利用内存和缓存等方式来优化Spark的执行过程，从而提高作业的整体性能。</p>
<h3 id="谈一谈spark中的容错机制">谈一谈Spark中的容错机制
</h3><p>Spark是一个分布式计算框架，具备强大的容错机制，以确保在集群中发生故障时能够保持计算的正确性和可靠性。下面是Spark中的几个主要容错机制：</p>
<ol>
<li><strong>数据容错</strong> ：Spark通过将数据划分为一系列弹性分布式数据集（RDD）来实现数据容错。RDD是不可变的、可分区的数据集合，它可以在集群中的多个节点上进行并行计算。当节点发生故障时，Spark可以使用RDD的血统（lineage）信息重新计算丢失的数据分区，从而实现数据的容错。</li>
<li><strong>任务容错</strong> ：Spark将计算任务划分为一系列的阶段（stage），每个阶段包含一组可以并行执行的任务。每个任务都会在执行过程中生成一系列的输出数据，这些输出数据会被持久化到稳定存储中。如果任务失败，Spark可以通过重新执行该任务来实现任务的容错。</li>
<li><strong>节点容错</strong> ：Spark通过使用主从架构来实现节点容错。在集群中，有一个主节点（Driver）负责协调任务的执行和结果的收集，以及监控集群中的节点状态。如果主节点发生故障，Spark可以通过重新启动一个新的主节点来实现节点的容错。</li>
<li><strong>任务调度容错</strong> ：Spark使用任务调度器来管理任务的执行顺序和资源分配。如果某个任务在执行过程中发生错误，任务调度器可以重新调度该任务，并将其分配给其他可用的节点进行执行。</li>
<li><strong>数据丢失容错</strong> ：Spark在执行过程中会将数据缓存在内存中，以提高计算性能。为了防止数据丢失，Spark提供了可配置的数据持久化选项，可以将数据持久化到磁盘或其他外部存储系统中。这样即使发生节点故障，数据也可以从持久化存储中恢复。</li>
</ol>
<p>总的来说，Spark的容错机制主要依赖于RDD的血统信息、任务调度器、主从架构和数据持久化等技术手段，通过这些手段可以实现数据、任务、节点和数据丢失的容错处理，保证计算的正确性和可靠性。</p>
<h3 id="说说spark数据倾斜">说说Spark数据倾斜
</h3><p>Spark数据倾斜是指在数据处理过程中，某些数据分区的负载远远超过其他分区，导致计算资源不均衡，从而影响整体性能。数据倾斜可能导致任务执行时间延长，资源浪费，甚至导致任务失败。下面是一些常见的数据倾斜情况和解决方法：</p>
<ol>
<li><strong>Key数据倾斜</strong> ：当使用某个字段作为数据集的Key时，如果该字段的分布不均匀，就会导致Key数据倾斜。解决方法包括：</li>
</ol>
<ul>
<li><strong>增加分区</strong> ：通过增加数据集的分区数，可以将数据均匀地分布到更多的分区中，减少数据倾斜的可能性。</li>
<li><strong>使用哈希分区</strong> ：使用哈希函数将Key映射到不同的分区，确保数据均匀分布。</li>
<li><strong>聚合合并</strong> ：将具有相同Key的数据进行聚合操作，减少数据量，降低倾斜程度。</li>
</ul>
<ol start="2">
<li><strong>数据倾斜的Join操作</strong> ：在进行Join操作时，如果连接字段的分布不均匀，就会导致数据倾斜。解决方法包括：</li>
</ol>
<ul>
<li><strong>使用Broadcast Join</strong> ：对于小表和大表进行Join操作时，可以将小表广播到每个节点上，避免数据倾斜。</li>
<li><strong>使用随机前缀</strong> ：对于连接字段分布不均匀的情况，可以在连接字段上添加随机前缀，使连接字段的值更均匀分布。</li>
</ul>
<ol start="3">
<li><strong>数据倾斜的聚合操作</strong> ：在进行聚合操作时，如果某些分组的数据量远远超过其他分组，就会导致数据倾斜。解决方法包括：</li>
</ol>
<ul>
<li><strong>使用预聚合</strong> ：对于可能导致数据倾斜的字段进行预聚合操作，将数据量较大的分组合并为一个分组，减少数据倾斜。</li>
<li><strong>使用多级聚合</strong> ：将聚合操作拆分为多个阶段，逐步聚合，减少单个阶段的数据量。</li>
</ul>
<ol start="4">
<li><strong>数据倾斜的排序操作</strong> ：在进行排序操作时，如果数据分布不均匀，就会导致数据倾斜。解决方法包括：</li>
</ol>
<ul>
<li><strong>使用随机前缀</strong> ：对于排序字段分布不均匀的情况，可以在排序字段上添加随机前缀，使排序字段的值更均匀分布。</li>
<li><strong>使用二次排序</strong> ：对于多个字段进行排序时，可以使用二次排序算法，先按照一个字段排序，再按照另一个字段排序，减少数据倾斜。</li>
</ul>
<p>除了上述方法，还可以通过动态资源分配、任务重试、数据重分区等方式来应对数据倾斜问题。数据倾斜是Spark中常见的性能瓶颈之一，需要根据具体情况选择合适的解决方法。</p>
<h2 id="基本">基本
</h2><h3 id="在源码中是怎么判断属于shufflemap-stage或result-stage的">在源码中是怎么判断属于ShuffleMap Stage或Result Stage的
</h3><p>在Spark 中，任务的执行过程被划分为多个Stage，Stage分为 <strong>ShuffleMapStage</strong> 和 <strong>ResultStage</strong>。这两种Stage的划分是根据任务的依赖关系和执行目标来确定的。</p>
<p>ShuffleMapStage：负责为后续的 Shuffle 操作准备数据，输出是 Shuffle 数据，供下游阶段使用，通常对应宽依赖。</p>
<p>ResultStage：负责执行最终的计算并输出结果，输出是最终的计算结果，通常对应Action操作。</p>
<p>在 Spark 源码中，阶段划分的核心逻辑位于 DAGScheduler 类中</p>
<p>Stage创建阶段：入口方法是DAGScheduler.handleJobSubmitted，在 DAGScheduler.createResultStage 和 DAGScheduler.getOrCreateShuffleMapStage 方法中创建阶段。</p>
<p>判断阶段：如果任务是最终的计算任务（如 collect、count、saveAsTextFile 等），则创建 ResultStage；如果任务是为 Shuffle 操作准备数据（如 groupByKey、reduceByKey、join 等），则创建 ShuffleMapStage；</p>
<h3 id="spark-join在什么情况下会变成窄依赖">Spark join在什么情况下会变成窄依赖
</h3><h3 id="数据本地性是在哪个环节确定的">数据本地性是在哪个环节确定的
</h3><h3 id="spark的数据本地性有哪几种">Spark的数据本地性有哪几种
</h3><p>Spark中的数据本地性有三种：
  1）PROCESS_LOCAL是指读取缓存在本地节点的数据
  2）NODE_LOCAL是指读取本地节点硬盘数据
  3）ANY是指读取非本地节点数据
  通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中。</p>
<h3 id="说说spark提交作业参数">说说Spark提交作业参数
</h3><p>提交作业时的重要参数：</p>
<ul>
<li>executor_cores
缺省值：1 in YARN mode, all the available cores on the worker in standalone and Mesos coarse-grained modes
生产环境中不宜设置为1！否则 work 进程中线程数过少，一般 2~5 为宜</li>
<li>executor_memory
缺省值：1g
该参数与executor分配的core有关，分配的core越多 executor_memory 值就应该越大；
core与memory的比值一般在 1:2 到 1:4 之间，即每个core可分配2~4G内存。如 executor_cores 为4，那么executor_memory 可以分配 8G ~ 16G；
单个Executor内存大小一般在 20G 左右（经验值），单个JVM内存太高易导致GC代价过高，或资源浪费</li>
<li>executor_cores * num_executors
表示的是能够并行执行Task的数目。不宜太小或太大！理想情况下，一般给每个core分配 2-3 个task，由此可反推 num_executors 的个数</li>
<li>driver-memory
driver 不做任何计算和存储，只是下发任务与yarn资源管理器和task交互，一般设为 1-2G 即可；</li>
</ul>
<p>增加每个executor的内存量，增加了内存量以后，对性能的提升，有三点：</p>
<ol>
<li>如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘，甚至不写入磁盘。减少了磁盘IO</li>
<li>对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存，会减少磁盘的写入操作，进而提升性能</li>
<li>task的执行，可能会创建很多对象。如果内存比较小，可能会频繁导致JVM堆内存满了，然后频繁GC，垃圾回收，minor GC和full GC。内存加大以后，带来更少的GC，垃圾回收，避免了速度变慢，性能提升；</li>
</ol>
<h3 id="spark的宽窄依赖以及spark如何划分stage如何确定每个stage中task个数">Spark的宽窄依赖，以及Spark如何划分Stage，如何确定每个Stage中Task个数
</h3><p>RDD之间的依赖关系分为窄依赖（narrow dependency）和宽依赖（Wide Depencency，也称为Shuffle Depencency）</p>
<ul>
<li><strong>窄依赖</strong> ：指父RDD的每个分区只被子RDD的一个分区所使用，子RDD分区通常对应常数个父RDD分区（O(1)，与数据规模无关）</li>
<li><strong>宽依赖</strong> ：是指父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应所有的父RDD分区（O(n),与数据规模有关）</li>
</ul>
<p>相比于宽依赖，窄依赖对优化很有利，主要基于以下几点：</p>
<ul>
<li>宽依赖往往对应着Shuffle操作，需要在运行过程中将同一个父RDD的分区传入到不同的子RDD分区中，中间可能涉及多个节点之间的数据传输；而窄依赖的每个父RDD的分区只会传入到一个子RDD分区中，通常可以在一个节点内完成转换</li>
<li>当RDD分区丢失时（某个节点故障），Spark会对数据进行重算</li>
<li>对于窄依赖，由于父RDD的一个分区只对应一个子RDD分区，这样只需要重算和子RDD分区对应的父RDD分区即可，所以这个重算对数据的利用率是100%的</li>
<li>对于宽依赖，重算的父RDD分区对应多个子RDD分区的，这样实际上父RDD中只有一部分的数据是被用于恢复这个丢失的子RDD分区的，另一部分对应子RDD的其他未丢失分区，这就造成了多余的计算；更一般的，宽依赖中子RDD分区通常来自多个父RDD分区，所有的父RDD分区都要进行重新计算</li>
</ul>
<p>Stage：根据RDD之间的依赖关系将Job划分成不同的Stage，遇到一个宽依赖则划分一个Stage。</p>
<p>Task：Stage是一个TaskSet，将Stage根据分区数划分成一个个的Task</p>
<h3 id="aqe是什么">AQE是什么
</h3><p>Adaptive Query Execution (AQE) 就像 SparkSQL的智能助手，它解决了查询计划不那么完美的问题，它通过在sparksSQL查询运行时从去从数据中去学习并通过算法模型优化来实现这一点。AQE不依赖于数据的固定信息，而是在查询展开时收集实时细节。通过这样做，AQE 可以对查询计划进行即时改进，确保其运行更高效，确保最终的结果尽可能快捷。</p>
<p><strong>动态合并Shuffle分区</strong>
当 Spark 处理大型数据集时，它通常需要通过网络重新组织或打乱数据以执行某些操作，例如ioin连接或聚合。这个shuffle过程涉及到将数据划分为多个分区，而这个分区的效率极大地影响了查询性能，而AQE 在查询运行时动态调整随机分区的数量，假设您有一个大数据集，最初，Spark 决定使用一定数量的分区进行shuffle。这个数字可能很难得到正确的值，因为如果它太低，每个分区可能太大，导致处理速度慢。另一方面，如果它太高，您会得到很多小分区，导致数据获取效率低下并增加开销。</p>
<p>如果没有 AQE，Spark 可能会决定在本地分组操作后创建五个分区。然而，AQE 很聪明。它注意到其中三个分区非常小。AQE 不会通过单独处理每个小分区来浪费资源，而是将这些小分区合并 (组合) 为一个。现在，最终的聚合只需要对三个任务执行，而不是五个，使得查询更加高效。在后台，AQE 动态调整随机分区的数量并根据需要合并它们，从而优化查询执行计划</p>
<p><strong>动态切换Join策略</strong></p>
<p>当您在 Spark 中执行join连接时，可以采用不同的方法来组合数据。最有效的方法之一称为广播哈希join (broadcast hash join)其中连接的一侧足够小以适合放到内存里，从而使Join过程更快，类似hive中的mapjoin。现在，假设您正在运行join连接操作，并且最初，Spark 计划根据其对数据大小的估计使用某种连接策略。这就是 AQE 的用武之地。它不会盲目地坚持最初的计划。相反，它会在查询运行时关注数据的实际大小，动态调整join策略。</p>
<p>如果在执行过程中，AQE 注意到join一侧的数据大小比最初估计的要小得多，它可以动态地将ioin连接策略切换为更高效的broadcasthash join。这一点至关重要，因为有时初始化时估计可能会由于各种因素而出现偏差，例如各种filter或一组复杂的操作后数据量明显变小</p>
<p>Spark 计划使用sort merge join ，刚开始以为是大表Join。然而，AQE 观察到运行时的实际大小要小得多，这使得使用广播哈希连接更加高效。因此，AQE 可以智能地切换连接策略来优化性能。现在我们开启AQE后，运行涉及join的查询时，AQE 将根据实际数据大小动态调整联接策略，确保最佳性能。</p>
<p><strong>动态优化倾斜连接</strong></p>
<p>我们知道当分区之间的数据分布不均匀时，就会出现数据倾斜。在 Spark 集群中，数据被划分为多个分区以进行并行处理。理想情况下每个分区的数据量大致相等，确保所有处理任务的工作负载均衡。然而，在数据倾斜的情况下，一个或几个分区可能包含比其他分区多得多的数据。数据倾斜对查询性能的影响: 任务相当存在数据倾斜时，与处理较少数据比，与倾斜分区相关的处理任务而要更长的时间才能完成。这种不平衡可能会导致性能瓶颈，因为某些任务会成为整个操作的瓶颈。这在join连接期间尤其成问题，其中涉及倾斜分在join连接期间尤其成问题，其中涉及倾斜分区的任务可能会滞后，从而降低整体查询性能。AQE通过其倾斜连接优化来解决这个问题AQE在运行时通过分析shuffle文件统计信息自动检测数据倾斜何时影响查询性能。一旦识别出倾斜，AQE就会动态调整方法。分割倾斜分区:AQE 不会让一些负载较重的分区成为处理瓶颈，而是智能地将这些倾斜的分区拆分为更小的子分区。现在，每个子分区包含更均衡的数据量，防止工作负载集中在少数任务上假设一个jin连接操作，其中join连接的一侧有一个分区 (我们称之为 P1) ，其中的数据明显多于其他分区:在 AQE 倾斜连接优化之前: 如果没有 AQEjoin连接操作可能会因负载过重的 P1分区而变慢，从而导致性能问题在AQE倾斜连接优化之后:AQE检测到倾斜并动态地将P1分区分割成更小、更均衡的子分区现在，工作负载的分布更加均匀，从而提高了查询性能。</p>
<p><strong>总结</strong>
Spark 3.0中的AQE标志着查询优化的重大飞跃。通过减少对静态统计的依赖和解决运行时的挑战，AQE增强了Spark对不同数据条件的适应性。三个关键特性一合并shuffle分区、切换JOIN策略和优化倾斜连接。有助于大幅提高性能，使Spark SQL在各种工作负载下更具弹性和效率。随着AQE成为标准特性，用户可以减少很多手动调优技能的输出，降低使用成本。</p>
<h3 id="spark累加器有哪些特点">Spark累加器有哪些特点
</h3><p>Spark累加器是一种用于在分布式计算中进行累加操作的特殊变量。以下是Spark累加器的几个特点：</p>
<ol>
<li><strong>分布式计算</strong> ：Spark累加器可以在分布式环境中进行并行计算。它可以在集群中的多个节点上并行更新和累加值。</li>
<li><strong>只写</strong> ：累加器的值只能增加，不能减少或修改。它们用于收集和聚合分布式计算中的统计信息。</li>
<li><strong>容错性</strong> ：Spark累加器具有容错性，即使在节点故障的情况下也可以保持正确的计算结果。如果节点失败，Spark可以自动重新计算丢失的部分。</li>
<li><strong>分布式共享变量</strong> ：累加器是一种分布式共享变量，可以在分布式任务之间共享和传递。这使得可以在不同的任务中更新和使用累加器的值。</li>
<li><strong>惰性计算</strong> ：Spark累加器是惰性计算的，只有在执行动作操作时才会真正计算累加器的值。这样可以减少不必要的计算开销。</li>
</ol>
<p>总之，Spark累加器是一种在分布式环境中进行累加操作的特殊变量，具有分布式计算、容错性和惰性计算等特点。它们在统计和聚合分布式计算中非常有用。</p>
<h3 id="spark使用parquet文件存储格式能带来哪些好处">Spark使用parquet文件存储格式能带来哪些好处
</h3><p>Spark使用Parquet文件存储格式可以带来以下几个好处：</p>
<ol>
<li><strong>高效的压缩和编码</strong> ：Parquet文件采用了列式存储的方式，将同一列的数据存储在一起，可以更好地利用压缩算法和编码方式。这样可以大大减小数据的存储空间，降低磁盘IO和网络传输的开销。</li>
<li><strong>列式存储和投影扫描</strong> ：Parquet文件的列式存储方式使得Spark可以只读取需要的列，而不必读取整个数据集。这种投影扫描的方式可以大大减少IO和内存的开销，提高查询性能。</li>
<li><strong>谓词下推和统计信息</strong> ：Parquet文件存储了每个列的统计信息，包括最小值、最大值、空值数量等。Spark可以利用这些统计信息进行谓词下推，即在读取数据时根据查询条件过滤掉不符合条件的数据，减少数据的读取量和处理量。</li>
<li><strong>列式压缩和编码</strong> ：Parquet文件支持多种压缩算法和编码方式，例如Snappy、Gzip、LZO等。Spark可以根据需求选择适合的压缩算法和编码方式，以平衡存储空间和查询性能。</li>
<li><strong>数据模式和架构演化</strong> ：Parquet文件存储了数据的模式信息，包括列名、数据类型、架构等。这使得Spark可以在读取数据时自动推断数据的模式，而不需要提前定义模式。同时，Parquet文件还支持架构演化，即在数据更新时可以增加、删除或修改列，而不需要重新创建整个数据集。</li>
</ol>
<p>综上所述，Spark使用Parquet文件存储格式可以提供高效的压缩和编码、列式存储和投影扫描、谓词下推和统计信息、列式压缩和编码，以及数据模式和架构演化等好处，从而提高数据存储和查询的性能。</p>
<h3 id="sparksql常用哪些算子">SparkSQL常用哪些算子
</h3><p>SparkSQL是Spark的一个模块，它提供了一套用于处理结构化数据的高级API。以下是SparkSQL中常用的一些算子：</p>
<ol>
<li><strong>select</strong> ：用于选择指定的列或表达式。</li>
<li><strong>filter</strong> ：用于根据指定的条件过滤数据。</li>
<li><strong>groupBy</strong> ：用于按照指定的列进行分组。</li>
<li><strong>orderBy</strong> ：用于按照指定的列对数据进行排序。</li>
<li><strong>join</strong> ：用于连接两个或多个数据集。</li>
<li><strong>union</strong> ：用于合并两个或多个数据集。</li>
<li><strong>distinct</strong> ：用于去重，返回唯一的数据。</li>
<li><strong>limit</strong> ：用于限制返回的数据量。</li>
<li><strong>agg</strong> ：用于对数据进行聚合操作，如求和、平均值等。</li>
<li><strong>window</strong> ：用于执行窗口函数操作，如滑动窗口、滚动窗口等。</li>
<li><strong>withColumn</strong> ：用于添加新的列或替换现有列。</li>
<li><strong>show</strong> ：用于显示数据集的内容。</li>
<li><strong>cache/persist</strong> ：用于将数据集缓存到内存或磁盘，以便后续快速访问。</li>
<li><strong>explain</strong> ：用于查看执行计划。</li>
<li><strong>createTempView</strong> ：用于将DataFrame注册为临时表，以便进行SQL查询。</li>
</ol>
<p>这些算子可以通过DataFrame API或SQL语句来使用。使用这些算子，可以方便地进行数据的筛选、转换、聚合等操作，以满足不同的数据处理需求。</p>
<h3 id="spark有哪两种算子">Spark有哪两种算子
</h3><p>Spark有两种主要的算子：转换算子（Transformation）和动作算子（Action）。</p>
<ol>
<li><strong>转换算子（Transformation）</strong> ：转换算子是指对RDD（弹性分布式数据集）进行转换操作的算子。转换算子不会立即执行，而是生成一个新的RDD。常见的转换算子包括 <code>map</code>、<code>filter</code>、<code>flatMap</code>、<code>reduceByKey</code>等。这些算子可以用于对RDD进行各种操作和变换，例如对每个元素进行映射、过滤、扁平化等。</li>
<li><strong>动作算子（Action）</strong> ：动作算子是指对RDD进行实际计算并返回结果的算子。动作算子会触发Spark作业的执行，并返回一个结果或将结果保存到外部存储系统中。常见的动作算子包括 <code>count</code>、<code>collect</code>、<code>reduce</code>、<code>saveAsTextFile</code>等。这些算子会触发Spark的执行过程，对RDD进行计算并返回结果。</li>
</ol>
<p>通过转换算子和动作算子的组合，可以构建复杂的数据处理流程，并在需要时触发计算并获取结果。这种延迟计算的特性使得Spark能够进行高效的数据处理和分布式计算。</p>
<h3 id="对于spark中的数据倾斜问题你有什么好的方案">对于Spark中的数据倾斜问题你有什么好的方案
</h3><p>在Spark中，数据倾斜是指在数据处理过程中，某些分区的数据量远远超过其他分区，导致任务执行时间延长或者任务失败。解决数据倾斜问题可以采取以下几种方案：</p>
<ol>
<li><strong>随机前缀</strong> ：对于可能导致数据倾斜的键进行随机前缀处理，将原本可能集中在某个分区的数据均匀分散到多个分区中。这种方法可以通过在键前添加随机字符串或者随机数来实现。</li>
<li><strong>扩容分区</strong> ：对于数据倾斜的分区，可以考虑将其拆分成多个小分区，以增加并行度。可以使用repartition或者coalesce方法来重新分区。</li>
<li><strong>聚合优化</strong> ：对于需要进行聚合操作的场景，可以先对数据进行预聚合，将数据量减少到一个可接受的范围，然后再进行全局聚合。</li>
<li><strong>广播变量</strong> ：如果数据倾斜是由于某个较小的数据集引起的，可以将该数据集使用广播变量的方式分发到所有的Executor上，避免数据重复加载。</li>
<li><strong>分桶和排序</strong> ：对于连接操作或者聚合操作中的数据倾斜问题，可以通过对数据进行分桶和排序来解决。将数据按照某个特定的列进行分桶，然后在连接或者聚合时，只对相同桶中的数据进行操作。</li>
<li><strong>动态调整资源</strong> ：如果在任务执行过程中发现某个分区的数据量过大，可以动态调整资源，增加该分区的处理能力，以加快任务的执行速度。</li>
</ol>
<p>以上是一些常见的解决数据倾斜问题的方法，具体应该根据实际情况选择合适的方案。在实际应用中，可能需要结合多种方法来解决复杂的数据倾斜问题。</p>
<h3 id="spark技术栈有哪些组件每个组件都有什么功能适合什么应用场景">Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景
</h3><p>Spark 技术栈包含以下几个核心组件：</p>
<ol>
<li><strong>Spark Core</strong> ：Spark 的基础组件，提供了任务调度、内存管理和错误恢复等功能。它还定义了 RDD（Resilient Distributed Datasets）数据结构，用于在集群上进行分布式计算。</li>
<li><strong>Spark SQL</strong> ：用于处理结构化数据的组件，支持使用 SQL 查询数据。它提供了 DataFrame 和 Dataset 两个 API，可以方便地进行数据处理和分析。适合处理大规模的结构化数据。</li>
<li><strong>Spark Streaming</strong> ：用于实时数据处理的组件，可以将实时数据流划分为小批次进行处理。它支持各种数据源，如 Kafka、Flume 和 HDFS，并提供了窗口操作和状态管理等功能。适合实时数据分析和流式处理。</li>
<li><strong>Spark MLlib</strong> ：用于机器学习的组件，提供了常见的机器学习算法和工具。它支持分类、回归、聚类和推荐等任务，并提供了特征提取、模型评估和模型调优等功能。适合大规模的机器学习任务。</li>
<li><strong>Spark GraphX</strong> ：用于图计算的组件，提供了图结构的抽象和常见的图算法。它支持图的构建、遍历和计算，并提供了图分析和图挖掘等功能。适合社交网络分析和图计算任务。</li>
<li><strong>SparkR</strong> ：用于在 R 语言中使用 Spark 的组件，提供了在 R 中进行分布式计算和数据处理的能力。它支持使用 R 语法进行数据操作，并提供了与 Spark SQL 和 MLlib 的集成。适合 R 语言用户进行大规模数据处理和分析。</li>
</ol>
<p>这些组件可以根据具体的应用场景进行组合和使用，例如：</p>
<ul>
<li>如果需要处理大规模的结构化数据，并进行复杂的数据分析和查询操作，可以使用 Spark SQL。</li>
<li>如果需要进行实时数据处理和流式计算，可以使用 Spark Streaming。</li>
<li>如果需要进行大规模的机器学习任务，可以使用 Spark MLlib。</li>
<li>如果需要进行图计算和图分析，可以使用 Spark GraphX。</li>
<li>如果需要在 R 语言中进行分布式计算和数据处理，可以使用 SparkR。</li>
</ul>
<p>总的来说，Spark 技术栈提供了一套强大的工具和组件，可以满足不同场景下的大规模数据处理和分析需求。</p>
<p>Spark是一个快速、通用的大数据处理框架，它提供了丰富的核心组件和功能，用于处理和分析大规模数据集。下面是Spark的核心组件及其功能的详细介绍：</p>
<ol>
<li><strong>Spark Core</strong> ：Spark的核心组件，提供了分布式任务调度、内存管理和错误恢复等基本功能。它还定义了RDD（弹性分布式数据集）的概念，RDD是Spark中的基本数据结构，用于表示可并行处理的数据集。</li>
<li><strong>Spark SQL</strong> ：用于处理结构化数据的模块，支持SQL查询和DataFrame API。它可以将数据从多种数据源（如Hive、Avro、Parquet等）加载到Spark中，并提供了强大的查询优化和执行功能。</li>
<li><strong>Spark Streaming</strong> ：用于实时数据流处理的模块，支持高吞吐量的数据流处理和复杂事件处理。它可以将实时数据流分成小批次，并在每个批次上应用批处理操作。</li>
<li><strong>MLlib</strong> ：Spark的机器学习库，提供了常见的机器学习算法和工具，如分类、回归、聚类、推荐等。它支持分布式训练和推理，并提供了丰富的特征提取、转换和选择功能。</li>
<li><strong>GraphX</strong> ：用于图计算的模块，支持图的创建、操作和分析。它提供了一组高级图算法和操作符，可以用于社交网络分析、推荐系统等领域。</li>
<li><strong>SparkR</strong> ：用于在R语言中使用Spark的接口。它提供了与Spark Core和Spark SQL的集成，可以在R中使用Spark的分布式计算和数据处理功能。</li>
</ol>
<p>除了以上核心组件，Spark还提供了一些其他功能和工具，如：</p>
<ul>
<li>Spark Streaming可以与Kafka、Flume等数据源集成，实现实时数据流处理。</li>
<li>Spark可以与Hadoop、Hive、HBase等大数据生态系统中的其他工具和框架无缝集成。</li>
<li>Spark提供了交互式Shell（spark-shell）和Web界面（Spark UI）等开发和监控工具。</li>
<li>Spark支持在本地模式、独立模式和集群模式下运行，可以根据需求进行灵活部署和扩展。</li>
</ul>
<h3 id="描述以下算子的区别与联系">描述以下算子的区别与联系
</h3><p><strong>一、groupByKey、reduceByKey、aggreageByKey</strong></p>
<p><strong>reduceByKey</strong> ：可以在每个分区移动数据之前将输出数据与一个共用的 <code>key</code>结合，适用于大数据集计算</p>
<p><strong>groupByKey</strong> ：所有的键值对(key-value pair) 都会被移动,在网络上传输这些数据非常没必要，因此避免使用 groupByKey</p>
<p><strong>aggreageByKey</strong> ：</p>
<p><strong>二、cache、presist</strong></p>
<p>都用于将RDD进行缓存</p>
<p><strong>cache</strong> ：只有一个默认的缓存级别MEMORY_ONLY</p>
<p><strong>presist</strong> ：persist有一个 StorageLevel 类型的参数，总共有12种缓存级别</p>
<p><strong>三、repartition、coalesce</strong></p>
<p>都是RDD的分区进行重新划分</p>
<p><strong>repartition</strong> ：repartition只是coalesce接口中shuffle为true的实现</p>
<p><strong>coalesce</strong> ：如果shuff为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDD的分区数变多的</p>
<p><strong>四、map、flatMap</strong></p>
<p><strong>map</strong> ：将函数用于RDD中的每个元素，将返回值构成新的RDD</p>
<p><strong>flatMap</strong> ：将函数应用于RDD中的每个元素，将返回的迭代器的所有内容构成新的RDD</p>
<h3 id="spark算子可分为两类算子区别是什么分别举例">Spark算子可分为两类算子，区别是什么，分别举例
</h3><p>Spark的算子可以分为两类：Transformation、Action</p>
<ul>
<li><strong>Transformation</strong> ：从现有的数据集创建一个新的数据集，返回一个新的 RDD 操作。Transformation都是惰性的，它们并不会立刻执行，只是记住了这些应用到 RDD 上的转换动作</li>
<li><strong>Action</strong> ：触发在 RDD 上的计算，这些计算可以是向应用程序返回结果，也可以是向存储系统保存数据</li>
</ul>
<p>Transformation 最重要的特点：<strong>延迟执行、返回 RDD</strong></p>
<p>Action最重要的特点：<strong>触发 Job ，返回的结果一定不是 RDD</strong></p>
<ul>
<li>常见的 Transformation 包括：<strong>map、mapVaules、filter、flatMap、mapPartitions、uoin、join、distinct、xxxByKey</strong></li>
<li>常见的 Action 包括：<strong>count、collect、collectAsMap、first、reduce、fold、aggregate、saveAsTextFile</strong></li>
</ul>
<p>有Shuffle的 Transformation 包括：</p>
<ul>
<li>一堆的 xxxByKey（sortBykey、groupByKey、reduceByKey、foldByKey、aggreageByKey、combineByKey）。备注：不包括countByKey</li>
<li>join相关（join、leftOuterJoin、rightOuterJoin、fullOuterJoin、cogroup）</li>
<li>distinct、intersection、subtract、partionBy、repartition</li>
</ul>
<h3 id="说说你对spark分区器的理解">说说你对Spark分区器的理解
</h3><p>Spark分区器是用于将数据集划分为多个分区的组件。分区器决定了数据在集群中的分布方式，对于数据处理和计算的效率具有重要影响。</p>
<p>Spark提供了多种分区器，其中常见的有哈希分区器（HashPartitioner）和范围分区器（RangePartitioner）。</p>
<p>哈希分区器根据键的哈希值将数据均匀地分布到不同的分区中。它适用于键的分布相对均匀的情况，可以保证相同键的数据被分配到同一个分区，从而方便后续的聚合操作。</p>
<p>范围分区器根据键的大小范围将数据划分到不同的分区中。它适用于键的分布有序的情况，可以保证相邻键的数据被划分到相邻的分区，从而提高了数据的局部性，有利于后续的数据处理操作。</p>
<p>除了这两种常见的分区器，Spark还提供了自定义分区器的接口，用户可以根据自己的需求实现自己的分区策略。</p>
<p>分区器的选择对于Spark作业的性能至关重要。一个合适的分区策略可以使得数据在集群中的负载均衡，减少数据的传输和重组开销，提高作业的并行度和执行效率。因此，在使用Spark时，需要根据数据的特点和作业的需求选择合适的分区器来优化作业的性能。</p>
<h3 id="spark-streaming-中有哪些消费kafka数据的方式它们之间的区别是什么">Spark Streaming 中有哪些消费Kafka数据的方式，它们之间的区别是什么
</h3><p>在Spark Streaming中，有两种主要的方式可以消费Kafka数据：直接方式（Direct Approach）和接收器方式（Receiver Approach）。</p>
<ol>
<li>直接方式（Direct Approach）：
<ul>
<li>使用 <code>createDirectStream</code>方法从Kafka直接读取数据。</li>
<li>Spark Streaming会直接连接到Kafka的分区，每个Spark分区都会对应一个Kafka分区。</li>
<li>消费者的偏移量由应用程序自己管理，通常与外部存储（如ZooKeeper）结合使用。</li>
<li>可以实现端到端的一次性语义（Exactly-once semantics）。</li>
</ul>
</li>
<li>接收器方式（Receiver Approach）：
<ul>
<li>使用 <code>createStream</code>方法创建一个接收器（Receiver）来接收Kafka数据。</li>
<li>接收器是一个独立的线程，负责从Kafka主题中接收数据。</li>
<li>接收到的数据会存储在Spark的内存中，并由Spark Streaming进行处理。</li>
<li>消费者的偏移量由接收器自动管理，并定期保存到检查点（checkpoint）中。</li>
<li>无法实现端到端的一次性语义，可能会有数据丢失或重复。</li>
</ul>
</li>
</ol>
<p>两种方式之间的区别主要在于数据的接收和处理方式，以及语义保证的能力。</p>
<p>直接方式具有更低的延迟和更高的吞吐量，因为它直接连接到Kafka分区，避免了接收器线程的开销。同时，直接方式可以实现端到端的一次性语义，确保数据的精确处理。</p>
<p>接收器方式相对简单，适用于较低的数据吞吐量和较高的延迟容忍度。它使用接收器线程将数据存储在Spark的内存中，并由Spark Streaming进行处理。但是，由于接收器的存在，可能会引入一些额外的延迟，并且无法提供端到端的一次性语义。</p>
<p>根据具体的需求和应用场景，可以选择适合的方式来消费Kafka数据。如果对延迟和一次性语义有较高的要求，直接方式是更好的选择；如果对延迟和一次性语义要求较低，接收器方式可能更简单方便。</p>
<h3 id="说说对spark-shuffle的理解">说说对Spark Shuffle的理解
</h3><p>Spark中的Shuffle是指在数据重分区（Data Redistribution）的过程中，将数据从一个或多个输入分区重新分布到新的输出分区的操作。Shuffle是Spark中一种非常重要的操作，它在许多转换操作（如groupByKey、reduceByKey和join等）中都会被使用到。</p>
<p>Shuffle的原理可以分为三个主要步骤：Map阶段、Shuffle阶段和Reduce阶段。</p>
<p><strong>1. Map阶段</strong> ：</p>
<ul>
<li>在Map阶段，Spark将输入数据按照指定的键（key）进行分组，形成多个（key, value）对。每个（key, value）对被称为一个记录（record）。</li>
<li>每个记录被分配到对应的分区（Partition），分区的数量可以通过配置参数 <code>spark.sql.shuffle.partitions</code>进行设置，默认为200个分区。分区的数量决定了并行度和任务的负载均衡。</li>
</ul>
<p><strong>2. Shuffle阶段</strong> ：</p>
<ul>
<li>在Shuffle阶段，Spark会将每个分区的数据按照键（key）进行排序，并将相同键的记录聚合在一起，形成多个（key, list of values）对。这个过程称为排序和聚合（Sort and Aggregation）。</li>
<li>排序和聚合的目的是将相同键的记录放在同一个分区中，以便后续的Reduce阶段可以更高效地进行计算。</li>
</ul>
<p><strong>3. Reduce阶段</strong> ：</p>
<ul>
<li>在Reduce阶段，Spark将每个（key, list of values）对分发给不同的Executor节点上的Reduce任务进行处理。</li>
<li>Reduce任务会对每个键（key）对应的值列表进行迭代，并对值进行合并、计算或聚合等操作，生成最终的结果。</li>
</ul>
<p>Shuffle过程中的数据传输是通过网络进行的，因此Shuffle的性能对Spark的性能影响很大。为了提高Shuffle的性能，Spark引入了多种优化技术，包括：</p>
<ul>
<li>压缩：可以使用压缩算法对Shuffle数据进行压缩，减少网络传输的数据量。</li>
<li>聚合缓冲区（Aggregation Buffer）：在Shuffle阶段，可以使用聚合缓冲区对相同键的记录进行缓存，减少网络传输和磁盘读写的次数。</li>
<li>基于Sort的Merge：在Reduce阶段，可以使用基于Sort的Merge算法对具有相同键的记录进行合并，减少计算的次数。</li>
</ul>
<p>通过合理配置Shuffle相关的参数，使用适当的优化技术，可以提高Shuffle的性能，从而提升Spark应用程序的整体性能和可扩展性。</p>
<h3 id="说说spark的优化">说说Spark的优化
</h3><p>Spark是一个基于内存的分布式计算框架，为了提高计算性能和资源利用率，它采用了多种优化技术。下面是Spark的一些优化策略：</p>
<ol>
<li><strong>延迟执行（Lazy Evaluation）</strong> ：Spark采用了延迟执行的策略，也就是说，它在遇到转换操作时不会立即执行，而是将这些操作记录下来，直到遇到行动操作时才执行。这样可以将多个转换操作合并为一个任务，减少数据的读写和中间结果的存储。</li>
<li><strong>数据分区（Data Partitioning）</strong> ：Spark将数据划分为多个分区，每个分区可以在集群中的不同节点上并行处理。通过合理的数据分区策略，可以减少数据的传输和网络开销，提高计算效率。</li>
<li><strong>内存管理（Memory Management）</strong> ：Spark使用了内存管理技术来加速数据的处理。它将数据存储在内存中，减少了磁盘IO的开销。同时，Spark还使用了内存缓存和数据序列化等技术来提高内存利用率和数据传输效率。</li>
<li><strong>任务调度（Task Scheduling）</strong> ：Spark使用了任务调度器来将任务分配给集群中的不同节点进行执行。它可以根据节点的负载情况和数据位置等因素来决定任务的调度顺序，以提高资源利用率和数据本地性。</li>
<li><strong>数据本地性优化（Data Locality Optimization）</strong> ：Spark会尽量将任务调度到与数据所在位置相近的节点上执行，以减少数据的网络传输。它可以通过数据划分和任务调度策略来实现数据本地性的优化。</li>
<li><strong>广播变量（Broadcast Variable）</strong> ：当需要在集群中的所有节点上共享一个较小的只读变量时，Spark可以使用广播变量来减少数据的传输和复制开销。广播变量会将变量的副本缓存在每个节点上，以便任务可以直接访问，而不需要从驱动程序传输。</li>
<li><strong>部分聚合（Partial Aggregation）</strong> ：在进行聚合操作时，Spark可以在每个分区上进行部分聚合，然后再将结果合并起来。这样可以减少数据的传输和聚合的计算量，提高聚合操作的性能。</li>
<li><strong>动态资源分配（Dynamic Resource Allocation）</strong> ：Spark可以根据任务的需求动态分配集群资源。它可以根据任务的执行情况来调整资源的分配，以提高资源的利用率和整体性能。</li>
</ol>
<p>这些优化策略使得Spark能够高效地处理大规模数据，并提供快速的计算和查询能力。同时，Spark还提供了丰富的配置选项和调优参数，可以根据应用的需求进行定制化的优化。</p>
<h3 id="说说spark工作机制">说说Spark工作机制
</h3><p>Spark是一个开源的分布式计算框架，它提供了高效的数据处理和分析能力。Spark的工作机制可以分为以下几个关键步骤：</p>
<ol>
<li><strong>应用程序启动</strong> ：用户编写Spark应用程序，并通过SparkContext对象与Spark集群进行通信。SparkContext负责与集群管理器通信，获取资源并分配任务。</li>
<li><strong>任务划分</strong> ：Spark将应用程序划分为一系列任务，每个任务处理数据的一部分。任务的划分是根据数据分区和用户定义的转换操作来完成的。</li>
<li><strong>数据分区</strong> ：Spark将输入数据划分为多个分区，每个分区包含数据的一个子集。分区可以是文件、HDFS块或其他数据源。</li>
<li><strong>任务调度</strong> ：Spark将任务分发到集群中的执行器节点上执行。任务调度器负责将任务分配给可用的执行器节点，并考虑数据本地性以提高性能。</li>
<li><strong>任务执行</strong> ：每个执行器节点接收到任务后，会在本地执行相应的任务操作。执行器会将数据加载到内存中，并执行用户定义的转换和操作。</li>
<li><strong>数据流动</strong> ：Spark的核心概念是弹性分布式数据集（Resilient Distributed Dataset，简称RDD）。RDD是一个可并行操作的对象集合，可以在集群中的多个节点上进行转换和操作。数据在RDD之间流动，允许在不同的转换操作之间进行缓存和重用。</li>
<li><strong>任务完成与结果返回</strong> ：一旦任务执行完成，执行器将结果返回给驱动程序。驱动程序可以将结果保存到内存、磁盘或其他外部存储中，也可以将结果返回给应用程序。</li>
<li><strong>容错和恢复</strong> ：Spark具有容错机制，可以在节点故障时自动恢复。如果某个节点失败，Spark可以重新调度任务并在其他节点上执行。</li>
</ol>
<p>总的来说，Spark的工作机制包括任务划分、任务调度、任务执行和数据流动等过程。通过这些步骤，Spark能够高效地处理大规模数据，并提供快速的分布式计算能力。</p>
<h3 id="说说spark-job执行流程spark的执行流程简要描述spark写数据的流程">说说Spark Job执行流程？spark的执行流程？简要描述Spark写数据的流程？
</h3><p>Spark Job的执行流程可以简要描述为以下几个步骤：</p>
<ol>
<li><strong>创建SparkContext</strong> ：在执行Spark应用程序之前，首先需要创建一个SparkContext对象。SparkContext是与集群通信的入口点，它负责与集群管理器进行通信，分配资源，并将任务分发给集群中的执行器。</li>
<li><strong>创建RDD</strong> ：在Spark中，数据被组织成弹性分布式数据集（RDD）。RDD可以从外部存储系统（如HDFS）中读取数据，也可以通过对已有RDD进行转换操作来创建。</li>
<li><strong>转换操作</strong> ：Spark提供了一系列的转换操作，如map、filter、reduce等。这些操作可以对RDD进行转换，生成一个新的RDD。转换操作是惰性求值的，即不会立即执行，而是在遇到一个行动操作时才会触发执行。</li>
<li><strong>行动操作</strong> ：行动操作是对RDD进行实际计算并返回结果的操作，例如collect、count、reduce等。当执行行动操作时，Spark将根据依赖关系图（DAG）将转换操作和行动操作组织成一个有向无环图（DAG），并将其划分为一系列的阶段。</li>
<li><strong>任务调度</strong> ：Spark将每个阶段划分为一组任务，并将这些任务分发给集群中的执行器进行执行。任务调度是根据数据的分区进行的，每个任务处理一个或多个分区的数据。</li>
<li><strong>任务执行</strong> ：每个执行器接收到任务后，会在其分配的资源上执行任务。执行器将数据从内存或磁盘中读取到内存中，并执行相应的转换和行动操作。执行结果可以保存在内存中或写入外部存储系统。</li>
</ol>
<p>Spark写数据的流程可以简要描述为以下几个步骤：</p>
<ol>
<li><strong>创建DataFrame或RDD</strong> ：首先，需要创建一个包含要写入的数据的DataFrame或RDD对象。DataFrame是一种结构化的数据集，RDD是弹性分布式数据集。</li>
<li><strong>指定写入选项</strong> ：根据要写入的数据的格式和目标存储系统，可以指定相应的写入选项，如文件格式、文件路径、分区方式等。</li>
<li><strong>执行写入操作</strong> ：调用DataFrame或RDD的写入方法，如 <code>write</code>或 <code>save</code>，并传入写入选项。Spark会将数据按照指定的格式和方式写入到目标存储系统中。</li>
<li><strong>等待写入完成</strong> ：写入操作是异步的，Spark会在后台执行写入任务。可以使用 <code>awaitTermination</code>等方法等待写入任务完成。</li>
</ol>
<p>需要注意的是，Spark的写入操作是幂等的，即可以多次执行相同的写入操作而不会导致数据重复写入。</p>
<h3 id="说说spark的运行模式简单描述wc">说说Spark的运行模式，简单描述WC
</h3><p>Spark有两种运行模式：本地模式和集群模式。</p>
<ol>
<li><strong>本地模式（Local Mode）</strong> ：在本地模式下，Spark运行在单个计算机上，使用单个进程进行任务的执行。这种模式适用于开发和测试阶段，可以快速验证代码的正确性。</li>
<li><strong>集群模式（Cluster Mode）</strong> ：在集群模式下，Spark将任务分发给多个计算节点进行并行处理。集群模式适用于大规模数据处理和生产环境。集群模式有两种部署方式：</li>
</ol>
<ul>
<li>Standalone模式：在Standalone模式下，Spark自带了一个集群管理器，可以通过启动和配置Master和Worker节点来管理集群资源和任务调度。</li>
<li>YARN模式：YARN是Hadoop的资源管理系统，Spark可以通过YARN来管理集群资源。Spark作为YARN的一个应用程序提交到集群中，YARN负责资源的分配和任务的调度。</li>
</ul>
<p>WC（WordCount）是一个经典的示例程序，用于统计文本中每个单词的出现次数。在Spark中，可以使用以下步骤实现WordCount：</p>
<ol>
<li><strong>创建SparkContext</strong> ：首先，需要创建一个SparkContext对象，作为与Spark集群通信的入口点。</li>
<li><strong>加载文本数据</strong> ：使用SparkContext的 <code>textFile</code>方法加载文本文件，并将其转换为RDD（弹性分布式数据集）。</li>
<li><strong>数据转换</strong> ：对RDD进行一系列转换操作，例如使用 <code>flatMap</code>方法将每行文本拆分为单词，使用 <code>map</code>方法将每个单词映射为(key, value)对，其中key是单词，value是1。</li>
<li><strong>数据聚合</strong> ：使用 <code>reduceByKey</code>方法对(key, value)对进行聚合操作，将相同key的value相加。</li>
<li><strong>结果输出</strong> ：使用 <code>collect</code>方法将聚合结果收集到Driver程序，并输出结果。</li>
</ol>
<p>这就是Spark中简单的WordCount示例程序。通过并行处理和分布式计算，Spark能够高效地处理大规模数据集。</p>
<h3 id="如何理解standalone模式下spark资源分配是粗粒度的">如何理解Standalone模式下，Spark资源分配是粗粒度的？
</h3><p>在Standalone模式下，Spark的资源分配是粗粒度的，这意味着资源的分配单位是整个应用程序，而不是每个任务或每个操作。在Standalone模式下，Spark应用程序被划分为一个或多个独立的执行器（Executors），每个执行器运行在独立的JVM进程中，并且可以分配一定数量的CPU核心和内存资源。</p>
<p>当一个Spark应用程序提交到Standalone集群时，用户需要指定应用程序需要的总体资源需求，例如CPU核心数和内存大小。然后，Spark的资源管理器将根据这些需求来分配执行器，并将应用程序的任务分配给这些执行器。每个执行器被分配的资源是固定的，直到应用程序完成或释放资源。</p>
<p>由于资源分配是以整个应用程序为单位进行的，因此在Standalone模式下，无法根据任务的实际需求进行细粒度的资源分配。这可能导致资源的浪费或不足，影响应用程序的性能。为了更好地利用资源，可以考虑使用其他资源管理器，如YARN或Mesos，它们支持更细粒度的资源分配和共享，可以根据任务的需求进行动态的资源分配。</p>
<h3 id="spark中standalone模式特点有哪些优点和缺点">Spark中standalone模式特点，有哪些优点和缺点？
</h3><p>在Spark的standalone模式中，以下是其主要特点、优点和缺点：</p>
<p><strong>特点：</strong></p>
<ol>
<li><strong>独立性</strong> ：Spark standalone模式是Spark自带的资源管理器，可以独立于其他资源管理系统运行，如Hadoop YARN或Mesos。</li>
<li><strong>简单易用</strong> ：相对于其他资源管理系统，Spark standalone模式配置和使用相对简单，适合初学者或小规模集群。</li>
<li><strong>高可用性</strong> ：Spark standalone模式支持主备模式，即可以配置一个或多个备用的主节点，以提高系统的可用性。</li>
</ol>
<p><strong>优点：</strong></p>
<ol>
<li><strong>效率高</strong> ：Spark standalone模式的资源分配是粗粒度的，以整个应用程序为单位进行分配，减少了资源管理的开销，提高了任务的执行效率。</li>
<li><strong>简化部署</strong> ：使用Spark standalone模式，无需依赖其他资源管理系统，可以快速部署和运行Spark应用程序。</li>
<li><strong>灵活性</strong> ：Spark standalone模式支持动态资源分配，可以根据应用程序的需求在运行时调整资源的分配情况。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li><strong>资源浪费</strong> ：由于资源分配是以整个应用程序为单位进行的，可能导致资源的浪费。如果应用程序中某些任务的资源需求较小，但被分配了较多的资源，会导致资源的浪费。</li>
<li><strong>资源不足</strong> ：如果应用程序的资源需求超过了集群的可用资源，可能会导致资源不足，影响任务的执行。</li>
<li><strong>缺乏细粒度调整</strong> ：由于资源分配是粗粒度的，无法对每个任务进行细粒度的资源调整，可能导致资源的利用率不高。</li>
</ol>
<p>综上所述，Spark standalone模式在简单易用和高效性方面具有优势，但在资源利用和灵活性方面存在一些限制。选择使用该模式还需根据具体应用程序的特点和需求进行考虑。</p>
<h3 id="简要描述spark分布式集群搭建的步骤">简要描述Spark分布式集群搭建的步骤
</h3><p>搭建Spark分布式集群的步骤如下：</p>
<ol>
<li><strong>准备环境</strong> ：确保每台机器上都安装了Java和Spark的依赖库，并且网络连接正常。</li>
<li><strong>配置主节点</strong> ：选择一台机器作为主节点，编辑Spark的配置文件（spark-defaults.conf和spark-env.sh），设置主节点的IP地址、端口号、内存分配等参数。</li>
<li><strong>配置工作节点</strong> ：编辑Spark的配置文件，设置工作节点的IP地址、端口号、内存分配等参数。</li>
<li><strong>配置集群管理器</strong> ：如果使用集群管理器（如Standalone、YARN或Mesos），需要配置相关的参数，如集群管理器的地址、端口号等。</li>
<li><strong>分发Spark安装包</strong> ：将Spark安装包分发到所有的机器上，确保每台机器上都可以访问到Spark的安装目录。</li>
<li><strong>启动集群</strong> ：在主节点上执行启动命令，启动集群管理器和工作节点。具体命令可以是 <code>./sbin/start-master.sh</code>启动集群管理器，<code>./sbin/start-worker.sh &lt;master-url&gt;</code>启动工作节点。</li>
<li><strong>验证集群</strong> ：通过访问Spark的Web界面（通常是 <code>http://&lt;master-ip&gt;:8080</code>）来验证集群是否正常运行。在Web界面上可以查看集群的状态、任务的执行情况等信息。</li>
<li><strong>提交任务</strong> ：使用Spark提供的命令行工具或编写Spark应用程序，将任务提交到集群中执行。具体命令可以是 <code>./bin/spark-submit --class &lt;main-class&gt; --master &lt;master-url&gt; &lt;application-jar&gt;</code>。</li>
</ol>
<p>以上是Spark分布式集群搭建的一般步骤，具体的步骤和命令可能会因为使用的集群管理器和环境的不同而有所差异。在实际搭建过程中，还需要根据具体的需求和环境进行相应的配置和调整。</p>
<h3 id="spark-streaming中如何实现精准一次消费">Spark Streaming中如何实现精准一次消费
</h3><p>在Spark Streaming中，实现精确一次消费（exactly-once semantics）是通过以下步骤来实现的：</p>
<ol>
<li><strong>使用Kafka作为数据源</strong> ：首先，将Kafka作为Spark Streaming的数据源。Kafka是一个分布式流处理平台，具有高吞吐量和容错性。</li>
<li><strong>使用Kafka的Direct方式消费数据</strong> ：Spark Streaming提供了两种方式来消费Kafka数据，即Direct方式和Receiver方式。为了实现精确一次消费，我们使用Direct方式。Direct方式通过直接从Kafka分区中读取数据来消费，而不是通过Kafka Receiver。</li>
<li><strong>将偏移量保存到ZooKeeper或Kafka</strong> ：在Direct方式中，Spark Streaming会周期性地将消费的偏移量（offset）保存到外部存储系统，比如ZooKeeper或Kafka的特殊主题。这样，在发生故障或重启时，Spark Streaming可以从上一次保存的偏移量继续消费数据，从而实现精确一次消费。</li>
<li><strong>定期提交偏移量</strong> ：Spark Streaming会定期提交已经处理的偏移量，以确保即使在处理过程中发生故障，也不会重复处理已经处理过的数据。</li>
<li><strong>容错机制</strong> ：Spark Streaming具有容错机制，即使在处理过程中发生故障，也可以恢复并继续处理数据。当发生故障时，Spark Streaming会使用保存的偏移量来重新启动，并从上一次保存的偏移量处继续消费数据，确保数据不会丢失或重复处理。</li>
</ol>
<p>通过以上步骤，Spark Streaming能够实现精确一次消费，确保每条数据只被处理一次，从而保证数据处理的准确性和一致性。</p>
<h3 id="spark中master-实现ha有哪些方式">Spark中Master 实现HA有哪些方式
</h3><p>在Spark中，实现Master的高可用性（HA）可以采用以下几种方式：</p>
<ol>
<li><strong>Standalone模式下的HA</strong> ：在Spark的Standalone模式中，可以通过启动多个Master节点来实现HA。这些Master节点将组成一个高可用性的集群，其中一个节点将被选举为活动Master，而其他节点则处于备用状态。当活动Master节点发生故障时，备用节点将自动接管并成为新的活动Master节点。</li>
<li><strong>YARN模式下的HA</strong> ：在Spark的YARN模式中，可以利用YARN的HA机制来实现Master的高可用性。YARN提供了ResourceManager的HA配置，可以使用ZooKeeper来进行故障检测和自动故障转移。当活动的ResourceManager节点发生故障时，ZooKeeper将自动选举一个备用节点作为新的活动节点。</li>
<li><strong>Mesos模式下的HA</strong> ：在Spark的Mesos模式中，可以通过启动多个Mesos Master节点来实现HA。这些Master节点将组成一个高可用性的集群，其中一个节点将被选举为活动Master，而其他节点则处于备用状态。当活动Master节点发生故障时，备用节点将自动接管并成为新的活动Master节点。</li>
</ol>
<p>需要注意的是，无论是哪种模式下的HA，都需要使用外部组件（如ZooKeeper）来进行故障检测和自动故障转移。这些组件负责监控Master节点的状态，并在需要时进行故障转移。同时，还需要配置Spark的相关参数，以便Spark能够与这些外部组件进行通信和协调。</p>
<p>总结起来，Spark中实现Master的HA可以通过在Standalone、YARN或Mesos模式下配置多个Master节点，并结合外部组件（如ZooKeeper）进行故障检测和自动故障转移来实现。</p>
<h3 id="spark-master使用zookeeper进行ha有哪些元数据保存在zookeeper">Spark master使用zookeeper进行HA，有哪些元数据保存在Zookeeper?
</h3><p>在Spark中，使用ZooKeeper进行Master的高可用性（HA）时，以下元数据将保存在ZooKeeper中：</p>
<ol>
<li><strong>Master的主节点选举信息</strong> ：ZooKeeper用于协调多个Master节点之间的主节点选举过程。每个Master节点都会在ZooKeeper上创建一个临时顺序节点，称为&quot;master_election&quot;。当一个Master节点启动时，它会尝试创建这个节点。如果创建成功，它将成为主节点，并且其他Master节点将成为备用节点。如果创建失败，它将监听前一个节点的删除事件，一旦前一个节点被删除，它将尝试再次创建节点并成为主节点。</li>
<li><strong>Master的元数据信息</strong> ：每个Master节点都会在ZooKeeper上创建一个持久节点，称为&quot;master&quot;。这个节点中保存了Master节点的元数据信息，包括Master节点的主机名、端口号、Web UI地址等。其他Spark组件（如Worker节点和Driver程序）可以通过查询这个节点获取Master节点的信息，以便与Master节点进行通信。</li>
<li><strong>Worker节点的注册信息</strong> ：每个Worker节点都会在ZooKeeper上创建一个临时顺序节点，称为&quot;workers&quot;。这个节点中保存了Worker节点的注册信息，包括Worker节点的主机名、端口号、状态等。Master节点可以通过监听这个节点的变化来实时获取Worker节点的注册和注销信息，并进行资源调度和任务分配。</li>
<li><strong>Application的注册信息</strong> ：每个运行的Spark应用程序都会在ZooKeeper上创建一个临时顺序节点，称为&quot;applications&quot;。这个节点中保存了应用程序的注册信息，包括应用程序的ID、驱动程序的主机名、端口号等。Master节点可以通过监听这个节点的变化来实时获取应用程序的注册和注销信息，并进行任务调度和资源管理。</li>
</ol>
<p>通过将这些元数据保存在ZooKeeper中，Spark的Master节点可以实现高可用性和故障转移。当一个Master节点发生故障时，其他备用节点可以通过监听ZooKeeper上的节点变化来感知到故障，并自动选举新的主节点。同时，其他Spark组件也可以通过查询ZooKeeper上的节点来获取Master节点和Worker节点的信息，从而实现与它们的通信和协调。</p>
<h3 id="spark-master-ha-主从切换过程不会影响集群已有的作业运行为什么">Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么？
</h3><p>Spark的Master节点高可用性（HA）切换过程不会影响集群中已有的作业运行，这是因为Spark的Master节点只负责作业的调度和资源管理，并不直接参与作业的执行过程。下面是详细的说明：</p>
<ol>
<li><strong>Master节点的故障转移</strong> ：当当前的Master节点发生故障时，通过使用ZooKeeper等外部的协调服务来实现Master节点的故障转移。ZooKeeper会监视Master节点的状态，并在Master节点故障时自动选举出新的Master节点。在选举过程中，已有的作业不会受到影响，因为它们已经在Worker节点上运行，并由Worker节点自主地管理。</li>
<li><strong>Worker节点的自治性</strong> ：Spark的Worker节点是独立的执行引擎，它们负责运行作业的任务。一旦作业被分配到Worker节点上，它们就会独立地运行，不依赖于Master节点的状态。即使Master节点发生故障，Worker节点仍然可以继续执行已经接收到的任务，因为它们已经获取了执行任务所需的资源和数据。</li>
<li><strong>作业的元数据和状态信息</strong> ：Spark的作业元数据和状态信息通常存储在外部的持久化存储中，如HDFS或数据库中。这些信息包括作业的执行计划、任务的状态、作业的进度等。即使Master节点发生故障，这些信息仍然可以被保留，并且可以在新的Master节点上进行恢复和管理。</li>
</ol>
<p>综上所述，Spark的Master节点的故障转移过程不会影响已有的作业运行，因为作业的执行是由Worker节点独立完成的，并且作业的元数据和状态信息可以在故障转移后进行恢复和管理。这种设计使得Spark集群具有高可用性和容错性，能够保证作业的稳定运行。</p>
<h3 id="spark-master如何通过zookeeper做ha">Spark master如何通过Zookeeper做HA？
</h3><p>Spark的Master节点可以通过ZooKeeper来实现高可用性（HA）。下面是Spark Master节点通过ZooKeeper实现HA的详细过程：</p>
<ol>
<li><strong>ZooKeeper的安装和配置</strong> ：首先，需要在Spark集群中安装和配置ZooKeeper。ZooKeeper是一个分布式协调服务，用于管理集群中的各个节点和数据。</li>
<li><strong>Master节点的注册</strong> ：当Spark的Master节点启动时，它会向ZooKeeper注册自己的信息，包括主机名、端口号和其他必要的元数据。这样，ZooKeeper就知道了Master节点的存在。</li>
<li><strong>主节点选举</strong> ：在ZooKeeper中，可以设置一个临时的有序节点（EPHEMERAL_SEQUENTIAL），用于表示Master节点的候选人。所有的Master节点都会尝试创建这个临时节点，并在节点上记录自己的ID。ZooKeeper会根据节点的创建顺序和ID来选举出一个主节点。</li>
<li><strong>主节点的选举过程</strong> ：当Master节点启动并注册到ZooKeeper后，它会检查是否已经存在一个主节点。如果不存在，则它会尝试创建一个临时节点，并成为主节点。如果已经存在主节点，则当前的Master节点会成为备用节点，并监听主节点的状态。</li>
<li><strong>备用节点的监听和故障转移</strong> ：备用节点会监听主节点的状态变化。如果主节点发生故障，ZooKeeper会自动将备用节点提升为新的主节点。此时，备用节点将接管Master节点的角色，并继续管理作业的调度和资源分配。</li>
<li><strong>元数据的保存和监听</strong> ：除了主节点的选举外，Spark还会将其他重要的元数据信息保存在ZooKeeper中，如Worker节点的注册信息、Application的注册信息等。这些信息的保存和监听可以实现Master节点的故障转移和其他组件与Master节点的通信和协调。</li>
</ol>
<p>通过以上步骤，Spark的Master节点可以利用ZooKeeper来实现高可用性。当当前的Master节点发生故障时，ZooKeeper会自动选举出新的Master节点，并且已有的作业不会受到影响。这种方式可以确保Spark集群的稳定运行和容错性。</p>
<h3 id="说说spark-hashparitioner的弊端是什么">说说spark hashParitioner的弊端是什么
</h3><p>Spark的 <code>HashPartitioner</code>是一种常用的分区器，它根据键的哈希值将数据分布到不同的分区中。虽然 <code>HashPartitioner</code>在许多情况下都能提供良好的性能，但它也存在一些弊端，具体如下：</p>
<ol>
<li><strong>数据倾斜</strong> ：<code>HashPartitioner</code>使用键的哈希值来确定分区索引，如果数据中的某些键的哈希值分布不均匀，就会导致数据倾斜问题。这意味着一些分区可能会比其他分区更大，从而导致负载不平衡和计算性能下降。</li>
<li><strong>无法保证有序性</strong> ：<code>HashPartitioner</code>将数据根据哈希值分散到不同的分区中，这意味着相同键的数据可能会分散到不同的分区中，无法保证数据的有序性。在某些场景下，需要保持数据的有序性，这就需要使用其他类型的分区器。</li>
<li><strong>分区数量固定</strong> ：<code>HashPartitioner</code>在创建时需要指定分区的数量，这意味着分区数量是固定的。在某些情况下，数据规模可能会发生变化，需要动态调整分区数量来更好地利用集群资源，但 <code>HashPartitioner</code>无法满足这种需求。</li>
</ol>
<p>总的来说，<code>HashPartitioner</code>在一些常见场景下表现良好，但在数据倾斜、有序性和动态调整分区数量等方面存在一些限制。在这些情况下，可能需要考虑使用其他类型的自定义分区器来解决这些问题。</p>
<h3 id="spark读取数据是几个partition呢">Spark读取数据，是几个Partition呢
</h3><p>在Spark中，数据的分区数量取决于数据源和集群的配置。当使用Spark读取数据时，可以通过以下几个因素来确定数据的分区数量：</p>
<ol>
<li><strong>数据源的分区情况</strong> ：不同的数据源在读取数据时会有不同的分区策略。例如，当从Hadoop分布式文件系统（HDFS）读取数据时，Spark会根据HDFS块的大小来确定分区数量。每个HDFS块通常对应一个分区。而当从数据库或其他数据源读取数据时，分区策略可能会有所不同。</li>
<li><strong>数据源的切片数</strong> ：Spark将数据源切分为多个切片，每个切片对应一个分区。切片的数量由Spark的 <code>spark.sql.files.maxPartitionBytes</code>和 <code>spark.sql.files.openCostInBytes</code>等配置参数控制。这些参数可以调整以控制分区数量。</li>
<li><strong>数据规模和集群资源</strong> ：数据的大小和集群的资源配置也会影响数据的分区数量。如果数据较小，Spark可能会使用较少的分区。而如果数据较大，Spark可能会使用更多的分区来更好地利用集群资源。</li>
</ol>
<p>需要注意的是，Spark并不保证每个数据分区都具有相同数量的数据。这取决于数据的分布情况和分区策略。有时候，数据可能会出现倾斜，导致某些分区比其他分区更大。</p>
<p>可以通过以下方式来查看数据的分区数量：</p>
<ul>
<li>对于RDD：可以使用 <code>getNumPartitions()</code>方法获取RDD的分区数量。</li>
<li>对于DataFrame或Dataset：可以使用 <code>rdd.getNumPartitions()</code>方法获取底层RDD的分区数量。</li>
</ul>
<p>需要注意的是，分区数量在读取数据时是动态确定的，并且可以根据数据源、配置和集群资源的不同而变化。因此，实际的分区数量可能会有所不同。</p>
<h3 id="rangepartitioner分区的原理">RangePartitioner分区的原理
</h3><p>RangePartitioner是Spark中的一种分区器，用于将数据按照一定的范围进行分区。它的原理是根据数据的键值范围将数据划分到不同的分区中。</p>
<p>RangePartitioner的工作流程如下：</p>
<ol>
<li>首先，RangePartitioner会对数据进行采样，以获取数据的键值范围。采样过程可以通过调用 <code>RDD.sample</code>方法来实现。</li>
<li>接下来，RangePartitioner会根据指定的分区数量，将键值范围划分为相应数量的区间。</li>
<li>然后，RangePartitioner会将数据按照键值范围划分到对应的分区中。具体的划分方式是根据数据的键值与区间的比较来确定数据所属的分区。</li>
<li>最后，RangePartitioner会返回一个分区器对象，可以将其应用到RDD或DataFrame中。在使用RDD时，可以通过调用 <code>RDD.partitionBy</code>方法，并传入RangePartitioner来进行分区；在使用DataFrame时，可以通过调用 <code>DataFrame.repartition</code>方法，并传入RangePartitioner来进行分区。</li>
</ol>
<p>RangePartitioner的优点是可以根据数据的键值范围进行分区，可以保证相同范围内的数据被划分到同一个分区中，从而提高数据的局部性。这对于一些需要按照键值范围进行聚合或排序的操作非常有用。</p>
<p>然而，RangePartitioner也存在一些限制。首先，它要求数据的键值范围是已知的，这对于一些动态生成的数据集可能不适用。其次，如果数据的分布不均匀，可能会导致某些分区的数据量过大或过小，从而影响计算性能。因此，在实际使用中，需要根据数据的特点选择合适的分区策略。</p>
<h3 id="rangepartioner分区器特点">RangePartioner分区器特点
</h3><p>RangePartitioner分区器具有以下特点：</p>
<ol>
<li><strong>基于范围划分</strong> ：RangePartitioner根据数据的键值范围将数据划分到不同的分区中。它将键值范围划分为若干个区间，并将数据按照键值与区间的比较进行划分。</li>
<li><strong>数据局部性</strong> ：RangePartitioner会将相同范围内的数据划分到同一个分区中，从而提高数据的局部性。这对于一些需要按照键值范围进行聚合或排序的操作非常有用。</li>
<li><strong>适用于有序数据</strong> ：RangePartitioner适用于有序数据的分区。它可以保证相同范围内的数据被划分到同一个分区中，从而保持数据的有序性。</li>
<li><strong>分区数量可控</strong> ：RangePartitioner可以通过指定分区数量来控制数据的分区个数。这对于优化计算性能和资源利用非常重要。</li>
<li><strong>采样获取范围</strong> ：为了确定数据的键值范围，RangePartitioner会对数据进行采样。采样过程可以通过调用 <code>RDD.sample</code>方法来实现。</li>
<li><strong>需要键值范围已知</strong> ：RangePartitioner要求数据的键值范围是已知的，这对于一些动态生成的数据集可能不适用。</li>
<li><strong>可应用于RDD和DataFrame</strong> ：RangePartitioner可以应用于RDD和DataFrame。在使用RDD时，可以通过调用 <code>RDD.partitionBy</code>方法，并传入RangePartitioner来进行分区；在使用DataFrame时，可以通过调用 <code>DataFrame.repartition</code>方法，并传入RangePartitioner来进行分区。</li>
</ol>
<p>需要注意的是，RangePartitioner也存在一些限制。如果数据的分布不均匀，可能会导致某些分区的数据量过大或过小，从而影响计算性能。因此，在使用RangePartitioner时，需要根据数据的特点选择合适的分区策略。</p>
<h3 id="介绍parition和block有什么关联关系">介绍parition和block有什么关联关系
</h3><p>在Spark中，Partition（分区）和Block（块）是两个不同的概念，但它们之间存在一定的关联关系。</p>
<p>Partition（分区）是将数据集拆分为较小、可并行处理的数据块的过程。在Spark中，数据集被划分为多个分区，每个分区包含数据的一个子集。每个分区都可以在集群中的不同节点上进行并行处理。</p>
<p>而Block（块）是Spark中数据存储和传输的基本单位。在Spark中，数据被划分为多个块，每个块的大小通常为128MB。每个块都会被存储在集群中的不同节点上，并且可以在节点之间进行传输和共享。</p>
<p>Partition和Block之间的关联关系在数据处理过程中体现出来。当Spark执行任务时，每个分区的数据会被加载到对应节点的内存中，并以块的形式进行存储。这样可以提高数据的读取和处理效率，因为每个节点只需要加载和处理自己负责的分区数据块。</p>
<p>此外，Spark还使用Partition和Block之间的关联关系来进行数据的本地性调度。Spark会尽量将任务调度到与数据所在的分区块相同的节点上执行，以减少数据传输的开销，并提高任务的执行效率。</p>
<p>总结起来，Partition和Block之间的关联关系可以简单描述为：Partition是数据集的逻辑划分，而Block是数据的物理存储和传输单位，二者配合使用可以提高数据处理的效率和性能。</p>
<h3 id="什么是二次排序你是如何用spark实现二次排序的互联网公司常面">什么是二次排序，你是如何用spark实现二次排序的？（互联网公司常面）
</h3><p>二次排序（Secondary Sorting）是指在对数据进行排序时，除了主排序键（Primary Key）外，还需要对次要排序键（Secondary Key）进行排序。在Spark中，可以使用自定义排序函数和自定义分区器来实现二次排序。</p>
<p>要实现二次排序，首先需要定义一个包含主排序键和次要排序键的元组作为数据的键。然后，可以使用 <code>sortByKey()</code>函数对键值对进行排序。在排序时，可以通过自定义的排序函数来指定主排序键和次要排序键的排序规则。</p>
<p>下面是一个使用Spark实现二次排序的示例代码：</p>
<h3 id="如何使用spark解决topn问题互联网公司常面">如何使用Spark解决TopN问题？（互联网公司常面）
</h3><p>在Spark中，解决TopN问题通常涉及以下几个步骤：</p>
<ol>
<li><strong>加载数据</strong> ：首先，你需要加载数据到Spark中。你可以使用 <code>textFile</code>方法加载文本文件，或者使用其他适合你数据格式的方法加载数据。</li>
<li><strong>数据预处理</strong> ：根据你的需求，对数据进行必要的预处理。这可能包括数据清洗、转换和过滤等操作。</li>
<li><strong>数据转换</strong> ：将数据转换为键值对的形式，其中键是你要进行TopN操作的字段，值是与该字段相关的其他数据。你可以使用 <code>map</code>或者 <code>flatMap</code>方法来实现这个步骤。</li>
<li><strong>按键分组</strong> ：使用 <code>groupByKey</code>方法将数据按键进行分组。这将把具有相同键的数据分组到一起。</li>
<li><strong>计算TopN</strong> ：对每个键的数据进行处理，以获取TopN结果。你可以使用 <code>mapValues</code>方法结合排序操作来实现这一步骤。例如，你可以使用 <code>takeOrdered</code>方法获取每个键的前N个元素。</li>
<li><strong>合并结果</strong> ：如果你需要获得全局的TopN结果，你需要将每个分区的TopN结果进行合并。你可以使用 <code>reduceByKey</code>方法来合并分区结果。</li>
</ol>
<h3 id="如何使用spark解决分组排序问题互联网公司常面">如何使用Spark解决分组排序问题？（互联网公司常面）
</h3><h3 id="hadoop中mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子">Hadoop中，Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？
</h3><p>在Hadoop中，MapReduce操作的Mapper和Reducer阶段可以与Spark中的多个算子进行对应。下面是对应关系的详细解释：</p>
<p>一、Mapper阶段：</p>
<ul>
<li>在Hadoop中，Mapper阶段负责对输入数据进行切分和映射操作。它将输入数据分割成小的数据块，并为每个数据块生成键值对。</li>
<li>在Spark中，可以使用 <code>map</code>算子来实现类似的功能。<code>map</code>算子会对输入RDD中的每个元素应用一个函数，生成一个新的RDD。</li>
</ul>
<p>二、Reducer阶段：</p>
<ul>
<li>在Hadoop中，Reducer阶段负责对Mapper阶段输出的键值对进行合并和归约操作，生成最终的输出结果。</li>
<li>在Spark中，可以使用 <code>reduceByKey</code>或 <code>groupByKey</code>算子来实现类似的功能。<code>reduceByKey</code>算子会将具有相同键的值进行合并，而 <code>groupByKey</code>算子会将具有相同键的值进行分组。</li>
</ul>
<p>需要注意的是，Spark的数据处理模型与Hadoop的MapReduce模型不同。Spark的数据处理模型是基于弹性分布式数据集（RDD）的，而不是基于键值对的。因此，在Spark中，可以使用更多的算子来进行数据转换和操作，例如 <code>filter</code>、<code>flatMap</code>、<code>join</code>等。</p>
<p>总结起来，Mapper阶段可以使用Spark的 <code>map</code>算子，而Reducer阶段可以使用Spark的 <code>reduceByKey</code>或 <code>groupByKey</code>算子。但是，需要注意的是，Spark中的算子更加灵活，可以进行更多种类的数据操作和转换。</p>
<h3 id="spark-shell启动时会启动derby">Spark shell启动时会启动derby
</h3><p>在Spark中，Derby是一个内置的关系型数据库，用于支持Spark的元数据存储和管理。当你启动Spark Shell时，它会自动启动Derby数据库作为元数据存储的后端。</p>
<p>Derby数据库是一个轻量级的Java数据库，它可以在本地模式下运行，不需要额外的配置或安装。Spark使用Derby数据库来存储关于Spark应用程序的元数据信息，包括表结构、数据源连接信息、执行计划等。</p>
<p>当你启动Spark Shell时，它会在本地启动一个Derby数据库实例，并将元数据存储在该实例中。这个Derby实例是与Spark Shell进程绑定的，当你退出Spark Shell时，Derby数据库也会随之关闭。</p>
<p>通过Derby数据库，Spark可以方便地管理和查询元数据信息，例如可以使用Spark的SQL语法来查询表结构、执行计划等。此外，Derby还支持事务处理和并发控制，确保对元数据的修改是安全和一致的。</p>
<p>总之，Spark Shell启动时会自动启动Derby数据库作为元数据存储的后端，这使得Spark能够方便地管理和查询应用程序的元数据信息。</p>
<h3 id="介绍一下你对unified-memory-management内存管理模型的理解">介绍一下你对Unified Memory Management内存管理模型的理解
</h3><p>Unified Memory Management（统一内存管理）是一种内存管理模型，它在计算机系统中统一了CPU和GPU之间的内存管理。传统上，CPU和GPU拥有各自独立的内存空间，数据需要在它们之间进行显式的复制。而在Unified Memory Management模型中，CPU和GPU共享同一块内存，数据可以在CPU和GPU之间自动进行迁移，无需显式的复制操作。</p>
<p>在Unified Memory Management模型中，程序员可以将内存分配给CPU和GPU使用，并通过简单的标记来指示数据在CPU和GPU之间的访问模式。当CPU或GPU需要访问数据时，系统会自动将数据从一个设备迁移到另一个设备。这种自动的数据迁移使得程序员可以更方便地编写跨设备的并行代码，而无需手动管理数据的复制和迁移。</p>
<p>使用Unified Memory Management模型可以简化并行编程，并提高代码的可移植性和性能。程序员可以更容易地利用GPU的并行计算能力，而无需关注数据的复制和迁移。同时，系统可以根据数据的访问模式进行智能的数据迁移，以提高访问数据的效率。</p>
<p>总之，Unified Memory Management模型通过统一CPU和GPU之间的内存管理，简化了并行编程，并提高了代码的可移植性和性能。</p>
<h3 id="hbase预分区个数和spark过程中的reduce个数相同么">HBase预分区个数和Spark过程中的reduce个数相同么
</h3><p>HBase的预分区个数和Spark过程中的reduce个数不一定相同。它们是两个不同的概念，分别用于不同的目的。</p>
<ol>
<li><strong>HBase的预分区个数</strong> ：HBase是一个分布式的NoSQL数据库，它使用行键（Row Key）来进行数据的存储和索引。为了实现数据的负载均衡和高效查询，HBase会将数据分散存储在不同的Region中。在创建HBase表时，可以指定表的预分区个数，也称为Region个数。预分区个数决定了HBase表在集群中的分布情况，可以根据数据的特点和负载要求进行调整。</li>
<li><strong>Spark过程中的reduce个数</strong> ：在Spark中，reduce是指对数据进行聚合操作的阶段，通常是在MapReduce模型中的reduce阶段。在Spark中，reduce的个数由数据的分区数决定，每个分区都会有一个reduce任务。分区数可以通过调整RDD的分区策略或使用 <code>repartition</code>、<code>coalesce</code>等操作来进行控制。reduce的个数影响了并行计算的程度，可以根据数据量和计算资源进行调整。</li>
</ol>
<p>虽然HBase的预分区个数和Spark过程中的reduce个数可以相互关联，但它们并不是一一对应的关系。HBase的预分区个数主要用于数据的存储和负载均衡，而Spark的reduce个数主要用于计算的并行度控制。在实际应用中，可以根据数据的特点和计算需求来调整它们的个数，以达到最佳的性能和效果。</p>
<h3 id="简要介绍下spark的内存管理">简要介绍下Spark的内存管理
</h3><h3 id="spark读取hdfs上的文件然后count有多少行的操作你可以说说过程吗那这个count是在内存中还是磁盘中计算的呢">Spark读取hdfs上的文件，然后count有多少行的操作，你可以说说过程吗。那这个count是在内存中，还是磁盘中计算的呢
</h3><h3 id="hbase-region多大会分区spark读取hbase数据是如何划分partition的">hbase region多大会分区，spark读取hbase数据是如何划分partition的
</h3><h3 id="说说blockmanager怎么管理硬盘和内存的">说说BlockManager怎么管理硬盘和内存的
</h3><h3 id="列举spark中常见的端口分别有什么功能">列举Spark中常见的端口，分别有什么功能
</h3><h3 id="集群上nodemanager和resourcemanager的数量关系">集群上nodemanager和ResourceManager的数量关系
</h3><h3 id="spark-如何防止内存溢出">Spark 如何防止内存溢出
</h3><h3 id="spark的通信方式">Spark的通信方式
</h3><h3 id="spark如何处理结构化数据spark如何处理非结构话数据">Spark如何处理结构化数据，Spark如何处理非结构话数据？
</h3><h3 id="对于spark你觉得他对于现有大数据的现状的优势和劣势在哪里">对于Spark你觉得他对于现有大数据的现状的优势和劣势在哪里
</h3><h3 id="简要描述你了解的一些数据挖掘算法与内容">简要描述你了解的一些数据挖掘算法与内容
</h3><h3 id="spark-streaming中对接的socket的缓存策略是什么">Spark Streaming中对接的socket的缓存策略是什么
</h3><h3 id="mapreduce和spark的相同和区别">Mapreduce和Spark的相同和区别
</h3><p>MapReduce和Spark都是用于并行计算的框架。</p>
<p><strong>相同点：</strong></p>
<ol>
<li><strong>并行计算</strong> ：两者都支持将大规模的数据集划分为多个小任务，并在分布式环境中并行执行这些任务。</li>
<li><strong>可扩展性</strong> ：它们都可以在大规模集群上运行，通过添加更多的计算节点来扩展计算能力。</li>
<li><strong>容错性</strong> ：它们都具备故障恢复机制，能够处理计算节点的故障，并保证计算的正确性。</li>
</ol>
<p><strong>区别：</strong></p>
<ol>
<li><strong>内存使用</strong> ：MapReduce将中间数据写入磁盘，而Spark将中间数据存储在内存中，这使得Spark在某些情况下比MapReduce更快，尤其是对于迭代计算和交互式查询等需要多次读写数据的场景。</li>
<li><strong>数据处理模型</strong> ：MapReduce采用了经典的&quot;map&quot;和&quot;reduce&quot;操作模型，而Spark引入了更多的数据处理操作，如过滤、排序、连接等，使得编写数据处理逻辑更加灵活。</li>
<li><strong>实时计算支持</strong> ：Spark提供了实时流处理功能，可以对数据进行实时处理和分析，而MapReduce主要用于离线批处理。</li>
<li><strong>编程接口</strong> ：MapReduce使用Java编程接口，而Spark支持多种编程语言接口，包括Java、Scala、Python和R，使得开发者可以使用自己熟悉的语言进行开发。</li>
</ol>
<p>总体而言，Spark相对于MapReduce来说更加灵活和高效，尤其适用于需要实时计算和复杂数据处理的场景。但对于一些传统的离线批处理任务，MapReduce仍然是一个可靠的选择。</p>
<h3 id="对rdddag和task的理解">对RDD、DAG和Task的理解
</h3><h3 id="dag为什么适合spark">DAG为什么适合Spark
</h3><h3 id="介绍下spark的dag以及它的生成过程">介绍下Spark的DAG以及它的生成过程
</h3><h3 id="dagscheduler如何划分干了什么活">DAGScheduler如何划分？干了什么活
</h3><h3 id="spark的容错机制">Spark的容错机制
</h3><h3 id="rdd的容错">RDD的容错
</h3><h3 id="executor内存分配">Executor内存分配
</h3><h3 id="spark的batchsize怎么解决小文件合并问题">Spark的batchsize，怎么解决小文件合并问题
</h3><h3 id="spark参数性能调优">Spark参数(性能)调优
</h3><h3 id="介绍一下spark是怎么基于内存计算的">介绍一下Spark是怎么基于内存计算的
</h3><h3 id="说下什么是rdd对rdd的理解rdd有哪些特点说下知道的rdd算子">说下什么是RDD(对RDD的理解)，RDD有哪些特点？说下知道的RDD算子
</h3><h3 id="rdd底层原理">RDD底层原理
</h3><h3 id="rdd属性">RDD属性
</h3><h3 id="rdd的缓存级别">RDD的缓存级别
</h3><h3 id="spark广播变量的实现和原理">Spark广播变量的实现和原理
</h3><h3 id="reducebykey和groupbykey的区别和作用">reduceByKey和groupByKey的区别和作用
</h3><h3 id="reducebykey和reduce的区别">reduceByKey和reduce的区别
</h3><h3 id="使用reducebykey出现数据倾斜怎么办">使用reduceByKey出现数据倾斜怎么办
</h3><h3 id="sparksql的执行原理">SparkSQL的执行原理
</h3><h3 id="sparksql的优化">SparkSQL的优化
</h3><h3 id="说下spark-checkpoint">说下Spark checkpoint
</h3><h3 id="sparksql自定义函数怎么创建dataframe">Sparksql自定义函数？怎么创建DataFrame
</h3><h3 id="hashpartitioner和rangepartitioner的实现">HashPartitioner和RangePartitioner的实现
</h3><h3 id="dagschedulertaskschedulerschedulerbackend实现原理">DAGScheduler、TaskScheduler、SchedulerBackend实现原理
</h3><h3 id="driver怎么管理executor">Driver怎么管理executor
</h3><h3 id="spark的map和flatmap的区别">Spark的map和flatmap的区别
</h3><h3 id="map和mappartition的区别">map和mapPartition的区别
</h3><h3 id="spark的cache和persist的区别它们是transformation算子还是action算子">Spark的cache和persist的区别？它们是transformation算子还是action算子？
</h3><h3 id="sparkstreaming的工作原理">SparkStreaming的工作原理
</h3><h3 id="spark-streaming的dstream和dstreamgraph的区别">Spark Streaming的DStream和DStreamGraph的区别
</h3><h3 id="spark输出文件的个数如何合并小文件">Spark输出文件的个数，如何合并小文件
</h3><h3 id="spark的driver是怎么驱动作业流程的">Spark的driver是怎么驱动作业流程的
</h3><h3 id="sparksql的劣势">SparkSQL的劣势
</h3><h3 id="dag划分spark源码实现">DAG划分Spark源码实现
</h3><h3 id="spark-steaming的双流join的过程怎么做的">Spark Steaming的双流join的过程，怎么做的
</h3><h3 id="spark的block管理">Spark的Block管理
</h3><h3 id="spark怎么保证数据不丢失">Spark怎么保证数据不丢失
</h3><h3 id="sparksql如何使用udf">SparkSQL如何使用UDF
</h3><h3 id="sparksql读取文件内存不够使用如何处理">SparkSQL读取文件，内存不够使用，如何处理
</h3><h3 id="spark的lazy提现在哪里">Spark的lazy提现在哪里
</h3><h3 id="spark中的并行度等于什么">Spark中的并行度等于什么
</h3><h3 id="spark运行时并行度的设置">Spark运行时并行度的设置
</h3><h3 id="sparksql的数据倾斜">SparkSQL的数据倾斜
</h3><h3 id="spark的exactly-one">Spark的exactly-one
</h3><h3 id="spark的rdd和partition的联系">Spark的RDD和partition的联系
</h3><h3 id="spark3的特性">Spark3的特性
</h3><h3 id="spakr计算的灵活性提现在哪里">Spakr计算的灵活性提现在哪里
</h3><h2 id="实战">实战
</h2><h3 id="spark数据倾斜问题如何定位解决方案">Spark数据倾斜问题，如何定位，解决方案
</h3><h3 id="spark提交你的jar包时所用的命令是什么">Spark提交你的jar包时所用的命令是什么
</h3><h3 id="spark-submit的时候如何引入外部jar包">spark-submit的时候如何引入外部jar包
</h3><h3 id="你如何从kafka中获取数据">你如何从Kafka中获取数据
</h3><h3 id="spark-streaming对接kafka两种整合方式的区别">Spark Streaming对接Kafka两种整合方式的区别
</h3><h3 id="如何配置spark-master的ha">如何配置Spark master的HA
</h3><h3 id="在一个不确定的数据规模的范围内进行排序可以采用以下几种方法">在一个不确定的数据规模的范围内进行排序可以采用以下几种方法
</h3><ol>
<li><strong>内存排序</strong> ：如果数据规模较小，可以将所有数据加载到内存中进行排序。这种方法简单快速，适用于能够一次性加载到内存的数据集。</li>
<li><strong>外部排序</strong> ：当数据规模较大，无法一次性加载到内存时，可以采用外部排序算法。外部排序将数据划分为多个较小的块，并在磁盘上进行排序和合并操作。</li>
</ol>
<ul>
<li>首先，将数据分割成适当大小的块，并将每个块加载到内存中进行排序。</li>
<li>然后，使用归并排序等算法将排序好的块逐一合并，直到得到完整排序的结果。
外部排序的优点是可以处理大规模数据，但需要额外的磁盘空间和IO操作。</li>
</ul>
<ol start="3">
<li><strong>分布式排序</strong> ：对于超大规模的数据，可以采用分布式排序算法。分布式排序将数据分布在多台计算机上进行并行排序和合并。</li>
</ol>
<ul>
<li>首先，将数据划分为多个分区，并将每个分区分配给不同的计算节点。</li>
<li>每个节点独立对自己负责的数据分区进行排序。</li>
<li>最后，使用归并排序等算法将各个节点的排序结果合并成最终的全局排序结果。
分布式排序可以充分利用集群的计算资源，高效地处理大规模数据。</li>
</ul>
<p>无论使用哪种方法，在不确定数据规模的情况下，都需要考虑内存和磁盘的限制，选择适当的算法和数据分割策略，以实现高效的排序操作。</p>
<h3 id="spark如何自定义partitioner分区器">Spark如何自定义partitioner分区器
</h3><h3 id="使用shell和scala代码实现wordcount">使用shell和scala代码实现WordCount
</h3><h3 id="怎么用spark做数据清洗">怎么用Spark做数据清洗
</h3><h3 id="说说spark怎么整合hive">说说Spark怎么整合hive
</h3>
</section>


    <footer class="article-footer">
    

    </footer>


    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">相关文章</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="has-image">
    <a href="/p/%E4%B8%80%E6%96%87%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82raft%E7%AE%97%E6%B3%95/">
        
        
            <div class="article-image">
                <img src="/p/%E4%B8%80%E6%96%87%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82raft%E7%AE%97%E6%B3%95/JzXWDq.c66ad75b4b2530aec67a8a6870f97170_hu14209571841734247621.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post 一文彻底搞懂Raft算法"
                        
                        data-hash="md5-xmrXW0slMK7GeopocPlxcA==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">一文彻底搞懂Raft算法</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/hive%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/">
        
        
            <div class="article-image">
                <img src="/p/hive%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/trees-8136806_1280.357dd549ed51f498bcbf5425f76e0416_hu9173719500181869714.png" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post Hive面试一文通"
                        
                        data-hash="md5-NX3VSe1R9Ji8v1Ql924EFg==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Hive面试一文通</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/flink%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/">
        
        
            <div class="article-image">
                <img src="/p/flink%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/leaf-8687801_1280.88b6b08dfc2ee2e24256c1491b57935f_hu14693269959298157630.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post Flink面试一文通"
                        
                        data-hash="md5-iLawjfwu4uJCVsFJG1eTXw==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Flink面试一文通</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86%E4%B8%80%E6%96%87%E9%80%9A/">
        
        
            <div class="article-image">
                <img src="/p/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86%E4%B8%80%E6%96%87%E9%80%9A/grebe-7972183_1280.6dd8a2d7b8cf99148cf7a877d9459ae8_hu10399000877992530385.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post 二叉树遍历一文通"
                        
                        data-hash="md5-bdii17jPmRSM96h32UWa6A==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">二叉树遍历一文通</h2>
        </div>
    </a>
</article>

            
                
<article class="has-image">
    <a href="/p/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B8%80%E6%96%87%E9%80%9A/">
        
        
            <div class="article-image">
                <img src="/p/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B8%80%E6%96%87%E9%80%9A/matt-le-SJSpo9hQf7s-unsplash.8eae06664ddfb3e8dc6e000756cb703b_hu13612953062116952428.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy"
                        alt="Featured image of post 动态规划一文通"
                        
                        data-hash="md5-jq4GZk3fs&#43;jcbgAHVstwOw==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">动态规划一文通</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    <script src="https://utteranc.es/client.js" 
        repo="sherlock-lin/utterances-comments"
        issue-term="pathname"
        
        crossorigin="anonymous"
        async
        >
</script>

<style>
    .utterances {
        max-width: unset;
    }
</style>

<script>
    let utterancesLoaded = false;

    function setUtterancesTheme(theme) {
        let utterances = document.querySelector('.utterances iframe');
        if (utterances) {
            utterances.contentWindow.postMessage(
                {
                    type: 'set-theme',
                    theme: `github-${theme}`
                },
                'https://utteranc.es'
            );
        }
    }

    addEventListener('message', event => {
        if (event.origin !== 'https://utteranc.es') return;

        
        utterancesLoaded = true;
        setUtterancesTheme(document.documentElement.dataset.scheme)
    });

    window.addEventListener('onColorSchemeChange', (e) => {
        if (!utterancesLoaded) return;
        setUtterancesTheme(e.detail)
    })
</script>


    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2022 - 
        
        2025 sherlock-lin
    </section>
    
    <section class="powerby">
        使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.27.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv">本站总访问量 <span id="busuanzi_value_site_pv"></span> 次 · </span>
<span id="busuanzi_container_site_uv">您是本站第 <span id="busuanzi_value_site_uv"></span> 位访问者</span>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
