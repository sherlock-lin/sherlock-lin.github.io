[{"content":"概述 Hive SQL 面试问题通常围绕性能优化、数据倾斜、数据清洗、复杂查询等场景展开。在回答问题时，可以结合实际项目经验，展示你的技术深度和解决问题的能力。以下是一些通用的面试技巧：\n结合实际案例：用具体的项目经验说明问题。 展示优化思路：从数据、SQL、资源配置等多个角度提出优化方案。 强调工具使用：如窗口函数、MapJoin、动态分区等高级功能。 高频 如何优化 Hive SQL 查询性能 数据量往往不是影响Hive执行效率的核心因素，数据倾斜、job数分配的不合理、磁盘或网络I/O过高、MapReduce配置的不合理等等才是影响Hive性能的关键。Hive在执行任务时，通常会将Hive SQL转化为MapReduce job进行处理。因此对Hive的调优，除了对Hive语句本身的优化，也要考虑Hive配置项以及MapReduce相关的优化。从更底层思考如何优化性能，而不是仅仅局限于代码/SQL的层面。\n列裁剪和分区裁剪\n要对表进行分区（如按日期、地区），减少数据扫描量。\nHive在读数据的时候，只读取查询中所需要用到的列，而忽略其它列。例如，若有以下查询：\n1 SELECT age, name FROM people WHERE age \u0026gt; 30; 在实施此项查询中，people表有3列（age，name，address），Hive只读取查询逻辑中真正需要的两列age、name，而忽略列address；这样做节省了读取开销，中间表存储开销和数据整合开销。\n同理，对于Hive分区表的查询，我们在写SQL时，通过指定实际需要的分区，可以减少不必要的分区数据扫描【当Hive表中列很多或者数据量很大时，如果直接使用select * 或者不指定分区，效率会很低下（全列扫描和全表扫描）】。\nHive中与列裁剪和分区裁剪优化相关的配置参数分别为：hive.optimize.cp和hive.optimize.pruner，默认都是true。\n谓词下推\n在关系型数据库如MySQL中，也有谓词下推（Predicate Pushdown，PPD）的概念。它就是将SQL语句中的where谓词逻辑都尽可能提前执行，减少下游处理的数据量。如下Hive SQL语句：\n1 2 3 4 5 6 select a.*, b.* from a join b on (a.id = b.id) where a.id \u0026gt; 15 and b.num \u0026gt; 16; 如果没有谓词下推，上述SQL需要在完成join处理之后才会执行where条件过滤。在这种情况下，参与join的数据可能会非常多，从而影响执行效率。使用谓词下推，那么where条件会在join之前被处理，参与join的数据量减少，提升效率。\n在Hive中，可以通过将参数hive.optimize.ppd设置为true，启用谓词下推。与它对应的逻辑优化器是PredicatePushDown。该优化器就是将OperatorTree中的FilterOperator向上提\n解答思路： 数据分区：对表进行分区（如按日期、地区），减少数据扫描量。\n数据分桶：对表进行分桶，优化 JOIN 和聚合操作。\n压缩数据：使用 ORC、Parquet 等列式存储格式，并启用压缩（如 Snappy）。\n避免数据倾斜：使用随机前缀或 MapJoin 解决数据倾斜问题。\n调整资源配置：增加 Mapper 和 Reducer 的数量，调整内存分配。\n如何处理数据倾斜问题 问题描述： 在 JOIN 或 GROUP BY 操作中，某些 Key 的数据量远大于其他 Key，导致任务负载不均衡。\n解答思路： 增加随机前缀：对倾斜的 Key 添加随机前缀，分散数据。\n使用 MapJoin：如果一张表较小，可以将其加载到内存中，避免 Shuffle。\n拆分倾斜 Key：将倾斜的 Key 单独处理，再与其他数据合并。\n如何解决小文件问题 问题描述： 写入 HDFS 时，生成大量小文件，影响存储和查询性能。\n解答思路： 合并小文件：使用 INSERT OVERWRITE 或 ALTER TABLE \u0026hellip; CONCATENATE 合并小文件。\n调整 Reducer 数量：通过 hive.merge.size.per.task 和 hive.merge.smallfiles.avgsize 参数控制文件大小。\n使用 DISTRIBUTE BY：在写入时对数据进行重分布，减少小文件。\n如何实现数据去重 问题描述： 数据中存在重复记录，如何去除重复数据？\n解答思路： 使用 GROUP BY：对重复字段进行分组，保留一条记录。\n使用 ROW_NUMBER()：通过窗口函数标记重复记录并过滤。\n使用 DISTINCT：直接去除重复记录。\n如何实现多表 JOIN 的优化 问题描述： 多表 JOIN 操作性能较差，如何优化？\n解答思路： MapJoin：如果一张表较小，可以将其加载到内存中。\n调整 JOIN 顺序：将小表放在 JOIN 的左侧。\n使用分桶表：对表进行分桶，优化 JOIN 操作。\n如何实现动态分区插入 问题描述： 如何将数据插入到动态分区表中？\n解答思路： 启用动态分区：设置 hive.exec.dynamic.partition=true。\n插入数据：使用 INSERT INTO \u0026hellip; PARTITION 语法插入数据。\n示例：\nsql 复制 INSERT INTO TABLE target_table PARTITION (dt, region) SELECT col1, col2, dt, region FROM source_table;\n如何实现数据分层存储 问题描述： 如何将数据按照 ODS、DWD、DWS 等层级存储？\n解答思路： ODS 层：存储原始数据，不做清洗。\nDWD 层：对数据进行清洗、去重、格式转换。\nDWS 层：对数据进行聚合，生成宽表或指标表。\nADS 层：存储最终的分析结果，供业务使用。\n如何实现增量数据同步 问题描述： 如何将增量数据同步到目标表？\n解答思路： 使用时间戳：通过时间戳字段过滤增量数据。\n使用 CDC 工具：如 Debezium、Canal，捕获数据变更。\n使用 MERGE 语句：将增量数据与目标表合并。\n如何实现数据一致性校验 问题描述： 如何确保源表和目标表的数据一致性？\n解答思路： 数据比对：通过 JOIN 或 EXCEPT 比对源表和目标表的数据。\n校验指标：比对记录数、SUM、MAX、MIN 等指标。\n使用 Checksum：计算数据的 Checksum 值进行比对。\n如何实现复杂指标的计算 问题描述： 如何计算复杂的业务指标（如 UV、留存率）？\n解答思路： UV：使用 COUNT(DISTINCT user_id) 计算。\n留存率：通过自连接或窗口函数计算用户的活跃状态。\n示例：\n基础 分区分桶的区别，为什么要分区 分区表 ：原来的一个大表存储的时候分成不同的数据目录进行存储。如果说是单分区表，那么在表的目录下就只有一级子目录，如果说是多分区表，那么在表的目录下有多少分区就有多少级子目录。不管是单分区表，还是多分区表，在表的目录下，和非最终分区目录下是不能直接存储数据文件的 分桶表 ：原理和HashPartitioner一样，将Hive中的一张表的数据进行归纳分类的时候，归纳分类规则就是HashPartitioner（需要指定分桶字段，指定分成多少桶） 区别\n除了存储的格式不同之外，主要是作用：\n分区表 ：细化数据管理，缩小MR程序需要扫描的数据量 分桶表 ：提高Join查询的效率，在一份数据会被经常用来做连接查询的时候建立分桶表，分桶字段就是连接字段；提高采样的效率 有了分区表为什么还要分桶\n获得更高的查询处理效率。桶为表加上了额外的结构，Hive在处理有些查询时可以利用这个结构 使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多的方便 分桶是相对分区进行更细粒度的划分。分桶将表或者分区的某列值进行Hash值进行分区 与分区不同的是，分区依据的不是真实数据表文件中的列，而是我们指定的伪列，但是分桶是依据数据表中真实的列而不是伪列 MapJoin的原理 MapJoin通常用于一个很小的表和一个大表进行join的场景，具体小表有多小，由参数 hive.mapjoin.smalltable.filesize来决定，该参数表示小表的总大小，默认值为25000000字节，即25M。\nHive0.7之前，需要使用hint提示 /*+ mapjoin(table) */才会执行MapJoin,否则执行Common Join，但在0.7版本之后，默认自动会转换Map Join，由参数 hive.auto.convert.join来控制，默认为true\nMapJoin简单说就是在Map阶段将小表数据从 HDFS 上读取到内存中的哈希表中，读完后将内存中的哈希表序列化为哈希表文件，在下一阶段，当 MapReduce 任务启动时，会将这个哈希表文件上传到 Hadoop 分布式缓存中，该缓存会将这些文件发送到每个 Mapper 的本地磁盘上。因此，所有 Mapper 都可以将此持久化的哈希表文件加载回内存，并像之前一样进行 Join。顺序扫描大表完成Join。减少昂贵的shuffle操作及reduce操作\nMapJoin分为两个阶段：\n通过MapReduce Local Task，将小表读入内存，生成HashTableFiles上传至Distributed Cache中，这里会HashTableFiles进行压缩。 MapReduce Job在Map阶段，每个Mapper从Distributed Cache读取HashTableFiles到内存中，顺序扫描大表，在Map阶段直接进行Join，将数据传递给下一个MapReduce任务 什么时候使用内部表,什么时候使用外部表 hive内部表和外部表的区别\n内部表：加载数据到hive所在的hdfs目录，删除时，元数据和数据文件都删除\n外部表：不加载数据到hive所在的hdfs目录，删除时，只删除表结构。\n这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据\n每天采集的ng日志和埋点日志,在存储的时候建议使用外部表，因为日志数据是采集程序实时采集进来的，一旦被误删，恢复起来非常麻烦。而且外部表方便数据的共享。 抽取过来的业务数据，其实用外部表或者内部表问题都不大，就算被误删，恢复起来也是很快的，如果需要对数据内容和元数据进行紧凑的管理, 那还是建议使用内部表 在做统计分析时候用到的中间表，结果表可以使用内部表，因为这些数据不需要共享，使用内部表更为合适。并且很多时候结果分区表我们只需要保留最近3天的数据，用外部表的时候删除分区时无法删除数据 hive 都有哪些函数，你平常工作中用到哪些 group by 为什么要排序 说说印象最深的一次优化场景，hive 常见的优化思路 印象最深的一次优化场景是在一个大型数据仓库项目中，需要对Hive中的表进行优化以提高查询性能。以下是一些常见的Hive优化思路：\n压缩数据 ：使用压缩算法（如Snappy、Gzip、LZO等）可以减小数据的存储空间，从而减少磁盘IO操作，提高查询性能。 使用列式存储格式 ：Hive支持多种列式存储格式（如ORC、Parquet），这些格式可以提供更高的压缩比和更快的查询速度，因为它们只读取所需的列，而不是整个行。 分桶 ：将表按照某个列的哈希值进行分桶，可以将数据均匀地分布在多个文件中，从而提高查询的并行度和性能。 使用索引 ：Hive支持基于B树的索引，通过在关键列上创建索引可以加快查询速度。但需要注意的是，索引会增加写入操作的开销，因此需要权衡索引对查询性能的提升和写入性能的影响。 合理设置分区 ：将表按照某个列的值进行分区可以减少查询的数据量，提高查询性能。同时，可以将常用的查询条件作为分区键，以进一步提高查询效率。 数据倾斜处理 ：当某个列的值分布不均匀时，可能会导致查询性能下降。可以通过对数据进行重新分区、使用随机前缀等方式来解决数据倾斜的问题。 使用适当的硬件资源 ：合理配置Hive的资源参数，如内存、CPU等，以及选择性能较好的硬件设备，可以提高查询的执行效率。 这些只是一些常见的Hive优化思路，实际的优化策略还需要根据具体的场景和需求来确定。在实际应用中，可以通过不断的实验和调整来找到最适合的优化方案。\nHive的Join底层MR是如何实现的 说说Hive的执行流程 ORC、Parquet等列式存储的优缺点 ORC:ORC文件是自描述的，它的元数据使用Protocol Buffers序列化，文件中的数据尽可能的压缩以降低存储空间的消耗；以二进制方式存储，不可以直接读取；自解析，包含许多元数据，这些元数据都是同构ProtoBuffer进行序列化的；会尽可能合并多个离散的区间尽可能的减少I/O次数；在新版本的ORC中也加入了对Bloom Filter的支持，它可以进一 步提升谓词下推的效率，在Hive 1.2.0版本以后也加入了对此的支持。\nParquet:Parquet支持嵌套的数据模型，类似于Protocol Buffers，每一个数据模型的schema包含多个字段，每一个字段有三个属性：重复次数、数据类型和字段名；Parquet中没有Map、Array这样的复杂数据结构，但是可以通过repeated和group组合来实现；通过Striping/Assembly算法，parquet可以使用较少的存储空间表示复杂的嵌套格式，并且通常Repetition level和Definition level都是较小的整数值，可以通过RLE算法对其进行压缩，进一步降低存储空间；Parquet文件以二进制方式存储，不可以直接读取和修改，Parquet文件是自解析的，文件中包括该文件的数据和元数据。\n为什么要对数据仓库分层 sort by 和 order by 的区别 使用过Hive解析JSON串吗 Hive 中的压缩格式TextFile、SequenceFile、RCfile 、ORCfile各有什么区别？ 1.TextFile\n默认格式，存储方式为行存储，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，压缩后的文件不支持split，Hive不会对数据进行切分，从而无法对数据进行并行操作。并且在反序列化过程中，必须逐个字符判断是不是分隔符和行结束符，因此反序列化开销会比SequenceFile高几十倍。\n2.SequenceFile\nSequenceFile是Hadoop API提供的一种二进制文件支持，存储方式为行存储，其具有使用方便、可分割、可压缩的特点。\nSequenceFile支持三种压缩选择：NONE，RECORD，BLOCK。Record压缩率低，一般建议使用BLOCK压缩。\n优势是文件和hadoop api中的MapFile是相互兼容的\n3.RCFile\n存储方式：数据按行分块，每块按列存储。结合了行存储和列存储的优点：\n首先，RCFile 保证同一行的数据位于同一节点，因此元组重构的开销很低；\n其次，像列存储一样，RCFile 能够利用列维度的数据压缩，并且能跳过不必要的列读取；\n4.ORCFile\n存储方式：数据按行分块 每块按照列存储。\n压缩快、快速列存取。\n效率比rcfile高，是rcfile的改良版本。\n小结：\n相比TEXTFILE和SEQUENCEFILE，RCFILE由于列式存储方式，数据加载时性能消耗较大，但是具有较好的压缩比和查询响应。\n数据仓库的特点是一次写入、多次读取，因此，整体来看，RCFILE相比其余两种格式具有较明显的优势。\n参考 Hive/HiveSQL常用优化方法全面总结 Hive解决数据倾斜的各种优化方法 Hive-Group by的优化(解决数据倾斜的问题) ","date":"2025-03-12T11:53:25+08:00","image":"https://sherlock-lin.github.io/p/hive%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/trees-8136806_1280_hu4473522823771471937.png","permalink":"https://sherlock-lin.github.io/p/hive%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/","title":"Hive面试一文通"},{"content":"高频 Spark 为什么比hive快 Spark 相对于 Hive 具有以下几个方面的优势：\nDAG计算模型：Spark 使用自己的DAG执行引擎，通过有向无环图（DAG）来表示任务，优化了任务的调度和执行，避免不必要的中间结果写入磁盘，也就是数据都存在内存中，直接通过算子处理，中间结果不落盘，从而大幅减少磁盘IO开销，而Hive默认使用 MapReduce 作为执行引擎，MapReduce计算模型比较简单，每个算子都会将中间结果写入磁盘，导致大量的磁盘 I/O 操作，所以速度较慢。 延迟计算: Spark 使用延迟计算机制，只有在遇到action操作如 collect、count时才会真正执行计算。这种机制允许 Spark 进行更多的优化，例如合并多个操作、减少数据 shuffle等并且内存分配更科学；而Hive 的查询通常是即时执行的，缺乏类似的优化机制。 数据分区和并行处理：Spark 支持更细粒度的数据分区和并行处理，可以更好地利用集群资源。Spark 的 RDD（弹性分布式数据集）和 DataFrame/Dataset API 提供了灵活的分区和并行处理机制。而Hive 的并行处理能力依赖于 MapReduce，MapReduce 的并行度受限于 Map 和 Reduce 任务的数量，且任务调度开销较大。 优化器：Spark 提供了 Catalyst 优化器，用于优化 SQL 查询的执行计划。Catalyst 优化器可以进行谓词下推、列剪裁、常量折叠等多种优化，显著提高查询性能。Hive 也有优化器，但其优化能力相对较弱，尤其是在处理复杂查询时，优化效果不如 Spark。 JVM 的优化 : Hive 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM，基于进程的操作。而 Spark 每次操作是基于线程的，只在启动 Executor 时候启动一次 JVM，内存的 Task 操作是线程复用的。每次启动 JVM 的时间可能就需要几秒甚至十几秒，那么当 Task 多了，mapreduce 不知道比 Spark 慢了多少。 总的来说Spark 之所以比 Hive 快，主要是因为 Spark 采用了DAG计算模型或者说内存计算、延迟计算、Catalyst 优化器等先进技术，减少了磁盘 I/O 和任务调度开销，提高了数据处理效率，因此Spark非常适合像批处理、机器学习这样的场景，当然对于一些传统的批处理场景，Mapreduce仍然是一种可靠的选择。\n缓存机制与checkpoint机制的区别 两者都是做RDD持久化的，RDD通过cache方法或者persist方法可以将前面的计算结果缓存，但并不是立即缓存，而是在调用Action算子的时候，这个RDD会被缓存在计算节点的内存中，并供后面使用。缓存既不是transformation也不是action类的算子，并且缓存结束后不会产生新的RDD。\n缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。\n使用缓存的条件：（或者说什么时候进行缓存）\n要求的计算速度快，对效率要求高的时候 集群的资源要足够大，能容得下要被缓存的数据 被缓存的数据会多次的触发Action（多次调用Action类的算子） 先进行过滤，然后将缩小范围后的数据缓存到内存中 在使用完数据之后，要释放缓存，否则会一直在内存中占用资源\nRDD的缓存容错机制能保证数据丢失也能正常的运行，是因为在每一个RDD中，都存有上一个RDD的信息，当RDD丢失以后，可以追溯到元数据，再进行计算。\nCheckPoint(本质是通过将RDD写入高可用的地方（例如 hdfs）做检查点）是为了通过lineage（血统）做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。\n设置checkpoint的目录，可以是本地的文件夹、也可以是HDFS、S3。一般是在具有容错能力，高可靠的文件系统上设置一个检查点路径，用于保存检查点数据。\n在设置检查点之后，该RDD之前的有依赖关系的父RDD都会被销毁，下次调用的时候直接从检查点开始计算。checkPoint和cache一样，都是通过调用一个Action类的算子才能运行。checkPoint降低运行时间的原因：第一次调用检查点的时候，会产生两个executor，两个进程分别是从hdfs读文件和计算（调用的Action类的算子），在第二次调用的时候会发现，运行的时间大大减少，是由于第二次调用算子的时候，不会再从hdfs读源文件，而是直接从hdfs读取缓存到的数据\n说下Spark join的分类 根据数据分布的方式，可以分为Shuffle Join、Broadcast Join和Bucket Join。\nShuffle Join：数据需要通过网络Shuffle到不同的节点，网络传输开销大，性能差。 Broadcast Join：将小表广播到所有Executor节点，避免Shuffle，适用于一个大表和小表的JOIN，避免了Shuffle，性能较高。 Bucket Join：两个表都按照JOIN Key进行分桶，且分桶数相同，避免了Shuffle，数据可以直接在本地JOIN。但需要两张表都进行分桶，且分桶规则匹配。 根据执行的方式，可以分为SortMergeJoin、Hash Join和Nested Loop Join。\nSort Merge Join：对JOIN Key进行排序后合并，是Spark中最常用的JOIN方式，适合大规模数据且性能稳定。 Hash Join：对JOIN Key进行哈希分区后JOIN，适用于内存足够的情况也就是一张表可以完全放入内存，性能较高，适合小表JOIN大表。 Nested Loop Join：通过嵌套循环的方式逐条匹配数据，性能较差，通常不推荐使用，适合两个小数据集的JOIN 根据JOIN类型分类，有Inner Join、Left Join、Right Join、Full Outer Join、Cross Join、Left Semi Join和Left Anti Join\nInner Join：只返回两个表中匹配的记录。\n1 SELECT * FROM table1 INNER JOIN table2 ON table1.key = table2.key; Left Join：返回左表的所有记录，以及右表中匹配的记录（不匹配的字段为 NULL）。\n1 SELECT * FROM table1 LEFT JOIN table2 ON table1.key = table2.key; Right Join：返回右表的所有记录，以及左表中匹配的记录（不匹配的字段为 NULL）。\n1 SELECT * FROM table1 RIGHT JOIN table2 ON table1.key = table2.key; Full Outer Join：返回两个表的所有记录，不匹配的字段为 NULL。\n1 SELECT * FROM table1 FULL OUTER JOIN table2 ON table1.key = table2.key; Cross Join：返回两个表的笛卡尔积。\n1 SELECT * FROM table1 CROSS JOIN table2; Left Semi Join：只返回左表中与右表匹配的记录。\n1 SELECT * FROM table1 LEFT SEMI JOIN table2 ON table1.key = table2.key; Left Anti Join：只返回左表中与右表不匹配的记录。\n1 SELECT * FROM table1 LEFT ANTI JOIN table2 ON table1.key = table2.key; Spark的Stage划分 Spark会将Job作业根据宽依赖划分为多个Stage，每个Stage包含一组可以并行执行的Task任务。\n它的划分流程是这样，首先会从最终的RDD开始，逆向遍历RDD的血缘关系(Lineage)，遇到窄依赖时就继续向上遍历，遇到宽依赖时，将当前已经遍历过的RDD划分为一个Stage，并创建一个新的Stage重复上诉过程，直到遍历完所有的RDD。\n尽量避免使用 groupByKey 等宽依赖操作，改用 reduceByKey 等窄依赖操作。同时可以通过 Spark UI 查看作业的阶段划分和任务执行情况，以及查看 Driver 和 Executor 的日志，分析阶段划分和任务执行的性能瓶颈。\nSpark处理数据的具体流程 Spark处理数据的流程包括数据加载、任务调度、数据计算、结果输出，这个过程也涉及到资源干礼以及容错等操作。\n首先第一步是数据加载，Spark支持多种数据源像HDFS、S3、本地文件系统、Kafka、JDBC等，可以通过textFile、read、parallelize等方法来加载数据，数据加载后会被划分为多个分区，每个分区都是一个独立的数据块。\n第二步是任务的调度，Spark 根据 RDD 的血缘关系（Lineage）生成有向无环图（DAG），DAG 描述了数据处理的逻辑流程，包括转换操作（Transformation）和行动操作（Action）。然后Spark 会将 DAG 划分为多个阶段（Stage），每个阶段包含一组可以并行执行的任务（Task）。阶段划分的依据是宽依赖（Shuffle Dependency）和窄依赖（Narrow Dependency）。然后Spark 将任务分配给 Executor 执行，任务调度由 Driver 程序负责。\n第三步是任务计算，每个Task任务处理一个分区的数据，执行转换操作如map、filter等，任务在Executor中执行，Executor是运行在集群节点上的进程，在遇到宽依赖比如groupByKey、join的时候，Spark会进行Shuffle操作，Shuffle 操作涉及数据的跨节点传输和重新分区。如果数据需要多次使用，可以将其缓存到内存或磁盘中（如 persist、cache）。\n最后一步就是结果输出，对于行动操作（如 collect、count），Spark 会将结果返回给 Driver 程序，对于输出操作（如 saveAsTextFile、saveAsTable），Spark 会将结果存储到外部存储系统中。\nSpark map join的实现原理 在 Spark 中，Map Join（也称为 Broadcast Join）是一种用于优化小表与大表 JOIN 操作的技术。它的核心思想是将小表的数据广播到所有 Executor 节点，从而避免 Shuffle 操作，显著提升 JOIN 的性能。在常规的 Shuffle Join 中，大表和小表的数据都需要通过网络传输到 Reduce 阶段进行 JOIN，这会导致大量的网络和磁盘 I/O 开销，而Map Join 通过将小表的数据广播到所有 Executor 节点，使得每个节点都可以在本地完成 JOIN 操作，从而避免 Shuffle。\n广播Join的原理是，Spark 使用广播变量将小表的数据序列化并分发到所有 Executor 节点，Executor 节点将接收到的数据反序列化并存储在内存中，然后每个 Executor 节点在本地将大表的分区数据与广播的小表数据进行 JOIN。Join的时候Spark会对大表的分区数据进行遍历，对于每条记录，根据JOIN Key在小表中查找匹配的记录并将匹配的结果输出，最后每个Executor 节点将 JOIN 结果输出到本地或外部存储系统。\n广播Join出发的条件是，小表的大小必须小于spark.sql.autoBroadcastJoinThreshold参数的值，默认10M，也就是小表的数据需要能够完全放入 Executor 的内存中，广播Join适用于Inner Join、Left Join 等 JOIN 类型。但Spark也支持用户手动使用broadcast函数触发。\n广播Join的优势在于避免了Shuffle操作，减少网络和磁盘的IO开销，同时JOIN操作在本地完成，显著提升了执行效率。但它也有自己的局限性，那就是小表的数据必须能够完全放入Executor 的内存中，并且广播Join不适用于Full Outer Join 等复杂的 JOIN 类型。\nSpark应用程序执行流程 Spark应用程序的执行过程可以分为以下几个步骤：\n创建SparkContext ：应用程序首先需要创建一个SparkContext对象，它是与Spark集群通信的入口点。SparkContext负责与集群管理器通信，并为应用程序分配资源。 创建RDD ：应用程序需要将数据加载到弹性分布式数据集（RDD）中。RDD是Spark的核心数据结构，它代表了分布在集群中的数据集合。RDD可以通过读取文件、从内存中的数据集创建、从其他RDD转换等方式来创建。 转换操作 ：一旦RDD被创建，应用程序可以对RDD进行一系列的转换操作，例如map、filter、reduce等。这些转换操作会生成新的RDD，而不会立即执行计算。 行动操作 ：当应用程序需要获取转换操作的结果时，需要执行行动操作。行动操作会触发Spark执行计划的生成和执行。常见的行动操作包括collect、count、save等，它们会返回计算结果给应用程序。 作业划分 ：Spark将应用程序的行动操作划分为一系列的作业（jobs），每个作业由一组相关的转换操作组成。作业划分是为了提高并行度和数据本地性，以便更好地利用集群资源。 任务划分 ：每个作业被划分为一系列的任务（tasks），每个任务处理数据的一部分。任务的划分是根据数据分区和可用资源来完成的。 任务调度和执行 ：Spark将任务分发到集群中的执行器节点上执行。任务调度器负责将任务分配给可用的执行器节点，并考虑数据本地性以提高性能。执行器节点会加载数据到内存中，并执行任务操作。 结果返回 ：一旦任务执行完成，执行器将结果返回给驱动程序。驱动程序可以将结果保存到内存、磁盘或其他外部存储中，也可以将结果返回给应用程序。 容错和恢复 ：Spark具有容错机制，可以在节点故障时自动恢复。如果某个节点失败，Spark可以重新调度任务并在其他节点上执行。 总的来说，Spark应用程序的执行过程包括创建SparkContext、创建RDD、转换操作、行动操作、作业划分、任务划分、任务调度和执行、结果返回以及容错和恢复等步骤。通过这些步骤，Spark能够以高效、可靠的方式处理大规模数据和复杂计算任务。\n介绍下Spark Shuffle以及其优缺点 在 Apache Spark 中，Shuffle 是一个关键的操作，它负责在分布式计算中重新分布数据。Shuffle 通常发生在宽依赖操作（如 groupByKey、reduceByKey、join 等）中，是 Spark 作业中最昂贵的操作之一。Shuffle 是 Spark 中数据重新分布的过程。当需要将数据按照某种规则（如 Key）重新分组或聚合时，Spark 会将数据从上游任务（Map 阶段）的输出分区重新分区并传输到下游任务（Reduce 阶段）的输入分区。\nShuffle 分为两个主要阶段，Shuffle Write：Map 任务将输出数据写入本地磁盘，并按照分区规则将数据分组，数据会被写入多个临时文件，每个文件对应一个 Reduce 分区。Shuffle Read：Reduce 任务从各个 Map 任务的输出文件中读取数据，并进行聚合或计算，数据读取过程可能涉及网络传输。\nSpark 提供了两种 Shuffle 实现方式，Hash Shuffle：每个 Map 任务为每个 Reduce 任务生成一个文件，导致文件数过多，适用于小规模数据，但会产生大量小文件，影响性能。Sort Shuffle：Map 任务将数据排序后写入单个文件，并生成索引文件，减少了文件数，适用于大规模数据，是 Spark 1.2 之后的默认实现。\nShuffle 的优点：Shuffle 使得 Spark 能够支持复杂的宽依赖操作，如 groupByKey、reduceByKey、join 等；Shuffle 可以根据业务需求重新分布数据，满足计算需求；灵活性：Shuffle 支持自定义分区器（Partitioner），用户可以根据业务需求定义分区规则。\nShuffle 的缺点\n性能开销大：磁盘 I/O：Shuffle Write 和 Shuffle Read 都涉及大量磁盘读写操作；网络传输：数据需要在节点之间传输，网络带宽可能成为瓶颈；序列化/反序列化：数据在传输前后需要序列化和反序列化，消耗 CPU 资源。\n数据倾斜：某些分区的数据量远大于其他分区，导致任务负载不均衡。\n小文件问题：Hash Shuffle 会产生大量小文件，影响存储和查询性能。\nShuffle 的优化\n减少 Shuffle 数据量：在 Shuffle 前过滤掉不必要的数据，使用 reduceByKey 代替 groupByKey，减少数据传输量。\n优化分区数：合理设置分区数，避免过多或过少的分区。\n使用高效的序列化库：使用 Kryo 序列化库，减少序列化开销。\n启用 Shuffle 压缩：通过 spark.shuffle.compress 参数启用压缩，减少网络传输量。\n数据倾斜处理：对倾斜的 Key 添加随机前缀，分散数据，使用自定义分区器，优化数据分布。\n什么情况下会产生Spark Shuffle 宽依赖操作会导致 Shuffle，因为数据需要重新分布到不同的分区，例如groupByKey、reduceByKey、join、cogroup、distinct、repartition、sortByKey。\n我们应当尽量避免不必要的shuffle或者要对shuffle做些调优，例如使用广播JOIN、使用 reduceByKey 代替 groupByKey(reduceByKey 在 Map 端进行局部聚合，减少了 Shuffle 数据量)、优化分区数、使用缓存等。避免不必要的 Shuffle，是优化 Spark 作业性能的关键，通过合理设计数据处理逻辑和优化配置，可以显著减少 Shuffle 的开销。\n为什么要Spark Shuffle 在分布式计算中，数据通常分布在不同的节点上。要对数据进行聚合或连接操作，需要将具有相同 Key 的数据重新分布到同一个节点上进行处理。\n说明yarn-cluster模式作业提交流程，yarn-cluster与yarn-client的区别 在Spark中，YARN是一种用于集群资源管理的框架，可以将Spark应用程序提交到YARN集群上进行执行。YARN提供了两种模式供Spark应用程序提交：yarn-cluster和yarn-client。\nyarn-cluster模式作业提交流程： 当你在使用 spark-submit命令提交Spark应用程序时，指定了 --deploy-mode cluster参数，即使用yarn-cluster模式。 提交命令发送给YARN的ResourceManager，ResourceManager会为应用程序分配一个ApplicationMaster。 ApplicationMaster是一个专门负责管理应用程序执行的组件，它会向ResourceManager请求资源。 ResourceManager会为ApplicationMaster分配一定数量的资源，这些资源将用于启动Executor。 ApplicationMaster会与各个NodeManager通信，启动Executor，并将应用程序的代码和依赖文件分发到各个Executor所在的节点上。 Executor会根据指定的任务分配策略执行具体的任务，任务的执行结果会返回给ApplicationMaster。 ApplicationMaster会将任务的执行结果返回给Driver程序，Driver程序可以根据需要进行后续处理。 yarn-cluster与yarn-client的区别： yarn-cluster模式下，Driver程序运行在集群中的一个Executor上，并且Driver程序与ApplicationMaster是独立的进程。这意味着Driver程序的执行不会受到本地环境的限制，可以充分利用集群资源。 yarn-client模式下，Driver程序运行在提交Spark应用程序的本地机器上，而不是集群中的一个Executor上。Driver程序与ApplicationMaster是同一个进程。这意味着Driver程序的执行受到本地环境的限制，例如可用的内存和CPU资源。 在yarn-cluster模式下，提交的应用程序可以在集群中长时间运行，即使与提交应用程序的客户端断开连接，应用程序仍然可以继续执行。而在yarn-client模式下，如果与客户端断开连接，应用程序会被终止。 yarn-cluster模式适用于长时间运行的应用程序，例如批处理作业。yarn-client模式适用于交互式应用程序，例如Spark Shell。 Yarn Client模式(一般用于测试)\nDriver在任务提交的本地机器上运行 Driver启动后会向ResourceManager通讯申请启动ApplicationMaster ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，负责向ResourceManager申请Executor内存 ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分配指定的NodeManager上驱动Executor进程 Executor进程启动后会向Driver反向注册，Executor全部注册完成之后，Driver开始执行Main函数 之后执行到Action算子，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行 Yarn Cluster模式\n任务提交后，Client会先和ResourceManager通讯，申请启动ApplicationMaster ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver Driver启动后会向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后在合适的NodeManager上启动Executor进程 Executor进程启动后会向Driver反向注册，Executor全部注册完成之后，Driver开始执行Main函数 之后执行到Action算子，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行 总结：yarn-cluster模式下，Driver程序运行在集群中的一个Executor上，与ApplicationMaster是独立的进程；yarn-client模式下，Driver程序运行在本地机器上，与ApplicationMaster是同一个进程。\nSpark的内存模型 Spark中的内存使用分为两部分：执行（execution）与存储（storage）。执行内存主要用于shuffles、joins、sorts和aggregations，存储内存则用于缓存或者跨节点的内部数据传输。1.6之前，对于一个Executor，内存都由以下部分构成： 1）ExecutionMemory。这片内存区域是为了解决 shuffles,joins, sorts and aggregations 过程中为了避免频繁IO需要的buffer。 通过spark.shuffle.memoryFraction(默认 0.2) 配置。 2）StorageMemory。这片内存区域是为了解决 block cache(就是你显示调用rdd.cache, rdd.persist等方法), 还有就是broadcasts,以及task results的存储。可以通过参数 spark.storage.memoryFraction(默认0.6)设置。 3）OtherMemory。给系统预留的，因为程序本身运行也是需要内存的(默认为0.2)。 传统内存管理的不足： 1）Shuffle占用内存0.2*0.8，内存分配这么少，可能会将数据spill到磁盘，频繁的磁盘IO是很大的负担，Storage内存占用0.6，主要是为了迭代处理。传统的Spark内存分配对操作人的要求非常高。（Shuffle分配内存：ShuffleMemoryManager, TaskMemoryManager, ExecutorMemoryManager）一个Task获得全部的Execution的Memory，其他Task过来就没有内存了，只能等待； 2）默认情况下，Task在线程中可能会占满整个内存，分片数据\n说说RDD、DataFrame、DataSet三者的区别与联系 RDD (Spark1.0) —\u0026gt; Dataframe(Spark1.3) —\u0026gt; Dataset(Spark1.6)，如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。\nRDD\nRDD叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表了一个弹性的、不可变的、可分区、里面的元素可进行计算的集合。\nRDD是一个懒执行的不可变的可以支持Lambda表达式的并行数据集合。 RDD的最大好处就是简单，API的人性化程度很高。 RDD的劣势是性能限制，它是一个JVM驻内存对象，这也就决定了存在GC的限制和数据增加时Java序列化成本的升高。\nDataFrame\n与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待，DataFrame也是懒执行的。\nDataFrame性能上比RDD要高，主要有两方面原因：\n定制化内存管理：数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了GC的限制\n优化的执行计划：查询计划通过Spark catalyst optimiser进行优化，为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。对于普通开发者而言，查询优化器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。Dataframe的劣势在于在编译期缺少类型安全检查，导致运行时出错.\nDataFrame 带有schema元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型，便于Spark SQL的操作 支持嵌套数据类型（struct、array和Map），从易用性来说，DataFrame提供的是一套高层的关系操作，比函数式的RDD API更加友好 因为优化的查询执行计划，导致DataFrame执行效率优于RDD RDD 无法得到所存数据元素的具体结构，SparkCore只能在Stage层面进行简单、通用的流水线优化 操作门槛高 DataSet\n1.是Dataframe API的一个扩展，是Spark最新的数据抽象 2.用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。 3.Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。 4.样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。 5.Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为6.Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。 6.DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].\nDataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。\n三者的共性：\n1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利 2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过. 3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出 4、三者都有partition的概念 5、三者有许多共同的函数，如filter，排序等 6、在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持 import spark.implicits._ 7、DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型\n三者的区别\n1、RDD一般和spark mlib同时使用 2、RDD不支持sparksql操作 3、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值 4、DataFrame与Dataset一般不与spark ml同时使用 5、DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作 6、DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然\n介绍一下Spark SQL解析过程 Spark SQL是Apache Spark中的一个模块，用于处理结构化数据。它提供了一个用于查询数据的统一编程接口，并支持SQL查询和DataFrame API。Spark SQL的解析过程包括以下几个步骤：\n输入SQL查询或DataFrame操作 ：用户可以使用SQL语句或DataFrame操作来描述他们想要执行的查询或转换操作。 解析器（Parser） ：Spark SQL首先使用解析器将输入的SQL查询或DataFrame操作转换为逻辑计划。解析器负责将输入的查询语句或操作转换为一个抽象语法树（AST）表示。 语义分析器（Semantic Analyzer） ：在解析完成后，Spark SQL使用语义分析器对AST进行处理。语义分析器负责验证查询语句的语法和语义的正确性，包括检查表和列的存在性、解析函数和表达式，并进行类型检查等。 逻辑优化器（Logical Optimizer） ：在语义分析完成后，Spark SQL使用逻辑优化器对逻辑计划进行优化。逻辑优化器通过应用一系列的规则和转换来优化查询计划，以提高查询性能。例如，它可以进行谓词下推、投影消除、常量折叠等优化操作。 物理优化器（Physical Optimizer） ：在逻辑优化完成后，Spark SQL使用物理优化器对逻辑计划进行进一步的优化。物理优化器负责将逻辑计划转换为物理执行计划，选择最优的物理算子和执行策略。它考虑了数据的分布、数据大小、可用的计算资源等因素来选择最佳的执行计划。 代码生成器（Code Generator） ：在物理优化完成后，Spark SQL使用代码生成器将优化后的物理执行计划转换为可执行的Java字节码。代码生成器负责生成高效的执行代码，以提高查询的执行速度。 执行器（Executor） ：最后，Spark SQL将生成的Java字节码交给执行器执行。执行器负责将查询计划分解为一系列的任务，并在集群上执行这些任务。执行器还负责处理数据的读取、计算和输出等操作，以完成整个查询过程。 总结起来，Spark SQL的解析过程包括解析、语义分析、逻辑优化、物理优化、代码生成和执行等阶段。这些阶段的目标是将用户输入的查询或操作转换为高效的执行计划，并在集群上执行这些计划来处理结构化数据。\n说说Spark的动态资源分配 对于Spark应用来说，资源是影响Spark应用执行效率的一个重要因素。当一个长期运行的服务，若分配给它多个Executor，可是却没有任何任务分配给它，而此时有其他的应用却资源紧张，这就造成了很大的资源浪费和资源不合理的调度。\n简述广播变量和累加器的基本原理和用途 通常情况下，一个传递给 RDD 操作（如map、reduceByKey）的 func 是在远程节点上执行的。函数 func 在多个节点执行过程中使用的变量，是Driver上同一个变量的多个副本。这些变量以副本的方式拷贝到每个task中，并且各task中变量的更新并不会返回 Driver\n为了解决以上问题，Spark 提供了两种特定类型的共享变量 : 广播变量 和 累加器。广播变量主要用于高效分发较大的数据对象，累加器主要用于对信息进行聚合\n广播变量的好处，不需要每个task带上一份变量副本，而是变成每个节点的executor的一份副本。这样的话， 就可以让变量产生的副本大大减少。而且 Spark 使用高效广播算法（BT协议）分发广播变量以降低通信成本\n累加器是 Spark 中提供的一种分布式的变量机制，在Driver端进行初始化，task中对变量进行累加操作\n广播变量典型的使用案例是Map Side Join；累加器经典的应用场景是用来在 Spark 应用中记录某些事件/信息的数量\nSpark的优化怎么做？Spark做过哪些优化？原理是什么 Spark的优化可以从多个方面进行，包括数据存储和压缩、数据分区和分桶、Shuffle操作的优化、内存管理和缓存、并行度和资源配置等。以下是一些常见的Spark优化技术和原理：\n数据存储和压缩 ：使用列式存储格式（如Parquet、ORC）可以减少磁盘IO和内存占用，提高查询性能。同时，使用压缩算法（如Snappy、Gzip）可以减小数据的存储空间，减少磁盘IO和网络传输开销。\n数据分区和分桶 ：合理的数据分区和分桶可以提高数据的局部性，减少Shuffle操作的数据传输量。通过将相同键的数据分配到同一个分区或桶中，可以减少数据的移动和网络传输。\nShuffle操作的优化 ：Shuffle是Spark中常见的开销较大的操作，可以通过使用合适的Shuffle操作算子、调整分区数、合理设置缓存等手段来优化Shuffle的性能。\n调整分区数：合理设置分区数，避免数据倾斜和资源浪费。 使用本地化Shuffle：尽可能将Shuffle数据放置在与计算节点相同的节点上，减少网络传输开销。 使用累加器和广播变量：避免将大量数据通过Shuffle传输，而是通过累加器和广播变量在节点间共享数据。 内存管理和缓存 ：Spark使用内存来加速数据处理，合理配置Spark的内存分配和缓存策略，可以充分利用内存资源，减少磁盘IO，提高查询速度。\n合理设置内存分配比例，如Executor内存和Storage内存的比例。 使用内存序列化：将数据以序列化的方式存储在内存中，减少内存占用和GC开销。 合理使用缓存：将频繁使用的数据缓存到内存中，避免重复计算和IO开销。 并行度和资源配置 ：合理设置并行度和资源配置可以充分利用集群资源，提高作业的执行效率。可以根据数据量、计算复杂度和集群规模等因素来调整并行度和资源分配。\n使用合适的算子和函数 ：选择合适的Spark算子和函数，避免使用性能较差的操作，如全量数据的操作和大量的shuffle操作。\n使用缓存和持久化 ：对于经常被重复使用的数据集，可以使用缓存或持久化技术将其保存在内存或磁盘中，避免重复计算，提高性能。\n使用广播变量 ：对于小规模的数据集，可以将其广播到每个Executor上，减少数据传输开销。\n资源预测和动态资源分配 ：通过资源预测和动态资源分配技术，根据作业的实际需求和集群的资源情况，动态调整资源的分配，提高资源的利用率。\nSpark做出这些优化的目的是为了提高作业的执行效率和性能，减少资源的浪费和开销。这些优化的原理主要是通过减少磁盘IO、网络传输和计算开销，提高数据的局部性和并行度，充分利用内存和缓存等方式来优化Spark的执行过程，从而提高作业的整体性能。\nSpark中Lineage的基本原理 在Spark中，Lineage（血统）是一种记录RDD（弹性分布式数据集）之间依赖关系的机制。每一个 RDD 都是一个不可变的分布式可重算的数据集，其记录着确定性的操作继承关系（ lineage ），所以只要输入数据是可容错的，那么任意一个 RDD 的分区（ Partition ）出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的，它是Spark实现容错性和数据恢复的关键。当我们对RDD进行转换操作时，Spark并不会立即执行这些操作，而是将操作添加到RDD的Lineage中。这样做的好处是，当某个RDD分区数据丢失或节点失败时，Spark可以根据Lineage重新计算丢失的分区，而无需重新计算整个RDD。\nLineage的基本原理如下：\nRDD的创建 ：当我们创建一个RDD时，Spark会将其初始数据划分为一系列的分区，并将这些分区的元数据保存在Lineage中。 转换操作 ：当我们对RDD进行转换操作（如map、filter、reduce等），Spark会根据转换操作的逻辑生成一个新的RDD，并将这个转换操作添加到当前RDD的Lineage中。 宽依赖和窄依赖 ：当一个RDD依赖于多个父RDD时，称为宽依赖；当一个RDD只依赖于一个父RDD时，称为窄依赖。Spark通过这种依赖关系来构建RDD之间的有向无环图（DAG）。 容错性和数据恢复 ：当某个RDD分区数据丢失或节点失败时，Spark可以根据Lineage重新计算丢失的分区。它会遍历RDD的Lineage，根据依赖关系逐级计算缺失的分区，直到达到原始数据的来源。 通过使用Lineage，Spark可以实现容错性和数据恢复。当节点失败或数据丢失时，Spark可以根据Lineage重新计算丢失的数据，而无需重新计算整个RDD，从而提高了计算效率和可靠性。\n如果一个分区出错了：\n对于窄依赖，则只要把丢失的父 RDD 分区重算即可，不依赖于其他分区 对于宽依赖，则父 RDD 的所有分区都需要重算，代价昂贵 所以在长“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点\n首先要明确一下 Spark 中 RDD 的容错机制。每一个 RDD 都是一个不可变的分布式可重算的数据集，其记录着确定性的操作继承关系（ lineage ），所以只要输入数据是可容错的，那么任意一个 RDD 的分区（ Partition ）出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的。如果一个分区出错了：\n对于窄依赖，则只要把丢失的父 RDD 分区重算即可，不依赖于其他分区 对于宽依赖，则父 RDD 的所有分区都需要重算，代价昂贵 所以在长“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点\n谈一谈Spark中的容错机制 Spark是一个分布式计算框架，具备强大的容错机制，以确保在集群中发生故障时能够保持计算的正确性和可靠性。下面是Spark中的几个主要容错机制：\n数据容错 ：Spark通过将数据划分为一系列弹性分布式数据集（RDD）来实现数据容错。RDD是不可变的、可分区的数据集合，它可以在集群中的多个节点上进行并行计算。当节点发生故障时，Spark可以使用RDD的血统（lineage）信息重新计算丢失的数据分区，从而实现数据的容错。 任务容错 ：Spark将计算任务划分为一系列的阶段（stage），每个阶段包含一组可以并行执行的任务。每个任务都会在执行过程中生成一系列的输出数据，这些输出数据会被持久化到稳定存储中。如果任务失败，Spark可以通过重新执行该任务来实现任务的容错。 节点容错 ：Spark通过使用主从架构来实现节点容错。在集群中，有一个主节点（Driver）负责协调任务的执行和结果的收集，以及监控集群中的节点状态。如果主节点发生故障，Spark可以通过重新启动一个新的主节点来实现节点的容错。 任务调度容错 ：Spark使用任务调度器来管理任务的执行顺序和资源分配。如果某个任务在执行过程中发生错误，任务调度器可以重新调度该任务，并将其分配给其他可用的节点进行执行。 数据丢失容错 ：Spark在执行过程中会将数据缓存在内存中，以提高计算性能。为了防止数据丢失，Spark提供了可配置的数据持久化选项，可以将数据持久化到磁盘或其他外部存储系统中。这样即使发生节点故障，数据也可以从持久化存储中恢复。 总的来说，Spark的容错机制主要依赖于RDD的血统信息、任务调度器、主从架构和数据持久化等技术手段，通过这些手段可以实现数据、任务、节点和数据丢失的容错处理，保证计算的正确性和可靠性。\n说说Spark数据倾斜 Spark数据倾斜是指在数据处理过程中，某些数据分区的负载远远超过其他分区，导致计算资源不均衡，从而影响整体性能。数据倾斜可能导致任务执行时间延长，资源浪费，甚至导致任务失败。下面是一些常见的数据倾斜情况和解决方法：\nKey数据倾斜 ：当使用某个字段作为数据集的Key时，如果该字段的分布不均匀，就会导致Key数据倾斜。解决方法包括： 增加分区 ：通过增加数据集的分区数，可以将数据均匀地分布到更多的分区中，减少数据倾斜的可能性。 使用哈希分区 ：使用哈希函数将Key映射到不同的分区，确保数据均匀分布。 聚合合并 ：将具有相同Key的数据进行聚合操作，减少数据量，降低倾斜程度。 数据倾斜的Join操作 ：在进行Join操作时，如果连接字段的分布不均匀，就会导致数据倾斜。解决方法包括： 使用Broadcast Join ：对于小表和大表进行Join操作时，可以将小表广播到每个节点上，避免数据倾斜。 使用随机前缀 ：对于连接字段分布不均匀的情况，可以在连接字段上添加随机前缀，使连接字段的值更均匀分布。 数据倾斜的聚合操作 ：在进行聚合操作时，如果某些分组的数据量远远超过其他分组，就会导致数据倾斜。解决方法包括： 使用预聚合 ：对于可能导致数据倾斜的字段进行预聚合操作，将数据量较大的分组合并为一个分组，减少数据倾斜。 使用多级聚合 ：将聚合操作拆分为多个阶段，逐步聚合，减少单个阶段的数据量。 数据倾斜的排序操作 ：在进行排序操作时，如果数据分布不均匀，就会导致数据倾斜。解决方法包括： 使用随机前缀 ：对于排序字段分布不均匀的情况，可以在排序字段上添加随机前缀，使排序字段的值更均匀分布。 使用二次排序 ：对于多个字段进行排序时，可以使用二次排序算法，先按照一个字段排序，再按照另一个字段排序，减少数据倾斜。 除了上述方法，还可以通过动态资源分配、任务重试、数据重分区等方式来应对数据倾斜问题。数据倾斜是Spark中常见的性能瓶颈之一，需要根据具体情况选择合适的解决方法。\n基本 在源码中是怎么判断属于ShuffleMap Stage或Result Stage的 在Spark 中，任务的执行过程被划分为多个Stage，Stage分为 ShuffleMapStage 和 ResultStage。这两种Stage的划分是根据任务的依赖关系和执行目标来确定的。\nShuffleMapStage：负责为后续的 Shuffle 操作准备数据，输出是 Shuffle 数据，供下游阶段使用，通常对应宽依赖。\nResultStage：负责执行最终的计算并输出结果，输出是最终的计算结果，通常对应Action操作。\n在 Spark 源码中，阶段划分的核心逻辑位于 DAGScheduler 类中\nStage创建阶段：入口方法是DAGScheduler.handleJobSubmitted，在 DAGScheduler.createResultStage 和 DAGScheduler.getOrCreateShuffleMapStage 方法中创建阶段。\n判断阶段：如果任务是最终的计算任务（如 collect、count、saveAsTextFile 等），则创建 ResultStage；如果任务是为 Shuffle 操作准备数据（如 groupByKey、reduceByKey、join 等），则创建 ShuffleMapStage；\nSpark join在什么情况下会变成窄依赖 数据本地性是在哪个环节确定的 Spark的数据本地性有哪几种 Spark中的数据本地性有三种： 1）PROCESS_LOCAL是指读取缓存在本地节点的数据 2）NODE_LOCAL是指读取本地节点硬盘数据 3）ANY是指读取非本地节点数据 通常读取数据PROCESS_LOCAL\u0026gt;NODE_LOCAL\u0026gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中。\n说说Spark提交作业参数 提交作业时的重要参数：\nexecutor_cores 缺省值：1 in YARN mode, all the available cores on the worker in standalone and Mesos coarse-grained modes 生产环境中不宜设置为1！否则 work 进程中线程数过少，一般 2~5 为宜 executor_memory 缺省值：1g 该参数与executor分配的core有关，分配的core越多 executor_memory 值就应该越大； core与memory的比值一般在 1:2 到 1:4 之间，即每个core可分配2~4G内存。如 executor_cores 为4，那么executor_memory 可以分配 8G ~ 16G； 单个Executor内存大小一般在 20G 左右（经验值），单个JVM内存太高易导致GC代价过高，或资源浪费 executor_cores * num_executors 表示的是能够并行执行Task的数目。不宜太小或太大！理想情况下，一般给每个core分配 2-3 个task，由此可反推 num_executors 的个数 driver-memory driver 不做任何计算和存储，只是下发任务与yarn资源管理器和task交互，一般设为 1-2G 即可； 增加每个executor的内存量，增加了内存量以后，对性能的提升，有三点：\n如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘，甚至不写入磁盘。减少了磁盘IO 对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存，会减少磁盘的写入操作，进而提升性能 task的执行，可能会创建很多对象。如果内存比较小，可能会频繁导致JVM堆内存满了，然后频繁GC，垃圾回收，minor GC和full GC。内存加大以后，带来更少的GC，垃圾回收，避免了速度变慢，性能提升； Spark的宽窄依赖，以及Spark如何划分Stage，如何确定每个Stage中Task个数 RDD之间的依赖关系分为窄依赖（narrow dependency）和宽依赖（Wide Depencency，也称为Shuffle Depencency）\n窄依赖 ：指父RDD的每个分区只被子RDD的一个分区所使用，子RDD分区通常对应常数个父RDD分区（O(1)，与数据规模无关） 宽依赖 ：是指父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应所有的父RDD分区（O(n),与数据规模有关） 相比于宽依赖，窄依赖对优化很有利，主要基于以下几点：\n宽依赖往往对应着Shuffle操作，需要在运行过程中将同一个父RDD的分区传入到不同的子RDD分区中，中间可能涉及多个节点之间的数据传输；而窄依赖的每个父RDD的分区只会传入到一个子RDD分区中，通常可以在一个节点内完成转换 当RDD分区丢失时（某个节点故障），Spark会对数据进行重算 对于窄依赖，由于父RDD的一个分区只对应一个子RDD分区，这样只需要重算和子RDD分区对应的父RDD分区即可，所以这个重算对数据的利用率是100%的 对于宽依赖，重算的父RDD分区对应多个子RDD分区的，这样实际上父RDD中只有一部分的数据是被用于恢复这个丢失的子RDD分区的，另一部分对应子RDD的其他未丢失分区，这就造成了多余的计算；更一般的，宽依赖中子RDD分区通常来自多个父RDD分区，所有的父RDD分区都要进行重新计算 Stage：根据RDD之间的依赖关系将Job划分成不同的Stage，遇到一个宽依赖则划分一个Stage。\nTask：Stage是一个TaskSet，将Stage根据分区数划分成一个个的Task\nAQE是什么 Adaptive Query Execution (AQE) 就像 SparkSQL的智能助手，它解决了查询计划不那么完美的问题，它通过在sparksSQL查询运行时从去从数据中去学习并通过算法模型优化来实现这一点。AQE不依赖于数据的固定信息，而是在查询展开时收集实时细节。通过这样做，AQE 可以对查询计划进行即时改进，确保其运行更高效，确保最终的结果尽可能快捷。\n动态合并Shuffle分区 当 Spark 处理大型数据集时，它通常需要通过网络重新组织或打乱数据以执行某些操作，例如ioin连接或聚合。这个shuffle过程涉及到将数据划分为多个分区，而这个分区的效率极大地影响了查询性能，而AQE 在查询运行时动态调整随机分区的数量，假设您有一个大数据集，最初，Spark 决定使用一定数量的分区进行shuffle。这个数字可能很难得到正确的值，因为如果它太低，每个分区可能太大，导致处理速度慢。另一方面，如果它太高，您会得到很多小分区，导致数据获取效率低下并增加开销。\n如果没有 AQE，Spark 可能会决定在本地分组操作后创建五个分区。然而，AQE 很聪明。它注意到其中三个分区非常小。AQE 不会通过单独处理每个小分区来浪费资源，而是将这些小分区合并 (组合) 为一个。现在，最终的聚合只需要对三个任务执行，而不是五个，使得查询更加高效。在后台，AQE 动态调整随机分区的数量并根据需要合并它们，从而优化查询执行计划\n动态切换Join策略\n当您在 Spark 中执行join连接时，可以采用不同的方法来组合数据。最有效的方法之一称为广播哈希join (broadcast hash join)其中连接的一侧足够小以适合放到内存里，从而使Join过程更快，类似hive中的mapjoin。现在，假设您正在运行join连接操作，并且最初，Spark 计划根据其对数据大小的估计使用某种连接策略。这就是 AQE 的用武之地。它不会盲目地坚持最初的计划。相反，它会在查询运行时关注数据的实际大小，动态调整join策略。\n如果在执行过程中，AQE 注意到join一侧的数据大小比最初估计的要小得多，它可以动态地将ioin连接策略切换为更高效的broadcasthash join。这一点至关重要，因为有时初始化时估计可能会由于各种因素而出现偏差，例如各种filter或一组复杂的操作后数据量明显变小\nSpark 计划使用sort merge join ，刚开始以为是大表Join。然而，AQE 观察到运行时的实际大小要小得多，这使得使用广播哈希连接更加高效。因此，AQE 可以智能地切换连接策略来优化性能。现在我们开启AQE后，运行涉及join的查询时，AQE 将根据实际数据大小动态调整联接策略，确保最佳性能。\n动态优化倾斜连接\n我们知道当分区之间的数据分布不均匀时，就会出现数据倾斜。在 Spark 集群中，数据被划分为多个分区以进行并行处理。理想情况下每个分区的数据量大致相等，确保所有处理任务的工作负载均衡。然而，在数据倾斜的情况下，一个或几个分区可能包含比其他分区多得多的数据。数据倾斜对查询性能的影响: 任务相当存在数据倾斜时，与处理较少数据比，与倾斜分区相关的处理任务而要更长的时间才能完成。这种不平衡可能会导致性能瓶颈，因为某些任务会成为整个操作的瓶颈。这在join连接期间尤其成问题，其中涉及倾斜分在join连接期间尤其成问题，其中涉及倾斜分区的任务可能会滞后，从而降低整体查询性能。AQE通过其倾斜连接优化来解决这个问题AQE在运行时通过分析shuffle文件统计信息自动检测数据倾斜何时影响查询性能。一旦识别出倾斜，AQE就会动态调整方法。分割倾斜分区:AQE 不会让一些负载较重的分区成为处理瓶颈，而是智能地将这些倾斜的分区拆分为更小的子分区。现在，每个子分区包含更均衡的数据量，防止工作负载集中在少数任务上假设一个jin连接操作，其中join连接的一侧有一个分区 (我们称之为 P1) ，其中的数据明显多于其他分区:在 AQE 倾斜连接优化之前: 如果没有 AQEjoin连接操作可能会因负载过重的 P1分区而变慢，从而导致性能问题在AQE倾斜连接优化之后:AQE检测到倾斜并动态地将P1分区分割成更小、更均衡的子分区现在，工作负载的分布更加均匀，从而提高了查询性能。\n总结 Spark 3.0中的AQE标志着查询优化的重大飞跃。通过减少对静态统计的依赖和解决运行时的挑战，AQE增强了Spark对不同数据条件的适应性。三个关键特性一合并shuffle分区、切换JOIN策略和优化倾斜连接。有助于大幅提高性能，使Spark SQL在各种工作负载下更具弹性和效率。随着AQE成为标准特性，用户可以减少很多手动调优技能的输出，降低使用成本。\nSpark累加器有哪些特点 Spark累加器是一种用于在分布式计算中进行累加操作的特殊变量。以下是Spark累加器的几个特点：\n分布式计算 ：Spark累加器可以在分布式环境中进行并行计算。它可以在集群中的多个节点上并行更新和累加值。 只写 ：累加器的值只能增加，不能减少或修改。它们用于收集和聚合分布式计算中的统计信息。 容错性 ：Spark累加器具有容错性，即使在节点故障的情况下也可以保持正确的计算结果。如果节点失败，Spark可以自动重新计算丢失的部分。 分布式共享变量 ：累加器是一种分布式共享变量，可以在分布式任务之间共享和传递。这使得可以在不同的任务中更新和使用累加器的值。 惰性计算 ：Spark累加器是惰性计算的，只有在执行动作操作时才会真正计算累加器的值。这样可以减少不必要的计算开销。 总之，Spark累加器是一种在分布式环境中进行累加操作的特殊变量，具有分布式计算、容错性和惰性计算等特点。它们在统计和聚合分布式计算中非常有用。\nSpark使用parquet文件存储格式能带来哪些好处 Spark使用Parquet文件存储格式可以带来以下几个好处：\n高效的压缩和编码 ：Parquet文件采用了列式存储的方式，将同一列的数据存储在一起，可以更好地利用压缩算法和编码方式。这样可以大大减小数据的存储空间，降低磁盘IO和网络传输的开销。 列式存储和投影扫描 ：Parquet文件的列式存储方式使得Spark可以只读取需要的列，而不必读取整个数据集。这种投影扫描的方式可以大大减少IO和内存的开销，提高查询性能。 谓词下推和统计信息 ：Parquet文件存储了每个列的统计信息，包括最小值、最大值、空值数量等。Spark可以利用这些统计信息进行谓词下推，即在读取数据时根据查询条件过滤掉不符合条件的数据，减少数据的读取量和处理量。 列式压缩和编码 ：Parquet文件支持多种压缩算法和编码方式，例如Snappy、Gzip、LZO等。Spark可以根据需求选择适合的压缩算法和编码方式，以平衡存储空间和查询性能。 数据模式和架构演化 ：Parquet文件存储了数据的模式信息，包括列名、数据类型、架构等。这使得Spark可以在读取数据时自动推断数据的模式，而不需要提前定义模式。同时，Parquet文件还支持架构演化，即在数据更新时可以增加、删除或修改列，而不需要重新创建整个数据集。 综上所述，Spark使用Parquet文件存储格式可以提供高效的压缩和编码、列式存储和投影扫描、谓词下推和统计信息、列式压缩和编码，以及数据模式和架构演化等好处，从而提高数据存储和查询的性能。\nSparkSQL常用哪些算子 SparkSQL是Spark的一个模块，它提供了一套用于处理结构化数据的高级API。以下是SparkSQL中常用的一些算子：\nselect ：用于选择指定的列或表达式。 filter ：用于根据指定的条件过滤数据。 groupBy ：用于按照指定的列进行分组。 orderBy ：用于按照指定的列对数据进行排序。 join ：用于连接两个或多个数据集。 union ：用于合并两个或多个数据集。 distinct ：用于去重，返回唯一的数据。 limit ：用于限制返回的数据量。 agg ：用于对数据进行聚合操作，如求和、平均值等。 window ：用于执行窗口函数操作，如滑动窗口、滚动窗口等。 withColumn ：用于添加新的列或替换现有列。 show ：用于显示数据集的内容。 cache/persist ：用于将数据集缓存到内存或磁盘，以便后续快速访问。 explain ：用于查看执行计划。 createTempView ：用于将DataFrame注册为临时表，以便进行SQL查询。 这些算子可以通过DataFrame API或SQL语句来使用。使用这些算子，可以方便地进行数据的筛选、转换、聚合等操作，以满足不同的数据处理需求。\nSpark有哪两种算子 Spark有两种主要的算子：转换算子（Transformation）和动作算子（Action）。\n转换算子（Transformation） ：转换算子是指对RDD（弹性分布式数据集）进行转换操作的算子。转换算子不会立即执行，而是生成一个新的RDD。常见的转换算子包括 map、filter、flatMap、reduceByKey等。这些算子可以用于对RDD进行各种操作和变换，例如对每个元素进行映射、过滤、扁平化等。 动作算子（Action） ：动作算子是指对RDD进行实际计算并返回结果的算子。动作算子会触发Spark作业的执行，并返回一个结果或将结果保存到外部存储系统中。常见的动作算子包括 count、collect、reduce、saveAsTextFile等。这些算子会触发Spark的执行过程，对RDD进行计算并返回结果。 通过转换算子和动作算子的组合，可以构建复杂的数据处理流程，并在需要时触发计算并获取结果。这种延迟计算的特性使得Spark能够进行高效的数据处理和分布式计算。\n对于Spark中的数据倾斜问题你有什么好的方案 在Spark中，数据倾斜是指在数据处理过程中，某些分区的数据量远远超过其他分区，导致任务执行时间延长或者任务失败。解决数据倾斜问题可以采取以下几种方案：\n随机前缀 ：对于可能导致数据倾斜的键进行随机前缀处理，将原本可能集中在某个分区的数据均匀分散到多个分区中。这种方法可以通过在键前添加随机字符串或者随机数来实现。 扩容分区 ：对于数据倾斜的分区，可以考虑将其拆分成多个小分区，以增加并行度。可以使用repartition或者coalesce方法来重新分区。 聚合优化 ：对于需要进行聚合操作的场景，可以先对数据进行预聚合，将数据量减少到一个可接受的范围，然后再进行全局聚合。 广播变量 ：如果数据倾斜是由于某个较小的数据集引起的，可以将该数据集使用广播变量的方式分发到所有的Executor上，避免数据重复加载。 分桶和排序 ：对于连接操作或者聚合操作中的数据倾斜问题，可以通过对数据进行分桶和排序来解决。将数据按照某个特定的列进行分桶，然后在连接或者聚合时，只对相同桶中的数据进行操作。 动态调整资源 ：如果在任务执行过程中发现某个分区的数据量过大，可以动态调整资源，增加该分区的处理能力，以加快任务的执行速度。 以上是一些常见的解决数据倾斜问题的方法，具体应该根据实际情况选择合适的方案。在实际应用中，可能需要结合多种方法来解决复杂的数据倾斜问题。\nSpark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景 Spark 技术栈包含以下几个核心组件：\nSpark Core ：Spark 的基础组件，提供了任务调度、内存管理和错误恢复等功能。它还定义了 RDD（Resilient Distributed Datasets）数据结构，用于在集群上进行分布式计算。 Spark SQL ：用于处理结构化数据的组件，支持使用 SQL 查询数据。它提供了 DataFrame 和 Dataset 两个 API，可以方便地进行数据处理和分析。适合处理大规模的结构化数据。 Spark Streaming ：用于实时数据处理的组件，可以将实时数据流划分为小批次进行处理。它支持各种数据源，如 Kafka、Flume 和 HDFS，并提供了窗口操作和状态管理等功能。适合实时数据分析和流式处理。 Spark MLlib ：用于机器学习的组件，提供了常见的机器学习算法和工具。它支持分类、回归、聚类和推荐等任务，并提供了特征提取、模型评估和模型调优等功能。适合大规模的机器学习任务。 Spark GraphX ：用于图计算的组件，提供了图结构的抽象和常见的图算法。它支持图的构建、遍历和计算，并提供了图分析和图挖掘等功能。适合社交网络分析和图计算任务。 SparkR ：用于在 R 语言中使用 Spark 的组件，提供了在 R 中进行分布式计算和数据处理的能力。它支持使用 R 语法进行数据操作，并提供了与 Spark SQL 和 MLlib 的集成。适合 R 语言用户进行大规模数据处理和分析。 这些组件可以根据具体的应用场景进行组合和使用，例如：\n如果需要处理大规模的结构化数据，并进行复杂的数据分析和查询操作，可以使用 Spark SQL。 如果需要进行实时数据处理和流式计算，可以使用 Spark Streaming。 如果需要进行大规模的机器学习任务，可以使用 Spark MLlib。 如果需要进行图计算和图分析，可以使用 Spark GraphX。 如果需要在 R 语言中进行分布式计算和数据处理，可以使用 SparkR。 总的来说，Spark 技术栈提供了一套强大的工具和组件，可以满足不同场景下的大规模数据处理和分析需求。\nSpark是一个快速、通用的大数据处理框架，它提供了丰富的核心组件和功能，用于处理和分析大规模数据集。下面是Spark的核心组件及其功能的详细介绍：\nSpark Core ：Spark的核心组件，提供了分布式任务调度、内存管理和错误恢复等基本功能。它还定义了RDD（弹性分布式数据集）的概念，RDD是Spark中的基本数据结构，用于表示可并行处理的数据集。 Spark SQL ：用于处理结构化数据的模块，支持SQL查询和DataFrame API。它可以将数据从多种数据源（如Hive、Avro、Parquet等）加载到Spark中，并提供了强大的查询优化和执行功能。 Spark Streaming ：用于实时数据流处理的模块，支持高吞吐量的数据流处理和复杂事件处理。它可以将实时数据流分成小批次，并在每个批次上应用批处理操作。 MLlib ：Spark的机器学习库，提供了常见的机器学习算法和工具，如分类、回归、聚类、推荐等。它支持分布式训练和推理，并提供了丰富的特征提取、转换和选择功能。 GraphX ：用于图计算的模块，支持图的创建、操作和分析。它提供了一组高级图算法和操作符，可以用于社交网络分析、推荐系统等领域。 SparkR ：用于在R语言中使用Spark的接口。它提供了与Spark Core和Spark SQL的集成，可以在R中使用Spark的分布式计算和数据处理功能。 除了以上核心组件，Spark还提供了一些其他功能和工具，如：\nSpark Streaming可以与Kafka、Flume等数据源集成，实现实时数据流处理。 Spark可以与Hadoop、Hive、HBase等大数据生态系统中的其他工具和框架无缝集成。 Spark提供了交互式Shell（spark-shell）和Web界面（Spark UI）等开发和监控工具。 Spark支持在本地模式、独立模式和集群模式下运行，可以根据需求进行灵活部署和扩展。 描述以下算子的区别与联系 一、groupByKey、reduceByKey、aggreageByKey\nreduceByKey ：可以在每个分区移动数据之前将输出数据与一个共用的 key结合，适用于大数据集计算\ngroupByKey ：所有的键值对(key-value pair) 都会被移动,在网络上传输这些数据非常没必要，因此避免使用 groupByKey\naggreageByKey ：\n二、cache、presist\n都用于将RDD进行缓存\ncache ：只有一个默认的缓存级别MEMORY_ONLY\npresist ：persist有一个 StorageLevel 类型的参数，总共有12种缓存级别\n三、repartition、coalesce\n都是RDD的分区进行重新划分\nrepartition ：repartition只是coalesce接口中shuffle为true的实现\ncoalesce ：如果shuff为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDD的分区数变多的\n四、map、flatMap\nmap ：将函数用于RDD中的每个元素，将返回值构成新的RDD\nflatMap ：将函数应用于RDD中的每个元素，将返回的迭代器的所有内容构成新的RDD\nSpark算子可分为两类算子，区别是什么，分别举例 Spark的算子可以分为两类：Transformation、Action\nTransformation ：从现有的数据集创建一个新的数据集，返回一个新的 RDD 操作。Transformation都是惰性的，它们并不会立刻执行，只是记住了这些应用到 RDD 上的转换动作 Action ：触发在 RDD 上的计算，这些计算可以是向应用程序返回结果，也可以是向存储系统保存数据 Transformation 最重要的特点：延迟执行、返回 RDD\nAction最重要的特点：触发 Job ，返回的结果一定不是 RDD\n常见的 Transformation 包括：map、mapVaules、filter、flatMap、mapPartitions、uoin、join、distinct、xxxByKey 常见的 Action 包括：count、collect、collectAsMap、first、reduce、fold、aggregate、saveAsTextFile 有Shuffle的 Transformation 包括：\n一堆的 xxxByKey（sortBykey、groupByKey、reduceByKey、foldByKey、aggreageByKey、combineByKey）。备注：不包括countByKey join相关（join、leftOuterJoin、rightOuterJoin、fullOuterJoin、cogroup） distinct、intersection、subtract、partionBy、repartition 说说你对Spark分区器的理解 Spark分区器是用于将数据集划分为多个分区的组件。分区器决定了数据在集群中的分布方式，对于数据处理和计算的效率具有重要影响。\nSpark提供了多种分区器，其中常见的有哈希分区器（HashPartitioner）和范围分区器（RangePartitioner）。\n哈希分区器根据键的哈希值将数据均匀地分布到不同的分区中。它适用于键的分布相对均匀的情况，可以保证相同键的数据被分配到同一个分区，从而方便后续的聚合操作。\n范围分区器根据键的大小范围将数据划分到不同的分区中。它适用于键的分布有序的情况，可以保证相邻键的数据被划分到相邻的分区，从而提高了数据的局部性，有利于后续的数据处理操作。\n除了这两种常见的分区器，Spark还提供了自定义分区器的接口，用户可以根据自己的需求实现自己的分区策略。\n分区器的选择对于Spark作业的性能至关重要。一个合适的分区策略可以使得数据在集群中的负载均衡，减少数据的传输和重组开销，提高作业的并行度和执行效率。因此，在使用Spark时，需要根据数据的特点和作业的需求选择合适的分区器来优化作业的性能。\nSpark Streaming 中有哪些消费Kafka数据的方式，它们之间的区别是什么 在Spark Streaming中，有两种主要的方式可以消费Kafka数据：直接方式（Direct Approach）和接收器方式（Receiver Approach）。\n直接方式（Direct Approach）： 使用 createDirectStream方法从Kafka直接读取数据。 Spark Streaming会直接连接到Kafka的分区，每个Spark分区都会对应一个Kafka分区。 消费者的偏移量由应用程序自己管理，通常与外部存储（如ZooKeeper）结合使用。 可以实现端到端的一次性语义（Exactly-once semantics）。 接收器方式（Receiver Approach）： 使用 createStream方法创建一个接收器（Receiver）来接收Kafka数据。 接收器是一个独立的线程，负责从Kafka主题中接收数据。 接收到的数据会存储在Spark的内存中，并由Spark Streaming进行处理。 消费者的偏移量由接收器自动管理，并定期保存到检查点（checkpoint）中。 无法实现端到端的一次性语义，可能会有数据丢失或重复。 两种方式之间的区别主要在于数据的接收和处理方式，以及语义保证的能力。\n直接方式具有更低的延迟和更高的吞吐量，因为它直接连接到Kafka分区，避免了接收器线程的开销。同时，直接方式可以实现端到端的一次性语义，确保数据的精确处理。\n接收器方式相对简单，适用于较低的数据吞吐量和较高的延迟容忍度。它使用接收器线程将数据存储在Spark的内存中，并由Spark Streaming进行处理。但是，由于接收器的存在，可能会引入一些额外的延迟，并且无法提供端到端的一次性语义。\n根据具体的需求和应用场景，可以选择适合的方式来消费Kafka数据。如果对延迟和一次性语义有较高的要求，直接方式是更好的选择；如果对延迟和一次性语义要求较低，接收器方式可能更简单方便。\n说说对Spark Shuffle的理解 Spark中的Shuffle是指在数据重分区（Data Redistribution）的过程中，将数据从一个或多个输入分区重新分布到新的输出分区的操作。Shuffle是Spark中一种非常重要的操作，它在许多转换操作（如groupByKey、reduceByKey和join等）中都会被使用到。\nShuffle的原理可以分为三个主要步骤：Map阶段、Shuffle阶段和Reduce阶段。\n1. Map阶段 ：\n在Map阶段，Spark将输入数据按照指定的键（key）进行分组，形成多个（key, value）对。每个（key, value）对被称为一个记录（record）。 每个记录被分配到对应的分区（Partition），分区的数量可以通过配置参数 spark.sql.shuffle.partitions进行设置，默认为200个分区。分区的数量决定了并行度和任务的负载均衡。 2. Shuffle阶段 ：\n在Shuffle阶段，Spark会将每个分区的数据按照键（key）进行排序，并将相同键的记录聚合在一起，形成多个（key, list of values）对。这个过程称为排序和聚合（Sort and Aggregation）。 排序和聚合的目的是将相同键的记录放在同一个分区中，以便后续的Reduce阶段可以更高效地进行计算。 3. Reduce阶段 ：\n在Reduce阶段，Spark将每个（key, list of values）对分发给不同的Executor节点上的Reduce任务进行处理。 Reduce任务会对每个键（key）对应的值列表进行迭代，并对值进行合并、计算或聚合等操作，生成最终的结果。 Shuffle过程中的数据传输是通过网络进行的，因此Shuffle的性能对Spark的性能影响很大。为了提高Shuffle的性能，Spark引入了多种优化技术，包括：\n压缩：可以使用压缩算法对Shuffle数据进行压缩，减少网络传输的数据量。 聚合缓冲区（Aggregation Buffer）：在Shuffle阶段，可以使用聚合缓冲区对相同键的记录进行缓存，减少网络传输和磁盘读写的次数。 基于Sort的Merge：在Reduce阶段，可以使用基于Sort的Merge算法对具有相同键的记录进行合并，减少计算的次数。 通过合理配置Shuffle相关的参数，使用适当的优化技术，可以提高Shuffle的性能，从而提升Spark应用程序的整体性能和可扩展性。\n说说Spark的优化 Spark是一个基于内存的分布式计算框架，为了提高计算性能和资源利用率，它采用了多种优化技术。下面是Spark的一些优化策略：\n延迟执行（Lazy Evaluation） ：Spark采用了延迟执行的策略，也就是说，它在遇到转换操作时不会立即执行，而是将这些操作记录下来，直到遇到行动操作时才执行。这样可以将多个转换操作合并为一个任务，减少数据的读写和中间结果的存储。 数据分区（Data Partitioning） ：Spark将数据划分为多个分区，每个分区可以在集群中的不同节点上并行处理。通过合理的数据分区策略，可以减少数据的传输和网络开销，提高计算效率。 内存管理（Memory Management） ：Spark使用了内存管理技术来加速数据的处理。它将数据存储在内存中，减少了磁盘IO的开销。同时，Spark还使用了内存缓存和数据序列化等技术来提高内存利用率和数据传输效率。 任务调度（Task Scheduling） ：Spark使用了任务调度器来将任务分配给集群中的不同节点进行执行。它可以根据节点的负载情况和数据位置等因素来决定任务的调度顺序，以提高资源利用率和数据本地性。 数据本地性优化（Data Locality Optimization） ：Spark会尽量将任务调度到与数据所在位置相近的节点上执行，以减少数据的网络传输。它可以通过数据划分和任务调度策略来实现数据本地性的优化。 广播变量（Broadcast Variable） ：当需要在集群中的所有节点上共享一个较小的只读变量时，Spark可以使用广播变量来减少数据的传输和复制开销。广播变量会将变量的副本缓存在每个节点上，以便任务可以直接访问，而不需要从驱动程序传输。 部分聚合（Partial Aggregation） ：在进行聚合操作时，Spark可以在每个分区上进行部分聚合，然后再将结果合并起来。这样可以减少数据的传输和聚合的计算量，提高聚合操作的性能。 动态资源分配（Dynamic Resource Allocation） ：Spark可以根据任务的需求动态分配集群资源。它可以根据任务的执行情况来调整资源的分配，以提高资源的利用率和整体性能。 这些优化策略使得Spark能够高效地处理大规模数据，并提供快速的计算和查询能力。同时，Spark还提供了丰富的配置选项和调优参数，可以根据应用的需求进行定制化的优化。\n说说Spark工作机制 Spark是一个开源的分布式计算框架，它提供了高效的数据处理和分析能力。Spark的工作机制可以分为以下几个关键步骤：\n应用程序启动 ：用户编写Spark应用程序，并通过SparkContext对象与Spark集群进行通信。SparkContext负责与集群管理器通信，获取资源并分配任务。 任务划分 ：Spark将应用程序划分为一系列任务，每个任务处理数据的一部分。任务的划分是根据数据分区和用户定义的转换操作来完成的。 数据分区 ：Spark将输入数据划分为多个分区，每个分区包含数据的一个子集。分区可以是文件、HDFS块或其他数据源。 任务调度 ：Spark将任务分发到集群中的执行器节点上执行。任务调度器负责将任务分配给可用的执行器节点，并考虑数据本地性以提高性能。 任务执行 ：每个执行器节点接收到任务后，会在本地执行相应的任务操作。执行器会将数据加载到内存中，并执行用户定义的转换和操作。 数据流动 ：Spark的核心概念是弹性分布式数据集（Resilient Distributed Dataset，简称RDD）。RDD是一个可并行操作的对象集合，可以在集群中的多个节点上进行转换和操作。数据在RDD之间流动，允许在不同的转换操作之间进行缓存和重用。 任务完成与结果返回 ：一旦任务执行完成，执行器将结果返回给驱动程序。驱动程序可以将结果保存到内存、磁盘或其他外部存储中，也可以将结果返回给应用程序。 容错和恢复 ：Spark具有容错机制，可以在节点故障时自动恢复。如果某个节点失败，Spark可以重新调度任务并在其他节点上执行。 总的来说，Spark的工作机制包括任务划分、任务调度、任务执行和数据流动等过程。通过这些步骤，Spark能够高效地处理大规模数据，并提供快速的分布式计算能力。\n说说Spark Job执行流程？spark的执行流程？简要描述Spark写数据的流程？ Spark Job的执行流程可以简要描述为以下几个步骤：\n创建SparkContext ：在执行Spark应用程序之前，首先需要创建一个SparkContext对象。SparkContext是与集群通信的入口点，它负责与集群管理器进行通信，分配资源，并将任务分发给集群中的执行器。 创建RDD ：在Spark中，数据被组织成弹性分布式数据集（RDD）。RDD可以从外部存储系统（如HDFS）中读取数据，也可以通过对已有RDD进行转换操作来创建。 转换操作 ：Spark提供了一系列的转换操作，如map、filter、reduce等。这些操作可以对RDD进行转换，生成一个新的RDD。转换操作是惰性求值的，即不会立即执行，而是在遇到一个行动操作时才会触发执行。 行动操作 ：行动操作是对RDD进行实际计算并返回结果的操作，例如collect、count、reduce等。当执行行动操作时，Spark将根据依赖关系图（DAG）将转换操作和行动操作组织成一个有向无环图（DAG），并将其划分为一系列的阶段。 任务调度 ：Spark将每个阶段划分为一组任务，并将这些任务分发给集群中的执行器进行执行。任务调度是根据数据的分区进行的，每个任务处理一个或多个分区的数据。 任务执行 ：每个执行器接收到任务后，会在其分配的资源上执行任务。执行器将数据从内存或磁盘中读取到内存中，并执行相应的转换和行动操作。执行结果可以保存在内存中或写入外部存储系统。 Spark写数据的流程可以简要描述为以下几个步骤：\n创建DataFrame或RDD ：首先，需要创建一个包含要写入的数据的DataFrame或RDD对象。DataFrame是一种结构化的数据集，RDD是弹性分布式数据集。 指定写入选项 ：根据要写入的数据的格式和目标存储系统，可以指定相应的写入选项，如文件格式、文件路径、分区方式等。 执行写入操作 ：调用DataFrame或RDD的写入方法，如 write或 save，并传入写入选项。Spark会将数据按照指定的格式和方式写入到目标存储系统中。 等待写入完成 ：写入操作是异步的，Spark会在后台执行写入任务。可以使用 awaitTermination等方法等待写入任务完成。 需要注意的是，Spark的写入操作是幂等的，即可以多次执行相同的写入操作而不会导致数据重复写入。\n说说Spark的运行模式，简单描述WC Spark有两种运行模式：本地模式和集群模式。\n本地模式（Local Mode） ：在本地模式下，Spark运行在单个计算机上，使用单个进程进行任务的执行。这种模式适用于开发和测试阶段，可以快速验证代码的正确性。 集群模式（Cluster Mode） ：在集群模式下，Spark将任务分发给多个计算节点进行并行处理。集群模式适用于大规模数据处理和生产环境。集群模式有两种部署方式： Standalone模式：在Standalone模式下，Spark自带了一个集群管理器，可以通过启动和配置Master和Worker节点来管理集群资源和任务调度。 YARN模式：YARN是Hadoop的资源管理系统，Spark可以通过YARN来管理集群资源。Spark作为YARN的一个应用程序提交到集群中，YARN负责资源的分配和任务的调度。 WC（WordCount）是一个经典的示例程序，用于统计文本中每个单词的出现次数。在Spark中，可以使用以下步骤实现WordCount：\n创建SparkContext ：首先，需要创建一个SparkContext对象，作为与Spark集群通信的入口点。 加载文本数据 ：使用SparkContext的 textFile方法加载文本文件，并将其转换为RDD（弹性分布式数据集）。 数据转换 ：对RDD进行一系列转换操作，例如使用 flatMap方法将每行文本拆分为单词，使用 map方法将每个单词映射为(key, value)对，其中key是单词，value是1。 数据聚合 ：使用 reduceByKey方法对(key, value)对进行聚合操作，将相同key的value相加。 结果输出 ：使用 collect方法将聚合结果收集到Driver程序，并输出结果。 这就是Spark中简单的WordCount示例程序。通过并行处理和分布式计算，Spark能够高效地处理大规模数据集。\n如何理解Standalone模式下，Spark资源分配是粗粒度的？ 在Standalone模式下，Spark的资源分配是粗粒度的，这意味着资源的分配单位是整个应用程序，而不是每个任务或每个操作。在Standalone模式下，Spark应用程序被划分为一个或多个独立的执行器（Executors），每个执行器运行在独立的JVM进程中，并且可以分配一定数量的CPU核心和内存资源。\n当一个Spark应用程序提交到Standalone集群时，用户需要指定应用程序需要的总体资源需求，例如CPU核心数和内存大小。然后，Spark的资源管理器将根据这些需求来分配执行器，并将应用程序的任务分配给这些执行器。每个执行器被分配的资源是固定的，直到应用程序完成或释放资源。\n由于资源分配是以整个应用程序为单位进行的，因此在Standalone模式下，无法根据任务的实际需求进行细粒度的资源分配。这可能导致资源的浪费或不足，影响应用程序的性能。为了更好地利用资源，可以考虑使用其他资源管理器，如YARN或Mesos，它们支持更细粒度的资源分配和共享，可以根据任务的需求进行动态的资源分配。\nSpark中standalone模式特点，有哪些优点和缺点？ 在Spark的standalone模式中，以下是其主要特点、优点和缺点：\n特点：\n独立性 ：Spark standalone模式是Spark自带的资源管理器，可以独立于其他资源管理系统运行，如Hadoop YARN或Mesos。 简单易用 ：相对于其他资源管理系统，Spark standalone模式配置和使用相对简单，适合初学者或小规模集群。 高可用性 ：Spark standalone模式支持主备模式，即可以配置一个或多个备用的主节点，以提高系统的可用性。 优点：\n效率高 ：Spark standalone模式的资源分配是粗粒度的，以整个应用程序为单位进行分配，减少了资源管理的开销，提高了任务的执行效率。 简化部署 ：使用Spark standalone模式，无需依赖其他资源管理系统，可以快速部署和运行Spark应用程序。 灵活性 ：Spark standalone模式支持动态资源分配，可以根据应用程序的需求在运行时调整资源的分配情况。 缺点：\n资源浪费 ：由于资源分配是以整个应用程序为单位进行的，可能导致资源的浪费。如果应用程序中某些任务的资源需求较小，但被分配了较多的资源，会导致资源的浪费。 资源不足 ：如果应用程序的资源需求超过了集群的可用资源，可能会导致资源不足，影响任务的执行。 缺乏细粒度调整 ：由于资源分配是粗粒度的，无法对每个任务进行细粒度的资源调整，可能导致资源的利用率不高。 综上所述，Spark standalone模式在简单易用和高效性方面具有优势，但在资源利用和灵活性方面存在一些限制。选择使用该模式还需根据具体应用程序的特点和需求进行考虑。\n简要描述Spark分布式集群搭建的步骤 搭建Spark分布式集群的步骤如下：\n准备环境 ：确保每台机器上都安装了Java和Spark的依赖库，并且网络连接正常。 配置主节点 ：选择一台机器作为主节点，编辑Spark的配置文件（spark-defaults.conf和spark-env.sh），设置主节点的IP地址、端口号、内存分配等参数。 配置工作节点 ：编辑Spark的配置文件，设置工作节点的IP地址、端口号、内存分配等参数。 配置集群管理器 ：如果使用集群管理器（如Standalone、YARN或Mesos），需要配置相关的参数，如集群管理器的地址、端口号等。 分发Spark安装包 ：将Spark安装包分发到所有的机器上，确保每台机器上都可以访问到Spark的安装目录。 启动集群 ：在主节点上执行启动命令，启动集群管理器和工作节点。具体命令可以是 ./sbin/start-master.sh启动集群管理器，./sbin/start-worker.sh \u0026lt;master-url\u0026gt;启动工作节点。 验证集群 ：通过访问Spark的Web界面（通常是 http://\u0026lt;master-ip\u0026gt;:8080）来验证集群是否正常运行。在Web界面上可以查看集群的状态、任务的执行情况等信息。 提交任务 ：使用Spark提供的命令行工具或编写Spark应用程序，将任务提交到集群中执行。具体命令可以是 ./bin/spark-submit --class \u0026lt;main-class\u0026gt; --master \u0026lt;master-url\u0026gt; \u0026lt;application-jar\u0026gt;。 以上是Spark分布式集群搭建的一般步骤，具体的步骤和命令可能会因为使用的集群管理器和环境的不同而有所差异。在实际搭建过程中，还需要根据具体的需求和环境进行相应的配置和调整。\nSpark Streaming中如何实现精准一次消费 在Spark Streaming中，实现精确一次消费（exactly-once semantics）是通过以下步骤来实现的：\n使用Kafka作为数据源 ：首先，将Kafka作为Spark Streaming的数据源。Kafka是一个分布式流处理平台，具有高吞吐量和容错性。 使用Kafka的Direct方式消费数据 ：Spark Streaming提供了两种方式来消费Kafka数据，即Direct方式和Receiver方式。为了实现精确一次消费，我们使用Direct方式。Direct方式通过直接从Kafka分区中读取数据来消费，而不是通过Kafka Receiver。 将偏移量保存到ZooKeeper或Kafka ：在Direct方式中，Spark Streaming会周期性地将消费的偏移量（offset）保存到外部存储系统，比如ZooKeeper或Kafka的特殊主题。这样，在发生故障或重启时，Spark Streaming可以从上一次保存的偏移量继续消费数据，从而实现精确一次消费。 定期提交偏移量 ：Spark Streaming会定期提交已经处理的偏移量，以确保即使在处理过程中发生故障，也不会重复处理已经处理过的数据。 容错机制 ：Spark Streaming具有容错机制，即使在处理过程中发生故障，也可以恢复并继续处理数据。当发生故障时，Spark Streaming会使用保存的偏移量来重新启动，并从上一次保存的偏移量处继续消费数据，确保数据不会丢失或重复处理。 通过以上步骤，Spark Streaming能够实现精确一次消费，确保每条数据只被处理一次，从而保证数据处理的准确性和一致性。\nSpark中Master 实现HA有哪些方式 在Spark中，实现Master的高可用性（HA）可以采用以下几种方式：\nStandalone模式下的HA ：在Spark的Standalone模式中，可以通过启动多个Master节点来实现HA。这些Master节点将组成一个高可用性的集群，其中一个节点将被选举为活动Master，而其他节点则处于备用状态。当活动Master节点发生故障时，备用节点将自动接管并成为新的活动Master节点。 YARN模式下的HA ：在Spark的YARN模式中，可以利用YARN的HA机制来实现Master的高可用性。YARN提供了ResourceManager的HA配置，可以使用ZooKeeper来进行故障检测和自动故障转移。当活动的ResourceManager节点发生故障时，ZooKeeper将自动选举一个备用节点作为新的活动节点。 Mesos模式下的HA ：在Spark的Mesos模式中，可以通过启动多个Mesos Master节点来实现HA。这些Master节点将组成一个高可用性的集群，其中一个节点将被选举为活动Master，而其他节点则处于备用状态。当活动Master节点发生故障时，备用节点将自动接管并成为新的活动Master节点。 需要注意的是，无论是哪种模式下的HA，都需要使用外部组件（如ZooKeeper）来进行故障检测和自动故障转移。这些组件负责监控Master节点的状态，并在需要时进行故障转移。同时，还需要配置Spark的相关参数，以便Spark能够与这些外部组件进行通信和协调。\n总结起来，Spark中实现Master的HA可以通过在Standalone、YARN或Mesos模式下配置多个Master节点，并结合外部组件（如ZooKeeper）进行故障检测和自动故障转移来实现。\nSpark master使用zookeeper进行HA，有哪些元数据保存在Zookeeper? 在Spark中，使用ZooKeeper进行Master的高可用性（HA）时，以下元数据将保存在ZooKeeper中：\nMaster的主节点选举信息 ：ZooKeeper用于协调多个Master节点之间的主节点选举过程。每个Master节点都会在ZooKeeper上创建一个临时顺序节点，称为\u0026quot;master_election\u0026quot;。当一个Master节点启动时，它会尝试创建这个节点。如果创建成功，它将成为主节点，并且其他Master节点将成为备用节点。如果创建失败，它将监听前一个节点的删除事件，一旦前一个节点被删除，它将尝试再次创建节点并成为主节点。 Master的元数据信息 ：每个Master节点都会在ZooKeeper上创建一个持久节点，称为\u0026quot;master\u0026quot;。这个节点中保存了Master节点的元数据信息，包括Master节点的主机名、端口号、Web UI地址等。其他Spark组件（如Worker节点和Driver程序）可以通过查询这个节点获取Master节点的信息，以便与Master节点进行通信。 Worker节点的注册信息 ：每个Worker节点都会在ZooKeeper上创建一个临时顺序节点，称为\u0026quot;workers\u0026quot;。这个节点中保存了Worker节点的注册信息，包括Worker节点的主机名、端口号、状态等。Master节点可以通过监听这个节点的变化来实时获取Worker节点的注册和注销信息，并进行资源调度和任务分配。 Application的注册信息 ：每个运行的Spark应用程序都会在ZooKeeper上创建一个临时顺序节点，称为\u0026quot;applications\u0026quot;。这个节点中保存了应用程序的注册信息，包括应用程序的ID、驱动程序的主机名、端口号等。Master节点可以通过监听这个节点的变化来实时获取应用程序的注册和注销信息，并进行任务调度和资源管理。 通过将这些元数据保存在ZooKeeper中，Spark的Master节点可以实现高可用性和故障转移。当一个Master节点发生故障时，其他备用节点可以通过监听ZooKeeper上的节点变化来感知到故障，并自动选举新的主节点。同时，其他Spark组件也可以通过查询ZooKeeper上的节点来获取Master节点和Worker节点的信息，从而实现与它们的通信和协调。\nSpark master HA 主从切换过程不会影响集群已有的作业运行，为什么？ Spark的Master节点高可用性（HA）切换过程不会影响集群中已有的作业运行，这是因为Spark的Master节点只负责作业的调度和资源管理，并不直接参与作业的执行过程。下面是详细的说明：\nMaster节点的故障转移 ：当当前的Master节点发生故障时，通过使用ZooKeeper等外部的协调服务来实现Master节点的故障转移。ZooKeeper会监视Master节点的状态，并在Master节点故障时自动选举出新的Master节点。在选举过程中，已有的作业不会受到影响，因为它们已经在Worker节点上运行，并由Worker节点自主地管理。 Worker节点的自治性 ：Spark的Worker节点是独立的执行引擎，它们负责运行作业的任务。一旦作业被分配到Worker节点上，它们就会独立地运行，不依赖于Master节点的状态。即使Master节点发生故障，Worker节点仍然可以继续执行已经接收到的任务，因为它们已经获取了执行任务所需的资源和数据。 作业的元数据和状态信息 ：Spark的作业元数据和状态信息通常存储在外部的持久化存储中，如HDFS或数据库中。这些信息包括作业的执行计划、任务的状态、作业的进度等。即使Master节点发生故障，这些信息仍然可以被保留，并且可以在新的Master节点上进行恢复和管理。 综上所述，Spark的Master节点的故障转移过程不会影响已有的作业运行，因为作业的执行是由Worker节点独立完成的，并且作业的元数据和状态信息可以在故障转移后进行恢复和管理。这种设计使得Spark集群具有高可用性和容错性，能够保证作业的稳定运行。\nSpark master如何通过Zookeeper做HA？ Spark的Master节点可以通过ZooKeeper来实现高可用性（HA）。下面是Spark Master节点通过ZooKeeper实现HA的详细过程：\nZooKeeper的安装和配置 ：首先，需要在Spark集群中安装和配置ZooKeeper。ZooKeeper是一个分布式协调服务，用于管理集群中的各个节点和数据。 Master节点的注册 ：当Spark的Master节点启动时，它会向ZooKeeper注册自己的信息，包括主机名、端口号和其他必要的元数据。这样，ZooKeeper就知道了Master节点的存在。 主节点选举 ：在ZooKeeper中，可以设置一个临时的有序节点（EPHEMERAL_SEQUENTIAL），用于表示Master节点的候选人。所有的Master节点都会尝试创建这个临时节点，并在节点上记录自己的ID。ZooKeeper会根据节点的创建顺序和ID来选举出一个主节点。 主节点的选举过程 ：当Master节点启动并注册到ZooKeeper后，它会检查是否已经存在一个主节点。如果不存在，则它会尝试创建一个临时节点，并成为主节点。如果已经存在主节点，则当前的Master节点会成为备用节点，并监听主节点的状态。 备用节点的监听和故障转移 ：备用节点会监听主节点的状态变化。如果主节点发生故障，ZooKeeper会自动将备用节点提升为新的主节点。此时，备用节点将接管Master节点的角色，并继续管理作业的调度和资源分配。 元数据的保存和监听 ：除了主节点的选举外，Spark还会将其他重要的元数据信息保存在ZooKeeper中，如Worker节点的注册信息、Application的注册信息等。这些信息的保存和监听可以实现Master节点的故障转移和其他组件与Master节点的通信和协调。 通过以上步骤，Spark的Master节点可以利用ZooKeeper来实现高可用性。当当前的Master节点发生故障时，ZooKeeper会自动选举出新的Master节点，并且已有的作业不会受到影响。这种方式可以确保Spark集群的稳定运行和容错性。\n说说spark hashParitioner的弊端是什么 Spark的 HashPartitioner是一种常用的分区器，它根据键的哈希值将数据分布到不同的分区中。虽然 HashPartitioner在许多情况下都能提供良好的性能，但它也存在一些弊端，具体如下：\n数据倾斜 ：HashPartitioner使用键的哈希值来确定分区索引，如果数据中的某些键的哈希值分布不均匀，就会导致数据倾斜问题。这意味着一些分区可能会比其他分区更大，从而导致负载不平衡和计算性能下降。 无法保证有序性 ：HashPartitioner将数据根据哈希值分散到不同的分区中，这意味着相同键的数据可能会分散到不同的分区中，无法保证数据的有序性。在某些场景下，需要保持数据的有序性，这就需要使用其他类型的分区器。 分区数量固定 ：HashPartitioner在创建时需要指定分区的数量，这意味着分区数量是固定的。在某些情况下，数据规模可能会发生变化，需要动态调整分区数量来更好地利用集群资源，但 HashPartitioner无法满足这种需求。 总的来说，HashPartitioner在一些常见场景下表现良好，但在数据倾斜、有序性和动态调整分区数量等方面存在一些限制。在这些情况下，可能需要考虑使用其他类型的自定义分区器来解决这些问题。\nSpark读取数据，是几个Partition呢 在Spark中，数据的分区数量取决于数据源和集群的配置。当使用Spark读取数据时，可以通过以下几个因素来确定数据的分区数量：\n数据源的分区情况 ：不同的数据源在读取数据时会有不同的分区策略。例如，当从Hadoop分布式文件系统（HDFS）读取数据时，Spark会根据HDFS块的大小来确定分区数量。每个HDFS块通常对应一个分区。而当从数据库或其他数据源读取数据时，分区策略可能会有所不同。 数据源的切片数 ：Spark将数据源切分为多个切片，每个切片对应一个分区。切片的数量由Spark的 spark.sql.files.maxPartitionBytes和 spark.sql.files.openCostInBytes等配置参数控制。这些参数可以调整以控制分区数量。 数据规模和集群资源 ：数据的大小和集群的资源配置也会影响数据的分区数量。如果数据较小，Spark可能会使用较少的分区。而如果数据较大，Spark可能会使用更多的分区来更好地利用集群资源。 需要注意的是，Spark并不保证每个数据分区都具有相同数量的数据。这取决于数据的分布情况和分区策略。有时候，数据可能会出现倾斜，导致某些分区比其他分区更大。\n可以通过以下方式来查看数据的分区数量：\n对于RDD：可以使用 getNumPartitions()方法获取RDD的分区数量。 对于DataFrame或Dataset：可以使用 rdd.getNumPartitions()方法获取底层RDD的分区数量。 需要注意的是，分区数量在读取数据时是动态确定的，并且可以根据数据源、配置和集群资源的不同而变化。因此，实际的分区数量可能会有所不同。\nRangePartitioner分区的原理 RangePartitioner是Spark中的一种分区器，用于将数据按照一定的范围进行分区。它的原理是根据数据的键值范围将数据划分到不同的分区中。\nRangePartitioner的工作流程如下：\n首先，RangePartitioner会对数据进行采样，以获取数据的键值范围。采样过程可以通过调用 RDD.sample方法来实现。 接下来，RangePartitioner会根据指定的分区数量，将键值范围划分为相应数量的区间。 然后，RangePartitioner会将数据按照键值范围划分到对应的分区中。具体的划分方式是根据数据的键值与区间的比较来确定数据所属的分区。 最后，RangePartitioner会返回一个分区器对象，可以将其应用到RDD或DataFrame中。在使用RDD时，可以通过调用 RDD.partitionBy方法，并传入RangePartitioner来进行分区；在使用DataFrame时，可以通过调用 DataFrame.repartition方法，并传入RangePartitioner来进行分区。 RangePartitioner的优点是可以根据数据的键值范围进行分区，可以保证相同范围内的数据被划分到同一个分区中，从而提高数据的局部性。这对于一些需要按照键值范围进行聚合或排序的操作非常有用。\n然而，RangePartitioner也存在一些限制。首先，它要求数据的键值范围是已知的，这对于一些动态生成的数据集可能不适用。其次，如果数据的分布不均匀，可能会导致某些分区的数据量过大或过小，从而影响计算性能。因此，在实际使用中，需要根据数据的特点选择合适的分区策略。\nRangePartioner分区器特点 RangePartitioner分区器具有以下特点：\n基于范围划分 ：RangePartitioner根据数据的键值范围将数据划分到不同的分区中。它将键值范围划分为若干个区间，并将数据按照键值与区间的比较进行划分。 数据局部性 ：RangePartitioner会将相同范围内的数据划分到同一个分区中，从而提高数据的局部性。这对于一些需要按照键值范围进行聚合或排序的操作非常有用。 适用于有序数据 ：RangePartitioner适用于有序数据的分区。它可以保证相同范围内的数据被划分到同一个分区中，从而保持数据的有序性。 分区数量可控 ：RangePartitioner可以通过指定分区数量来控制数据的分区个数。这对于优化计算性能和资源利用非常重要。 采样获取范围 ：为了确定数据的键值范围，RangePartitioner会对数据进行采样。采样过程可以通过调用 RDD.sample方法来实现。 需要键值范围已知 ：RangePartitioner要求数据的键值范围是已知的，这对于一些动态生成的数据集可能不适用。 可应用于RDD和DataFrame ：RangePartitioner可以应用于RDD和DataFrame。在使用RDD时，可以通过调用 RDD.partitionBy方法，并传入RangePartitioner来进行分区；在使用DataFrame时，可以通过调用 DataFrame.repartition方法，并传入RangePartitioner来进行分区。 需要注意的是，RangePartitioner也存在一些限制。如果数据的分布不均匀，可能会导致某些分区的数据量过大或过小，从而影响计算性能。因此，在使用RangePartitioner时，需要根据数据的特点选择合适的分区策略。\n介绍parition和block有什么关联关系 在Spark中，Partition（分区）和Block（块）是两个不同的概念，但它们之间存在一定的关联关系。\nPartition（分区）是将数据集拆分为较小、可并行处理的数据块的过程。在Spark中，数据集被划分为多个分区，每个分区包含数据的一个子集。每个分区都可以在集群中的不同节点上进行并行处理。\n而Block（块）是Spark中数据存储和传输的基本单位。在Spark中，数据被划分为多个块，每个块的大小通常为128MB。每个块都会被存储在集群中的不同节点上，并且可以在节点之间进行传输和共享。\nPartition和Block之间的关联关系在数据处理过程中体现出来。当Spark执行任务时，每个分区的数据会被加载到对应节点的内存中，并以块的形式进行存储。这样可以提高数据的读取和处理效率，因为每个节点只需要加载和处理自己负责的分区数据块。\n此外，Spark还使用Partition和Block之间的关联关系来进行数据的本地性调度。Spark会尽量将任务调度到与数据所在的分区块相同的节点上执行，以减少数据传输的开销，并提高任务的执行效率。\n总结起来，Partition和Block之间的关联关系可以简单描述为：Partition是数据集的逻辑划分，而Block是数据的物理存储和传输单位，二者配合使用可以提高数据处理的效率和性能。\n什么是二次排序，你是如何用spark实现二次排序的？（互联网公司常面） 二次排序（Secondary Sorting）是指在对数据进行排序时，除了主排序键（Primary Key）外，还需要对次要排序键（Secondary Key）进行排序。在Spark中，可以使用自定义排序函数和自定义分区器来实现二次排序。\n要实现二次排序，首先需要定义一个包含主排序键和次要排序键的元组作为数据的键。然后，可以使用 sortByKey()函数对键值对进行排序。在排序时，可以通过自定义的排序函数来指定主排序键和次要排序键的排序规则。\n下面是一个使用Spark实现二次排序的示例代码：\n如何使用Spark解决TopN问题？（互联网公司常面） 在Spark中，解决TopN问题通常涉及以下几个步骤：\n加载数据 ：首先，你需要加载数据到Spark中。你可以使用 textFile方法加载文本文件，或者使用其他适合你数据格式的方法加载数据。 数据预处理 ：根据你的需求，对数据进行必要的预处理。这可能包括数据清洗、转换和过滤等操作。 数据转换 ：将数据转换为键值对的形式，其中键是你要进行TopN操作的字段，值是与该字段相关的其他数据。你可以使用 map或者 flatMap方法来实现这个步骤。 按键分组 ：使用 groupByKey方法将数据按键进行分组。这将把具有相同键的数据分组到一起。 计算TopN ：对每个键的数据进行处理，以获取TopN结果。你可以使用 mapValues方法结合排序操作来实现这一步骤。例如，你可以使用 takeOrdered方法获取每个键的前N个元素。 合并结果 ：如果你需要获得全局的TopN结果，你需要将每个分区的TopN结果进行合并。你可以使用 reduceByKey方法来合并分区结果。 如何使用Spark解决分组排序问题？（互联网公司常面） Hadoop中，Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？ 在Hadoop中，MapReduce操作的Mapper和Reducer阶段可以与Spark中的多个算子进行对应。下面是对应关系的详细解释：\n一、Mapper阶段：\n在Hadoop中，Mapper阶段负责对输入数据进行切分和映射操作。它将输入数据分割成小的数据块，并为每个数据块生成键值对。 在Spark中，可以使用 map算子来实现类似的功能。map算子会对输入RDD中的每个元素应用一个函数，生成一个新的RDD。 二、Reducer阶段：\n在Hadoop中，Reducer阶段负责对Mapper阶段输出的键值对进行合并和归约操作，生成最终的输出结果。 在Spark中，可以使用 reduceByKey或 groupByKey算子来实现类似的功能。reduceByKey算子会将具有相同键的值进行合并，而 groupByKey算子会将具有相同键的值进行分组。 需要注意的是，Spark的数据处理模型与Hadoop的MapReduce模型不同。Spark的数据处理模型是基于弹性分布式数据集（RDD）的，而不是基于键值对的。因此，在Spark中，可以使用更多的算子来进行数据转换和操作，例如 filter、flatMap、join等。\n总结起来，Mapper阶段可以使用Spark的 map算子，而Reducer阶段可以使用Spark的 reduceByKey或 groupByKey算子。但是，需要注意的是，Spark中的算子更加灵活，可以进行更多种类的数据操作和转换。\nSpark shell启动时会启动derby 在Spark中，Derby是一个内置的关系型数据库，用于支持Spark的元数据存储和管理。当你启动Spark Shell时，它会自动启动Derby数据库作为元数据存储的后端。\nDerby数据库是一个轻量级的Java数据库，它可以在本地模式下运行，不需要额外的配置或安装。Spark使用Derby数据库来存储关于Spark应用程序的元数据信息，包括表结构、数据源连接信息、执行计划等。\n当你启动Spark Shell时，它会在本地启动一个Derby数据库实例，并将元数据存储在该实例中。这个Derby实例是与Spark Shell进程绑定的，当你退出Spark Shell时，Derby数据库也会随之关闭。\n通过Derby数据库，Spark可以方便地管理和查询元数据信息，例如可以使用Spark的SQL语法来查询表结构、执行计划等。此外，Derby还支持事务处理和并发控制，确保对元数据的修改是安全和一致的。\n总之，Spark Shell启动时会自动启动Derby数据库作为元数据存储的后端，这使得Spark能够方便地管理和查询应用程序的元数据信息。\n介绍一下你对Unified Memory Management内存管理模型的理解 Unified Memory Management（统一内存管理）是一种内存管理模型，它在计算机系统中统一了CPU和GPU之间的内存管理。传统上，CPU和GPU拥有各自独立的内存空间，数据需要在它们之间进行显式的复制。而在Unified Memory Management模型中，CPU和GPU共享同一块内存，数据可以在CPU和GPU之间自动进行迁移，无需显式的复制操作。\n在Unified Memory Management模型中，程序员可以将内存分配给CPU和GPU使用，并通过简单的标记来指示数据在CPU和GPU之间的访问模式。当CPU或GPU需要访问数据时，系统会自动将数据从一个设备迁移到另一个设备。这种自动的数据迁移使得程序员可以更方便地编写跨设备的并行代码，而无需手动管理数据的复制和迁移。\n使用Unified Memory Management模型可以简化并行编程，并提高代码的可移植性和性能。程序员可以更容易地利用GPU的并行计算能力，而无需关注数据的复制和迁移。同时，系统可以根据数据的访问模式进行智能的数据迁移，以提高访问数据的效率。\n总之，Unified Memory Management模型通过统一CPU和GPU之间的内存管理，简化了并行编程，并提高了代码的可移植性和性能。\nHBase预分区个数和Spark过程中的reduce个数相同么 HBase的预分区个数和Spark过程中的reduce个数不一定相同。它们是两个不同的概念，分别用于不同的目的。\nHBase的预分区个数 ：HBase是一个分布式的NoSQL数据库，它使用行键（Row Key）来进行数据的存储和索引。为了实现数据的负载均衡和高效查询，HBase会将数据分散存储在不同的Region中。在创建HBase表时，可以指定表的预分区个数，也称为Region个数。预分区个数决定了HBase表在集群中的分布情况，可以根据数据的特点和负载要求进行调整。 Spark过程中的reduce个数 ：在Spark中，reduce是指对数据进行聚合操作的阶段，通常是在MapReduce模型中的reduce阶段。在Spark中，reduce的个数由数据的分区数决定，每个分区都会有一个reduce任务。分区数可以通过调整RDD的分区策略或使用 repartition、coalesce等操作来进行控制。reduce的个数影响了并行计算的程度，可以根据数据量和计算资源进行调整。 虽然HBase的预分区个数和Spark过程中的reduce个数可以相互关联，但它们并不是一一对应的关系。HBase的预分区个数主要用于数据的存储和负载均衡，而Spark的reduce个数主要用于计算的并行度控制。在实际应用中，可以根据数据的特点和计算需求来调整它们的个数，以达到最佳的性能和效果。\n简要介绍下Spark的内存管理 Spark读取hdfs上的文件，然后count有多少行的操作，你可以说说过程吗。那这个count是在内存中，还是磁盘中计算的呢 hbase region多大会分区，spark读取hbase数据是如何划分partition的 说说BlockManager怎么管理硬盘和内存的 列举Spark中常见的端口，分别有什么功能 集群上nodemanager和ResourceManager的数量关系 Spark 如何防止内存溢出 Spark的通信方式 Spark如何处理结构化数据，Spark如何处理非结构话数据？ 对于Spark你觉得他对于现有大数据的现状的优势和劣势在哪里 简要描述你了解的一些数据挖掘算法与内容 Spark Streaming中对接的socket的缓存策略是什么 Mapreduce和Spark的相同和区别 MapReduce和Spark都是用于并行计算的框架。\n相同点：\n并行计算 ：两者都支持将大规模的数据集划分为多个小任务，并在分布式环境中并行执行这些任务。 可扩展性 ：它们都可以在大规模集群上运行，通过添加更多的计算节点来扩展计算能力。 容错性 ：它们都具备故障恢复机制，能够处理计算节点的故障，并保证计算的正确性。 区别：\n内存使用 ：MapReduce将中间数据写入磁盘，而Spark将中间数据存储在内存中，这使得Spark在某些情况下比MapReduce更快，尤其是对于迭代计算和交互式查询等需要多次读写数据的场景。 数据处理模型 ：MapReduce采用了经典的\u0026quot;map\u0026quot;和\u0026quot;reduce\u0026quot;操作模型，而Spark引入了更多的数据处理操作，如过滤、排序、连接等，使得编写数据处理逻辑更加灵活。 实时计算支持 ：Spark提供了实时流处理功能，可以对数据进行实时处理和分析，而MapReduce主要用于离线批处理。 编程接口 ：MapReduce使用Java编程接口，而Spark支持多种编程语言接口，包括Java、Scala、Python和R，使得开发者可以使用自己熟悉的语言进行开发。 总体而言，Spark相对于MapReduce来说更加灵活和高效，尤其适用于需要实时计算和复杂数据处理的场景。但对于一些传统的离线批处理任务，MapReduce仍然是一个可靠的选择。\n对RDD、DAG和Task的理解 DAG为什么适合Spark 介绍下Spark的DAG以及它的生成过程 DAGScheduler如何划分？干了什么活 Spark的容错机制 RDD的容错 Executor内存分配 Spark的batchsize，怎么解决小文件合并问题 Spark参数(性能)调优 介绍一下Spark是怎么基于内存计算的 说下什么是RDD(对RDD的理解)，RDD有哪些特点？说下知道的RDD算子 RDD底层原理 RDD属性 RDD的缓存级别 Spark广播变量的实现和原理 reduceByKey和groupByKey的区别和作用 reduceByKey和reduce的区别 使用reduceByKey出现数据倾斜怎么办 SparkSQL的执行原理 SparkSQL的优化 说下Spark checkpoint Sparksql自定义函数？怎么创建DataFrame HashPartitioner和RangePartitioner的实现 DAGScheduler、TaskScheduler、SchedulerBackend实现原理 Driver怎么管理executor Spark的map和flatmap的区别 map和mapPartition的区别 Spark的cache和persist的区别？它们是transformation算子还是action算子？ SparkStreaming的工作原理 Spark Streaming的DStream和DStreamGraph的区别 Spark输出文件的个数，如何合并小文件 Spark的driver是怎么驱动作业流程的 SparkSQL的劣势 DAG划分Spark源码实现 Spark Steaming的双流join的过程，怎么做的 Spark的Block管理 Spark怎么保证数据不丢失 SparkSQL如何使用UDF SparkSQL读取文件，内存不够使用，如何处理 Spark的lazy提现在哪里 Spark中的并行度等于什么 Spark运行时并行度的设置 SparkSQL的数据倾斜 Spark的exactly-one Spark的RDD和partition的联系 Spark3的特性 Spakr计算的灵活性提现在哪里 实战 Spark数据倾斜问题，如何定位，解决方案 Spark提交你的jar包时所用的命令是什么 spark-submit的时候如何引入外部jar包 你如何从Kafka中获取数据 Spark Streaming对接Kafka两种整合方式的区别 如何配置Spark master的HA 在一个不确定的数据规模的范围内进行排序可以采用以下几种方法 内存排序 ：如果数据规模较小，可以将所有数据加载到内存中进行排序。这种方法简单快速，适用于能够一次性加载到内存的数据集。 外部排序 ：当数据规模较大，无法一次性加载到内存时，可以采用外部排序算法。外部排序将数据划分为多个较小的块，并在磁盘上进行排序和合并操作。 首先，将数据分割成适当大小的块，并将每个块加载到内存中进行排序。 然后，使用归并排序等算法将排序好的块逐一合并，直到得到完整排序的结果。 外部排序的优点是可以处理大规模数据，但需要额外的磁盘空间和IO操作。 分布式排序 ：对于超大规模的数据，可以采用分布式排序算法。分布式排序将数据分布在多台计算机上进行并行排序和合并。 首先，将数据划分为多个分区，并将每个分区分配给不同的计算节点。 每个节点独立对自己负责的数据分区进行排序。 最后，使用归并排序等算法将各个节点的排序结果合并成最终的全局排序结果。 分布式排序可以充分利用集群的计算资源，高效地处理大规模数据。 无论使用哪种方法，在不确定数据规模的情况下，都需要考虑内存和磁盘的限制，选择适当的算法和数据分割策略，以实现高效的排序操作。\nSpark如何自定义partitioner分区器 使用shell和scala代码实现WordCount 怎么用Spark做数据清洗 说说Spark怎么整合hive ","date":"2025-03-12T11:53:11+08:00","image":"https://sherlock-lin.github.io/p/spark%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/nevada-8338929_1280_hu7796648983855650120.jpg","permalink":"https://sherlock-lin.github.io/p/spark%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/","title":"Spark面试一文通"},{"content":"高频 讲一下Flink的运行架构 Flink程序在运行时主要有TaskManager，JobManager，Client三种角色。当 Flink 集群启动后，⾸先会启动⼀个 JobManger 和⼀个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执⾏，然后 TaskManager 将⼼跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进⾏数据的传输。上述三者均为独⽴的 JVM 进程。\nJobManager：扮演着集群中的管理者Master的角色，它是整个集群的协调者，负责接收Flink Job，协调检查点，Failover 故障恢复等，同时管理Flink集群中从节点TaskManager。 TaskManager：是实际负责执行计算的Worker，在其上执行Flink Job的一组Task，每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。 Client：是Flink程序提交的客户端，当用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager。 Flink 中对窗口的支持包括哪几种，说说他们的使用场景，什么区别，以及如何定义 Flink支持两种划分窗口的方式，按照time和count，session也是一种时间。\nTumbling Time Window（滚动时间窗口）：当达到一定时间后的，进行滑动，可以联想到以前用的诺基亚的滑盖手机，这个其实就是微批。用于处理实时数据流中的时间序列数据，如股票价格走势、实时监测流量等。假如我们需要统计每一分钟中用户购买的商品的总数，需要将用户的行为事件按每一分钟进 行切分，这种切分被成为翻滚时间窗口（Tumbling Time Window）。翻滚窗口能将数据流切分成 不重叠的窗口，每一个事件只能属于一个窗口。 Sliding Time Window（滑动时间窗口）：当达到一定时间后，进行翻滚，可以有重叠。用于处理时间序列数据的近期分析，如近 5 分钟用户购买商品总数等。我们可以每 30 秒计算一次最近一分钟用户购买的商品总数。这种窗口我们称为滑动时间窗 口（Sliding Time Window）。在滑窗中，一个元素可以对应多个窗口。 Tumbling Count Window（滚动计数窗口）：当达到一定条数的时候执行计算，无折叠。用于处理计数型数据，如统计网站访问量、分析用户购买行为等。当我们想要每 100 个用户购买行为事件统计购买总数，那么每当窗口中填满 100 个元素了， 就会对窗口进行计算，这种窗口我们称之为翻滚计数窗口（Tumbling Count Window），上图所 示窗口大小为 3 个。 Sliding Count Window（滑动计数窗口）：当达到一定数量后进行滑动。用于处理计数型数据的实时分析，如实时监测广告点击率、实时统计投票数等。 Session Window（会话窗口）：窗口数据没有固定的大小，根据用户传入的参数进行划分，窗口数据无叠加。用于处理用户交互事件流中的数据，如计算每个用户在活跃期间总共购买的商品数量等。类似于当用户退出的时候，计算这个用户之前的动作。在这种用户交互事件流中，我们首先想到的是将事件聚合到会话窗口中（一段用户持续活跃 的周期），由非活跃的间隙分隔开。如上图所示，就是需要计算每个用户在活跃期间总共购买的 商品数量，如果用户 30 秒没有活动则视为会话断开（假设 raw data stream 是单个用户的购买 行为流）。一般而言，window 是在无限的流上定义了一个有限的元素集合。这个集合可以是基于时间的，元素个数的，时间和个数结合的，会话间隙的，或者是自定义的。Flink 的 DataStream API 提供了简洁的算子来满足常用的窗口操作，同时提供了通用的窗口机制来允许用户自己定义 窗口分配逻辑。 Flink的Checkpoint存在哪里？ Flink 的 Checkpoint 是 Flink 的核心组件之一，它用于记录应用程序在特定时刻的状态，以便在应用程序失败时进行恢复。Checkpoint 通常存储在 Flink 的存储系统中，可以是内存、文件系统或 RocksDB。\n内存 Flink 的内存状态是存储在 Java 内存中的。当应用程序运行时，Flink 会将状态数据存储在内存中，并定期将这些状态数据持久化到外部存储系统中。如果应用程序在运行时出现故障，Flink 可以从内存状态中恢复应用程序的状态。\n文件系统 Flink 也可以将状态数据存储在文件系统中。当应用程序运行时，Flink 会将状态数据写入分布式文件系统，如 HDFS 或 NFS。如果应用程序在运行时出现故障，Flink 可以从文件系统中恢复应用程序的状态。\nRocksDB Flink 还可以将状态数据存储在 RocksDB 中。RocksDB 是一种高性能、高可靠性的键值存储数据库，它支持高效的数据压缩和快速查找。当应用程序运行时，Flink 会将状态数据写入 RocksDB 数据库，并定期将这些状态数据持久化到外部存储系统中。如果应用程序在运行时出现故障，Flink 可以从 RocksDB 中恢复应用程序的状态。\n总之，Flink 的 Checkpoint 可以存储在内存、文件系统或 RocksDB 中，具体存储位置由用户配置决定。Flink 提供了一些 API 来管理 Checkpoint，如 checkpointCoordinator.checkpoint() 方法和 checkpointCoordinator.restoreFromCheckpoint() 方法。使用这些 API，用户可以手动触发 Checkpoint，也可以在应用程序失败时自动恢复状态。\n说一说Flink的checkpoint机制 说说Flink 的容错机制，Flink是如何做到容错的？ Flink 是一个分布式流处理框架，它实现了容错机制以确保在节点故障时，数据不会丢失并且可以进行故障恢复。Flink 的容错机制主要依靠两个强大的机制：Checkpoint 和 State。\nCheckpoint ：是一种快照机制，它用于定期备份 Flink 程序中的状态，并将其存储在外部存储系统中。当节点发生故障时，Flink 可以使用 Checkpoint 来恢复程序的状态，并从故障点继续处理数据流。Checkpoint 的备份可以是全量的，也可以是增量的，这取决于 Checkpoint 的触发条件和备份策略。Flink 还支持 Exactly-Once 语义，这意味着在故障恢复时，Flink 可以确保每个事件都被处理了一次且仅一次。 State ：是 Flink 中的另一种重要机制，它用于存储计算过程中的中间状态。State 可以分为两种类型：Operator State 和 Keyed State。Operator State 是一种基于算子的状态，它存储在算子内部，并随着算子的执行而更新。Keyed State 是一种基于键的状态，它存储在 Stateful Function 内部，并使用键来标识状态的数据。Keyed State 可以具有过期时间（TTL），这使得 Flink 可以在状态过期时自动清理过期的状态数据。 在 Flink 中，Checkpoint 和 State 是相互依存的。Checkpoint 用于备份 State，并确保在节点故障时，可以恢复程序的状态。而 State 则用于存储计算过程中的中间状态，并支持 Exactly-Once 语义。Flink 通过这两个机制的结合，实现了强大的容错和故障恢复能力，使得 Flink 在分布式流处理中具有高度的可靠性和可用性。\nFlink分布式快照的原理是什么 Flink的容错机制的核心部分是制作分布式数据流和操作算子状态的一致性快照。 这些快照充当一致性checkpoint，系统可以在发生故障时回滚。 Flink用于制作这些快照的机制在“分布式数据流的轻量级异步快照”中进行了描述。 它受到分布式快照的标准Chandy-Lamport算法的启发，专门针对Flink的执行模型而定制。\nbarriers在数据流源处被注入并行数据流中。快照n的barriers被插入的位置（我们称之为Sn）是快照所包含的数据在数据源中最大位置。\n例如，在Apache Kafka中，此位置将是分区中最后一条记录的偏移量。 将该位置Sn报告给checkpoint协调器（Flink的JobManager）。\n然后barriers向下游流动。当一个中间操作算子从其所有输入流中收到快照n的barriers时，它会为快照n发出barriers进入其所有输出流中。\n一旦sink操作算子（流式DAG的末端）从其所有输入流接收到barriers n，它就向checkpoint协调器确认快照n完成。\n在所有sink确认快照后，意味快照着已完成。一旦完成快照n，job将永远不再向数据源请求Sn之前的记录，因为此时这些记录（及其后续记录）将已经通过整个数据流拓扑，也即是已经被处理结束。\n说说Flink的几种时间语义 Flink 支持三种时间语义：Event Time、Ingestion Time 和 Processing Time。\n1. Event Time（事件时间）\nEvent Time 是事件创建的时间，它通常由事件中的时间戳描述。通常由事件生成器或者传感器生成。在 Flink 中，事件时间可以通过 water-mark 或者定时器来处理。例如，在采集日志数据时，每一条日志都会记录自己的生成时间，Flink 通过时间戳分配器访问事件时间戳。Event Time 是事件产生的时间，与数据处理的时间无关，因此它可以反映事件产生的实时性，但是对于数据处理的延迟和异步性无法体现。\n2. Ingestion Time（注入时间）\nIngestion Time 是数据进入 Flink 的时间。它是指数据被 Flink 算子处理的时间，与事件创建的时间无关。Ingestion Time 能够反映数据处理的延迟和异步性，但是无法反映事件产生的实时性。\n3. Processing Time（处理时间）\nProcessing Time 是每一个执行基于时间操作的算子的本地系统时间，与机器相关。它是指算子处理数据的时间，与事件创建的时间和数据进入 Flink 的时间无关。Processing Time 是默认的时间属性，除非明确指定时间语义为 Event Time 或 Ingestion Time。\n在实际应用中，选择合适的时间语义可以影响 Flink 处理的数据流的正确性和效率。\n例如，如果需要处理实时数据流，那么选择 Event Time 更为合适；\n如果需要处理延迟数据流，那么选择 Ingestion Time 更为合适；\n如果需要处理离线数据集，那么选择 Processing Time 更为合适。\n同时，Flink 也提供了 WaterMark 机制来处理延迟数据和异步数据，以保证数据处理的正确性和可靠性。\n说说Flink中的Watermark机制 Flink 中的 Watermark 机制是一种衡量 Event Time 进展的机制，可以用于处理乱序事件。在数据流处理过程中，由于网络延迟、背压等多种因素的影响，数据可能会乱序到达。为了正确处理这些乱序事件，Flink 引入了 Watermark 机制，结合窗口 (Window) 来实现。\nWatermark 是一个时间戳，用于表示事件时间小于等于该时间戳的数据都已经到达。在 Flink 中，每个 Operator 都会维护一个当前的 Watermark，当一个事件到达时，如果它的时间戳小于等于当前 Watermark，那么该事件就会被认为是到达了，会被放入窗口中进行处理。窗口的执行是由 Watermark 触发的，当 Watermark 达到窗口的结束时间时，窗口就会触发并执行其中的计算逻辑。\n为了实现窗口的正确处理，Flink 还引入了事件时间 (Event Time) 概念，每个事件都会携带一个时间戳，表示该事件产生的时间。在数据流处理过程中，Flink 会根据事件时间戳的顺序来处理事件，这样可以保证事件的正确顺序。但是，由于网络延迟、背压等原因，事件可能会乱序到达，这就需要使用 Watermark 机制来处理这些乱序事件。\n总结起来，Flink 中的 Watermark 机制是用于处理乱序事件的一种机制，它可以设定延迟触发，用于表示事件时间小于等于该时间戳的数据都已经到达。通过结合窗口机制，Watermark 机制可以实现对乱序事件的正确处理，保证数据流的正确性和完整性。\n说说Flink的状态储存机制 Flink 的状态储存是指在 Flink 程序运行过程中，用于存储和管理算子状态的数据结构和存储系统。Flink 提供了多种状态后端，以适应不同的应用场景和需求。这里将详细叙述 Flink 的状态储存，包括 1.13 版本之前的状态后端和 1.13 版本之后的状态后端。\n1.13 版本之前：\nMemoryStateBackend ：开发时使用。这是一种基于内存的状态后端，用于在开发过程中快速调试和测试 Flink 程序。由于它使用内存存储状态数据，因此适用于状态数据较小的场景。 FsStateBackend ：生产时使用，常用。这是一种基于文件系统的状态后端，将状态数据存储在磁盘上。FsStateBackend 提供了一种高可用性的状态备份和恢复机制，以确保在任务失败时能够恢复状态。 RocksDBStateBackend ：生产时使用，非常大的状态时用。这是一种基于 RocksDB 的状态后端，使用 RocksDB 数据库存储状态数据。RocksDB 是一种支持高效压缩和快速查找的键值存储系统，适用于处理大规模状态数据的场景。 1.13 版本之后：\nHashMapStateBackend ：即 MemoryStateBackend 和 FsStateBackend，根据 API 不同。从 1.13 版本开始，Flink 对状态后端进行了整合，将 MemoryStateBackend 和 FsStateBackend 合并为一个统一的 HashMapStateBackend。它使用 HashMap 数据结构存储状态数据，并提供了一些额外的功能，如快照、checkpoint 等。 EmbeddedRocksDBStateBackend ：生产时使用，非常大的状态时用。这是一种基于 RocksDB 的状态后端，但与 RocksDBStateBackend 不同的是，它将 RocksDB 数据库嵌入到 Flink 的 TaskManager 进程中。这样做的优点是在状态数据较大时，可以减少网络开销和提高访问性能。 总之，Flink 的状态储存系统包括多种状态后端，以适应不同的应用场景和需求。开发者可以根据实际情况选择合适的状态后端，以实现高效、可靠的 Flink 程序。 checkpoint和savepoint的区别 Checkpoint工作流程 Checkpoint的作用 Flink中Checkpoint超时原因 Flink的Exactly Once语义怎么保证 Flink的端到端Exactly Once TaskSolt是什么 TaskSlot 是 Flink 中用于控制 TaskManager 接收任务的数量的一个概念。它是一个抽象的概念，表示一个 TaskManager 能够处理的任务数量。在 Flink 中，TaskManager 是实际执行程序的工作节点，为了起到资源隔离和并行执行的作用，TaskManager 是一个 JVM 进程。通过 TaskSlot 的概念，可以控制 TaskManager 接收的任务数量，从而更好地利用集群资源。\n当有一个 source 需要指定三个并行度时，它就需要使用三个 TaskSlot。这是因为 TaskSlot 的数量决定了 TaskManager 能够处理的任务数量。如果 source 需要三个并行度，那么 TaskManager 就需要三个 TaskSlot 来处理这三个并行度的任务。\n还有一个需要主要的优化概念是，当算子的并行度相同，并且没有发生并行度改变、或者没有 shuffle 时，这些算子会合并在一起。这样做的目的是为了减少资源的消耗，提高计算效率。\nFlink的Slots和并行度有什么关系 Solt 是 TaskManager 中的概念，表示 TaskManager 的一个槽（Slot）。并行度是程序中的概念，表示程序并行执行的程度。在 Flink 中，Solt 和并行度有着密切的关系。\n具体来说，Solt 是 TaskManager 的资源分配单位，它决定了 TaskManager 能够支持的并行度。一个 TaskManager 有多个 Solt，每个 Solt 可以分配给一个 Task，用于执行程序。因此，TaskManager 的并行度就等于其 Solt 的数量。\n程序制定的并行度使用的是槽（Solt），也就是说，程序是通过分配 Solt 来控制并行度的。当程序需要更高的并行度时，它可以向 TaskManager 申请更多的 Solt，以便在同一时间内执行更多的 Task。\n因此，Solt 和并行度之间的关系可以总结为：Solt 是 TaskManager 中的概念，它决定了 TaskManager 能够支持的并行度；并行度是程序中的概念，它是通过分配 Solt 来控制的。TaskManager 是提供方，提供 Solt 资源给程序使用；程序是使用方，通过分配 Solt 来控制并行度。\nFlink的水印(Watermark)有哪几种 Flink的时间语义 Flink相比其他流式处理框架的优点 Flink和Spark的区别？什么情况下使用Flink？有什么优点？ Flink backPressure反压机制是什么样的，指标监控你是怎么做的 Flink如何保持一致性 Flink支持JobManager的HA吗？原理是怎么样的？ 如何确定Flink任务的合理并行度 Flink任务如何实现端到端一致 Flink如何处理背压 Flink解决数据延迟的问题 Flink消费kafka分区的数据室，Flink任务并行度之间的关系 使用Flink-client消费kafka数据还是使用flink-connector消费 如何动态修改Flink的配置，前提是Flink不能重启 Flink流批一体解释一下 说一下Flink的check和barrier 说一下Flink状态机制 Flink广播流 Flink实时topN Savepoint知道是什么吗 基本 checkpoint底层如何实现的 简单介绍一下 Flink Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。并且 Flink 提供了数据分布、容错机制以及资源管理等核心功能。Flink提供了诸多高抽象层的API以便用户编写分布式任务：\nDataSet API ， 对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python。 DataStream API ，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持Java和Scala。 Table API ，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类SQL的DSL对关系表进行各种查询操作，支持Java和Scala。 此外，Flink 还针对特定的应用领域提供了领域库，例如： Flink ML，Flink 的机器学习库，提供了机器学习Pipelines API并实现了多种机器学习算法。 Gelly，Flink 的图计算库，提供了图计算的相关API及多种图计算算法实现。\nFlink的主要特点是什么？ Flink的主要特点包括：\n流式处理和批处理一体化：Flink既支持流式处理，也支持批处理，可以无缝地在流处理和批处理之间切换。 事件驱动的处理模型：Flink使用事件时间和处理时间的概念，支持基于事件的处理和窗口操作，适用于实时数据处理和分析。 高性能和低延迟：Flink的优化引擎可以实现高吞吐量和低延迟的数据处理，适用于需要快速响应的应用场景。 容错性和可靠性：Flink具有容错机制，可以在节点故障时保证数据处理的正确性和一致性。 灵活的编程模型：Flink支持多种编程模型，包括基于流的API（DataStream API）和基于批的API（DataSet API），并提供了多种编程语言接口。 Flink的应用场景有哪些？ 实时数据处理和分析 ：Flink 可以处理实时数据流，支持实时数据处理和分析，适用于实时监控、实时报表和实时分析等场景。 批处理任务 ：Flink 可以处理有界数据集，支持批处理任务，适用于离线数据处理和大规模数据分析等场景。 基于事件的应用 ：Flink 的事件驱动处理模型适合构建基于事件的应用，如实时推荐系统、欺诈检测和实时预测等场景。 流批一体化应用 ：Flink 的流批一体化特性使得可以将流式和批式处理结合起来，适用于需要实时和离线处理结合的应用场景。 数据挖掘和机器学习 ：Flink 可以处理大规模的数据集，并支持各种数据挖掘和机器学习算法，适用于构建大规模的数据挖掘和机器学习应用。 实时计算和决策 ：Flink 支持实时计算和决策，可以根据实时数据流进行实时决策和行动，适用于需要实时决策和行动的场景，如实时定价、实时广告投放等。 物联网应用 ：Flink 可以处理大规模的实时数据流，适用于处理物联网应用中的实时数据，如智能家居、智能城市、智能交通等场景。 Flink编程模型是什么？ 其实就一句话，就是 Source-\u0026gt;Transformation-\u0026gt;Sink\nFlink 编程模型是一种用于处理流式数据的编程模型，它包括三个核心概念：Source、Transformation 和 Sink。数据流从 Source 开始，经过多个 Transformation 操作，最终到达 Sink 结束。在这个过程中，数据可以被处理、过滤、转换、聚合等操作，以实现数据的实时处理和分析。\n具体来说，Flink 编程模型中，开发者需要首先指定数据的 Source，即数据的来源，可以是文件、网络数据流、数据库等。然后，通过一系列 Transformation 操作对数据进行处理，例如过滤、映射、聚合、窗口等操作。这些 Transformation 操作可以组合使用，以实现复杂的数据处理和分析。最后，将处理后的数据发送到 Sink 端，即数据的去向，可以是文件、网络数据流、数据库等。\nFlink 编程模型支持事件时间语义，即数据处理按照事件发生的时间进行排序和处理。同时，Flink 还支持窗口操作、状态管理和事件处理等功能，以实现更复杂的数据处理和分析场景。\nFlink的集群部署模式有哪些？ Flink的集群部署模式包括：\n单机模式：在单个机器上运行Flink集群，适用于开发和测试环境。 本地模式：在本地的多个线程上模拟Flink集群，适用于开发和调试任务。 分离式部署：将JobManager和TaskManager分别部署在不同的机器上，适用于生产环境和大规模任务的执行。 嵌入式模式：将Flink集成到现有的应用程序中，作为库来使用，适用于需要将流处理能力集成到其他应用中的场景。 你们之前Flink集群规模有多大？ 说说Flink集群优化 Flink 集群优化是提高 Flink 集群性能的关键步骤。\n以下是一些 Flink 集群优化的建议：\ntaskmanager.heap.mb 调优：taskmanager.heap.mb 是 Flink 任务管理器堆内存的大小，默认为 1024MB。如果需要更高的内存，可以将其调整为 2048MB 或更高。这可以确保任务管理器有足够的内存来处理数据和执行任务。 调整执行任务的并行度：Flink 任务的并行度可以通过任务属性进行调整。增加并行度可以提高任务的执行速度，但也会增加内存和 CPU 的使用量。因此，需要根据具体情况调整任务的并行度。 优化任务调度：Flink 任务调度可以通过多种方式进行优化。例如，可以调整 taskmanager 的数量和分配策略，以确保任务在不同的 taskmanager 上均匀分配。还可以调整任务的优先级和资源要求，以确保任务能够优先获得所需的资源。 优化网络配置：Flink 集群的网络配置也对性能有很大的影响。例如，可以调整 taskmanager 之间的连接方式，以确保任务数据能够快速传输。还可以调整网络带宽和延迟，以确保任务能够在规定时间内完成。 优化状态管理：Flink 任务的状态管理也是一个重要的优化方面。例如，可以使用 Flink 的状态备份和恢复功能，以确保任务状态能够在集群中的不同节点之间同步。还可以调整状态的持久化方式和位置，以确保状态数据不会丢失。 使用 Flink 的高级优化功能：Flink 还提供了许多高级优化功能，例如代码生成、优化器和迭代算子等。这些功能可以显著提高 Flink 集群的性能，但需要根据具体情况进行调整和使用。 总结起来，Flink 集群优化需要综合考虑多个方面，包括内存管理、任务调度、网络配置、状态管理和高级优化功能等。通过调整这些参数和配置，可以显著提高 Flink 集群的性能和效率。\n公司怎么提交的实时任务，有多少Job Manager？ Flink的并行度了解吗？Flink的并行度设置是怎样的？ Flink 程序由多个任务（Source、Transformation、Sink）组成。任务被分成多个并行实例来执行，每个并行实例处理任务的输入数据的子集。任务的并行实例的数量称之为并行度。\n我们在实际生产环境中可以从四个不同层面设置并行度：\n操作算子层面 (Operator Level)：算子.setParallelism(3),实际算子时设置 执行环境层面 (Execution Environment Level)：构建Flink环境时getExecutionEnvironment.setParallelism(1)设置 客户端层面 (Client Level)：提交flink run -p的时候设置 系统层面 (System Level)：flink客户端的配置yml文件中设置 需要注意的 优先级：算子层面\u0026gt;环境层面\u0026gt;客户端层面\u0026gt;系统层面 （实际业务中通常设置和kafka分区数一样或者kafka分区倍数的并行度）。\nFlink 可以设置好几个 level 的 parallelism，其中包括 Operator Level、ExecutionEnvironment Level、Client Level、System Level\n在 flink-conf.yaml 中通过parallelism.default 配置项给所有 execution environments 指定系统级的默认parallelism；\n在 ExecutionEnvironment 里头可以通过 setParallelism 来给 operators、data sources、data sinks 设置默认的 parallelism；\n如果 operators、data sources、datasinks 自己有设置 parallelism 则会覆盖 ExecutionEnvironment 设置的 parallelism。\nFlink的checkpoint机制对比spark有什么不同和优势？ Flink 和 Spark 都是主流的大数据处理框架，它们都支持 Checkpoint 机制以保证实时数据的可靠性和容错性。然而，Flink 和 Spark 的 Checkpoint 机制在实现方式和功能上有一些不同之处。\n实现方式 Flink 的 Checkpoint 机制采用了轻量级的分布式快照技术，实现了每个算子的快照以及流动中的数据的快照。这种快照技术可以快速地保存和恢复状态数据，从而减少了故障恢复的时间。而 Spark 的 Checkpoint 机制主要是针对 Driver 的故障恢复做了数据和元数据的 Checkpoint，没有实现算子的快照。\n故障恢复 Flink 的 Checkpoint 机制可以支持任意节点的故障恢复，包括算子和 Driver。当一个节点出现故障时，Flink 会自动切换到其他可用节点，并从最近的 Checkpoint 开始恢复状态数据。而 Spark 的 Checkpoint 机制只能恢复 Driver 的故障，对于算子的故障则需要重新启动整个应用程序。\n数据一致性 Flink 的 Checkpoint 机制可以保证数据一致性，即同一个 Checkpoint 下的所有算子都处于同一个状态。这是因为 Flink 使用了分布式快照技术，确保每个算子都保存了相同的状态数据。而 Spark 的 Checkpoint 机制并不能保证数据一致性，因为在 Spark 中，每个算子都可能保存了不同的状态数据。\n性能影响 Flink 的 Checkpoint 机制采用了轻量级的分布式快照技术，因此其性能影响相对较小。Spark 的 Checkpoint 机制需要将整个应用程序的状态数据都保存到外部存储系统中，因此其性能影响相对较大。\n总的来说，Flink 的 Checkpoint 机制相对于 Spark 的 Checkpoint 机制更为复杂和强大，可以支持任意节点的故障恢复，并保证数据一致性。此外，Flink 的 Checkpoint 机制采用了轻量级的分布式快照技术，因此其性能影响相对较小。这些优势使得 Flink 在实时数据处理方面具有更好的可靠性和容错性。\nFlink常用的算子有哪些？ Flink 是一个流处理框架，提供了丰富的算子用于数据的处理和转换。以下是一些常见的算子：\nMap 算子：将一个数据流中的每个元素映射成另一个元素。Map 算子是 Flink 中最基本的算子之一，它接受一个映射函数作为参数，该函数将输入数据映射到输出数据。\nFilter 算子：将一个数据流中的每个元素映射成多个元素。Filter 算子根据指定的条件过滤掉不符合条件的元素，只输出符合条件的元素。\nKeyBy 算子：根据指定的 key 对数据流进行分组。KeyBy 算子将数据流中的元素按照指定的 key 进行分组，并将每个分组中的元素聚合在一起。\nWindow 窗口算子：对数据流进行窗口操作。Window 算子可以指定窗口的类型、大小和滑动方式等参数，对数据流进行窗口操作，例如滚动窗口、滑动窗口、session 窗口等。\nReduce 算子：对数据流中的元素进行归约操作，将多个元素合并成一个元素。Reduce 算子接受一个聚合函数作为参数，该函数将输入数据聚合成输出数据。\nAggregate 算子：对数据流中的元素进行聚合操作。Aggregate 算子与 Reduce 算子类似，但它可以指定多个聚合函数，同时支持局部聚合和全局聚合。\nJoin 算子：对数据流中的元素进行连接操作。Join 算子可以指定连接的方式、连接的键和连接的条件等参数，将两个数据流连接在一起。\n除了上述算子之外，Flink 还提供了很多其他算子，例如 Union、HashJoin、Sort、Limit 等，以实现更复杂的数据处理和分析场景。\nFlink的流式处理如何处理延迟？ Flink的流式处理可以通过以下方式处理延迟：\n事件时间处理：Flink支持事件时间处理，可以处理乱序事件，根据事件时间对数据进行排序和处理，从而解决延迟问题。\n窗口操作：Flink的窗口操作可以根据事件时间或处理时间对数据流进行划分和处理，可以根据需要设定窗口大小和滑动间隔来控制延迟的处理。\nFlink支持哪些第三方集成？ Flink支持与多种第三方工具和框架的集成，包括：\nApache Kafka ：Flink可以与Kafka进行无缝集成，作为数据源和数据接收器。 Apache Hadoop ：Flink可以与Hadoop集成，可以读取Hadoop文件系统中的数据，也可以将处理结果写入Hadoop文件系统。 Apache Hive ：Flink可以与Hive集成，可以读取Hive表中的数据进行处理和分析。 Apache HBase ：Flink可以与HBase集成，可以读取和写入HBase中的数据。 Elasticsearch ：Flink可以与Elasticsearch进行集成，可以将处理结果写入Elasticsearch进行实时搜索和分析。 Flink的source和sink有哪些？ Flink支持多种数据源和数据接收器，包括：\n数据源：可以从文件系统、Kafka、消息队列等数据源读取数据，并将其转化为数据流进行处理。\n数据接收器：可以将处理结果输出到文件系统、数据库、Kafka等数据接收器中，或者发送给下游处理环节。\nFlink内置了一些基本数据源和接收器，它们始终可用。该预定义的数据源包括文件、目录和Socket,并可以加载集合和迭代器的数据。该预定义的数据接收器支持写入文件，输出信息和异常 。\nFlink支持哪些批处理操作？ Flink支持多种批处理操作，包括：\nMap：对数据集中的每个元素应用指定的函数。 Reduce：对数据集进行归约操作，将数据归约为一个结果。 Filter：根据指定的条件过滤数据集中的元素。 Join：将两个数据集按照指定的键进行连接操作。 GroupBy：根据指定的键对数据集进行分组操作。 Flink的流处理和批处理如何切换？ Flink 可以无缝地在流处理和批处理之间切换，这主要归功于其基于事件时间的窗口处理机制和灵活的作业调度策略。Flink 提供了两种作业类型：批处理作业和流处理作业。\n1. 批处理作业 ：\n批处理作业将数据作为有界数据集进行处理，类似于传统的批处理作业。在批处理模式下，Flink 会将数据按照批次进行划分，然后对每个批次进行离线处理。批处理作业通常用于处理历史数据或者定期生成统计报告等场景。\n要运行批处理作业，用户需要将数据作为批量文件上传到 Flink 的分布式文件系统（如 HDFS 或本地文件系统），然后通过 Flink 作业的方式进行处理。在批处理作业中，用户可以指定数据的截止时间（截止时间之前的数据会被处理），以及作业的并发度等参数。\n2. 流处理作业 ：\n流处理作业将数据作为无界数据流进行处理，实时处理数据并生成实时结果。在流处理模式下，Flink 会实时地接收数据，并将其分配给不同的 Task 以进行处理。流处理作业通常用于实时数据处理、实时分析和实时监控等场景。\n要运行流处理作业，用户需要将数据源（如 Kafka、Flume 等）与 Flink 集群配置好，然后通过 Flink 作业的方式进行处理。在流处理作业中，用户可以指定数据处理的时间窗口、触发器等参数，以实现实时数据处理的需求。\n在 Flink 中，批处理作业和流处理作业之间的切换可以通过修改作业的配置文件实现。例如，修改 batch.file.path 和 streaming.file.path 参数，以指定批处理作业和流处理作业的输入数据路径。此外，用户还可以通过 Flink Web UI 查看和管理作业的状态，以确保作业的正确运行。\n为什么使用 Flink 替代 Spark？ Flink 相对于 Spark 的优势主要体现在以下几个方面：\n低延迟和高吞吐量：Flink 是基于事件驱动的流式计算框架，能够支持低延迟和高吞吐量的数据处理。Flink 的低延迟特性得益于其基于时间窗口的调度机制，可以支持毫秒级的延迟时间。同时，Flink 的高吞吐量也是其优势之一，能够支持每秒千万级别的数据处理。 对流式数据应用场景更好的支持：Flink 专注于流式数据处理，能够更好地支持流式数据的应用场景，如实时计算、实时监控、实时推荐等。而 Spark 更适合于批量数据的处理，如离线分析、批量报告等。 处理乱序数据的能力：Flink 能够很好地处理乱序数据，可以在数据处理的过程中自动处理数据顺序不一致的问题。而 Spark 在处理乱序数据时需要进行额外的配置和处理。 保证 exactly-once 的状态一致性：Flink 可以保证 exactly-once 的状态一致性，即每个事件都会被处理一次且仅一次。而 Spark 在处理数据时存在重复处理的问题，需要进行额外的优化和配置才能保证状态一致性。 综上所述，Flink 相对于 Spark 在低延迟、高吞吐量、流式数据应用场景支持、处理乱序数据和保证状态一致性等方面具有优势，因此被越来越多的公司和开发者所采用。\nFlink是如何做到高效的网络数据交换的 Flink 在网络数据交换方面做到了高效，主要归功于以下几个方面：\n分布式数据交换 ：Flink 使用了基于 JobGraph 的分布式计算模型，数据可以在不同的 Task 中进行交互。这种分布式数据交换使得 Flink 能够充分利用集群中的多个节点来处理大规模的数据流，从而提高了整个系统的并行度和吞吐量。 TaskManager 负责数据交互 ：在 Flink 中，TaskManager 负责管理 Task 的执行和数据交互。TaskManager 会从缓冲区（Buffer）中收集 Records，然后将其发送到其他 Task 中。这种集中式的数据管理方式可以减少网络连接次数，从而提高了网络吞吐量。 批次封装 ：Flink 中的批次（Batching）机制可以将多个 Records 封装在一起，形成一个批次（Batch）。批次封装可以大大减少网络连接次数，因为在分布式场景中，网络 I/O 是一种稀缺资源。减少网络连接次数可以提高系统的吞吐量和并发度。实际上，在 Kafka 源码剖析中，我们也可以看到 Kafka 采用了类似的记录封装机制来提高吞吐量。 网络拥塞控制 ：Flink 在网络数据交换过程中还采用了拥塞控制机制，以避免网络过载。当某个节点的网络带宽占用过高时，Flink 会通过减少该节点的数据输出速率来缓解网络拥塞，从而确保整个系统的稳定运行。 自适应网络拓扑 ：Flink 支持自适应网络拓扑，它可以根据集群中节点的数量和位置动态地调整数据交换的路由策略。这种自适应网络拓扑可以提高系统的性能和可靠性，因为它能够更好地利用集群中的网络资源。 综上所述，Flink 在网络数据交换方面实现了高效，主要通过分布式数据交换、TaskManager 负责数据交互、批次封装、网络拥塞控制和自适应网络拓扑等机制来实现。这些机制使得 Flink 在处理大规模数据流时具有高吞吐量、高并发度和高可靠性。\nFlink程序在面对数据高峰期时如何处理 当 Flink 程序面对数据高峰期时，一种常用的方法是使用大容量的 Kafka 作为数据源，将数据先放到消息队列中，然后再使用 Flink 进行消费。这种方法可以有效地削峰平谷，减缓数据流量对 Flink 程序的影响，从而提高程序的稳定性和可靠性。\n不过，使用 Kafka 作为数据源会影响一点实时性。因为 Kafka 是一个异步的消息队列，数据在队列中需要等待消费者消费，所以会存在一定的延迟。为了解决这个问题，可以采用以下方法：\n调整 Kafka 的参数，如增大 Kafka 的缓存大小、增加 Kafka 的并发消费者数量等，以提高 Kafka 的吞吐量和处理能力。 优化 Flink 程序的配置，如增大 Flink 的并行度、调整 Flink 的内存配置等，以提高 Flink 的处理能力和吞吐量。 采用 Flink 中的 Stateful Functions 或 Checkpointing 功能，以保持数据的一致性和可靠性。Stateful Functions 可以让 Flink 程序对数据的处理具有状态感知能力，从而更好地处理数据流中的事件。而 Checkpointing 功能可以让 Flink 程序在处理数据时，定期将中间状态持久化到外部存储系统中，以便在程序失败时进行恢复。 综上所述，使用 Kafka 作为数据源可以有效地处理数据高峰期，但需要注意 Kafka 和 Flink 的配置优化，以及数据处理的实时性和一致性问题。\nFlink跟Spark Streaming的区别 这个问题是一个非常宏观的问题，因为两个框架的不同点非常之多。但是在面试时有非常重要的一点一定要回答出来：Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。\n下面我们就分几个方面介绍两个框架的主要区别：\n架构模型Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、Executor，Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。 任务调度Spark Streaming 连续不断的生成微小的数据批次，构建有向无环图DAG，Spark Streaming 会依次创建 DStreamGraph、JobGenerator、JobScheduler。Flink 根据用户提交的代码生成 StreamGraph，经过优化生成 JobGraph，然后提交给 JobManager进行处理，JobManager 会根据 JobGraph 生成 ExecutionGraph，ExecutionGraph 是 Flink 调度最核心的数据结构，JobManager 根据 ExecutionGraph 对 Job 进行调度。 时间机制Spark Streaming 支持的时间机制有限，只支持处理时间。 Flink 支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时间。同时也支持 watermark 机制来处理滞后数据。 容错机制对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰好一次处理语义。Flink 则使用两阶段提交协议来解决这个问题。 Flink怎么做压力测试和监控 产生的数据流的速度如果过快，而下游的算子消费不过来的话，会产生背压。背压的监控可以使用Flink Web UI来可视化监控Metrics，一旦报警就能知道。一般情况下可能由于sink这个操作符没有优化好，做一下优化就可以了。\n设置watermark的最大延迟时间这个参数，如果设置的过大，可能会造成内存的压力。可以设置最大延迟时间小一些，然后把迟到的元素发送到测输出流中，晚一点更新结果。\n还有就是滑动窗口的长度如果过大，而滑动距离很短的话，Flink的性能也会下降的厉害。可以通过分片的方法，将每个元素只存入一个“重叠窗口”，这样就可以减少窗口处理中状态的写入。\nFlink是通过什么机制实现的背压机制 Flink在运行时主要由operators和streams两大构件组成。每个operator会消费中间状态的流，并在流上进行转换，然后生成新的流。对于Flink的网络机制一种形象的类比是，Flink使用了高效有界的分布式阻塞队列，就像Java通过的阻塞队列（BlockingQueue）一样。使用BlockingQueue的话，一个较慢的接受者会降低发送者的发送速率，因为一旦队列满了（有界队列）发送者会被阻塞。\n在Flink中，这些分布式阻塞队列就是这些逻辑流，而队列容量通过缓冲池（LocalBufferPool）实现的。每个被生产和消费的流都会被分配一个缓冲池。缓冲池管理者一组缓冲（Buffer），缓冲在被消费后可以被回收循环利用。\nFlink是如何处理反压的？如何监控和发现？ Flink 的反压（Backpressure）是指当一个 Operator 的输出速度比其下游 Operator 的输入速度慢时，下游 Operator 可能会积累一定数量的数据，导致处理速度变慢，甚至堵塞。为了解决这个问题，Flink 引入了反压机制，以便及时发现并解决数据处理速度不匹配的问题。\nFlink 在处理反压问题时，并没有使用复杂的机制，而是采用了一种简单而高效的方法。Flink 在数据传输过程中使用了分布式阻塞队列，从而有效地解决了反压问题。\n在 Flink 中，当一个算子的输出速度比下游算子快时，Flink 会使用分布式阻塞队列来缓存输出数据。这样可以避免上游算子过快地生成数据，导致下游算子无法及时处理，从而形成反压。当下游算子需要数据时，它会从队列中取出数据并进行处理。当队列中的数据达到一定阈值时，上游算子会收到通知，从而减缓数据生成速度。这样一来，Flink 就通过分布式阻塞队列实现了反压的缓解。\n另外，Flink 通过每个 TaskManager 和 JobManager 之间的通信来实现反压的缓解。当下游处理任务时间太长时，Flink 会检测到这种情况，并认为这是一个反压信号。此时，Flink 会将这个反压信号传递给上游任务的管理器。\n具体来说，Flink 的反压策略主要分为以下几个步骤：\n任务反压 ：当下游任务的处理速度较慢时，Flink 会检测到这种情况，并认为这是一个反压信号。此时，Flink 会将这个反压信号传递给上游任务的管理器。 调整数据生成速度 ：当上游任务的管理器收到反压信号后，会根据反压信号的强度来调整数据生成速度。通常情况下，反压信号越强，上游任务生成的数据量就会减少，以减轻下游任务的负担。 控制反压 ：Flink 还会通过一些控制机制来避免过度反压。例如，当上游任务的数据生成速度过慢时，Flink 会限制反压的强度，以避免数据积压过多。此外，Flink 还会设置一个反压阈值，当反压信号超过这个阈值时，Flink 会认为任务已经处于一个不稳定的状态，并会采取相应的措施，如调整任务并行度、暂停任务等。 恢复数据生成速度 ：当下游任务的处理速度恢复到正常水平时，Flink 会检测到这个变化，并逐渐增加上游任务的数据生成速度，以恢复数据流。 可以根据下游任务的处理速度来动态调整上游任务的数据生成速度，以缓解数据积压问题。这种策略在实际应用中可以提高 Flink 任务的处理效率和稳定性。\nFlink 的反压监控和发现主要通过以下方式进行：\nFlink Web UI ：Flink Web UI 是一个基于 Web 的用户界面，用于管理和监控 Flink 集群。在 Flink Web UI 中，用户可以查看作业的运行状态、任务管理信息以及反压状态。具体地，在“Jobs”页面中，用户可以查看每个作业的 Backpressure 状态，包括 OK、LOW 和 HIGH 三种状态。此外，在“Task Managers”页面中，用户还可以查看每个 TaskManager 的心跳信息和反压状态。 Flink 命令行工具 ：除了 Web UI 外，用户还可以使用 Flink 提供的命令行工具（如“flink”和“jobmanager”）来进行反压监控。例如，使用“jobmanager”命令，用户可以查看作业的详细信息，包括任务状态和反压状态。 第三方监控工具 ：除了 Flink 自带的监控工具外，还有一些第三方的 Flink 监控工具可以帮助用户监控反压状态。例如，Apache Kafka 提供了一个名为“Kafka Console Consumer”的工具，用于查看 Kafka 主题的消费情况。通过这个工具，用户可以了解数据生产的速度，从而判断是否存在反压问题。 自定义监控与报警 ：为了更加实时和准确地监控反压状态，用户可以编写自定义的监控与报警脚本。这些脚本可以定期获取 Flink 集群的状态信息，并根据预设的规则发送告警通知。例如，当发现某个 Operator 的反压状态为 HIGH 时，可以自动发送告警邮件给相关人员。 总之，Flink 通过 Web UI、命令行工具、第三方监控工具以及自定义监控与报警等多种方式，帮助用户实时监控和发现反压问题，从而确保数据处理的高效和稳定。\nFlink中的Window出现了数据倾斜，你有什么解决办法？ Flink 中的窗口操作是一种基于时间窗口的数据处理方式，可以用于统计分析、监控、实时计算等应用场景。然而，当数据量过大或者数据发送速度不均匀时，可能会导致窗口中堆积的数据量相差过多，即出现了数据倾斜的情况。\n数据倾斜会对 Flink 的性能产生负面影响，因为窗口计算需要对所有数据进行聚合操作，而数据倾斜会导致部分窗口的数据量过大，从而增加计算时间和资源消耗。为了解决数据倾斜问题，可以采用以下几种方法：\n在数据进入窗口之前做预聚合 ：这种方法可以在数据进入窗口之前，先进行一定的聚合操作，使得每个窗口中的数据量相对均匀。具体的做法可以是在数据源处进行预聚合，或者在 Flink 中使用 DataStream API 中的窗口聚合函数（如 Tumbling Windows 和 Sliding Windows）进行预聚合。 重新设计窗口聚合的 key ：在某些情况下，窗口聚合的 key 可能需要进行重新设计，以避免数据倾斜。例如，可以将 key 设计为数据发送的时间戳，而不是数据本身的某些属性。这样可以使得窗口中的数据量更加均匀，从而避免数据倾斜。 调整窗口参数 ：在某些情况下，可以通过调整窗口参数来避免数据倾斜。例如，可以增加窗口的大小或者增加窗口的滑动间隔，使得窗口中的数据量更加均匀。 使用 Flink 的 Ttl 操作 ：Flink 中的 Ttl 操作可以在数据到达窗口时，根据数据的时间戳进行淘汰操作，从而避免数据倾斜。具体的做法是设置一个 Ttl 时间，当数据到达窗口时，如果数据的时间戳已经超过了 Ttl 时间，则将该数据淘汰，从而避免数据倾斜。 综上所述，解决 Flink 中的窗口数据倾斜问题需要根据具体情况进行分析和处理。可以采用预聚合、重新设计窗口聚合的 key、调整窗口参数或者使用 Flink 的 Ttl 操作等方法来避免数据倾斜，从而提高 Flink 的性能和可靠性。\n使用KeyBy算子时，某一个Key的数据量过大，导致数据倾斜，怎么处理？ 当使用 KeyBy 算子时，如果某个 Key 的数据量过大，会导致数据倾斜，影响计算效率。为了解决这个问题，可以考虑以下方法：\n将 Key 进行散列，将 Key 转换为 Key-随机数的形式，这样可以保证数据散列，对打散后的数据进行聚合统计。这时，我们会得到原始的 Key 加上随机数的统计结果。 将散列的 Key 去除拼接的随机数，得到原始的 Key，然后进行二次 KeyBy 进行结果统计。这样可以保证数据倾斜不会影响最终的结果。 Flink在使用聚合函数之后出现数据热点怎么解决 Flink 在使用聚合函数之后出现数据热点的问题，主要是由于某些聚合函数的计算量比较大，导致数据处理速度较慢，从而产生了数据积压和延迟。这种情况下，可以通过以下几种方法来解决数据热点问题：\n增加计算资源 ：增加计算节点和内存资源，提高 Flink 集群的计算能力，从而加快数据处理速度，降低数据积压和延迟。 调整聚合函数参数 ：有些聚合函数的计算量比较大，可以考虑调整聚合函数的参数，减少计算量，从而提高数据处理速度。例如，可以调整窗口的大小或者滑动间隔等参数。 使用批量处理 ：将数据按照一定的时间间隔进行批量处理，降低实时处理的压力，从而减少数据积压和延迟。例如，可以使用 Flink 的 Batch 操作进行批量处理。 采用数据重复消除策略 ：在某些情况下，数据热点可能是由于某些数据的重复导致的。可以采用数据重复消除策略，例如使用 Flink 中的 Checkpointing 操作，从而避免重复数据导致的数据热点问题。 调整数据源参数 ：在某些情况下，数据源的参数设置可能导致数据热点问题。可以调整数据源的参数，例如发送数据的间隔时间、数据压缩等方式，从而降低数据热点问题。 综上所述，解决 Flink 中的数据热点问题需要根据具体情况进行分析和处理。可以采用增加计算资源、调整聚合函数参数、使用批量处理、采用数据重复消除策略或者调整数据源参数等方法来解决数据热点问题，从而提高 Flink 的性能和可靠性。\nFlink 任务延迟高，想解决这个问题，你会如何入手？ 如果 Flink 任务延迟高，需要从以下几个方面入手进行优化：\n资源调优 ：首先检查 Flink 集群的资源使用情况，如果发现某些节点资源使用率过高，可以考虑增加节点数量或者调整节点的资源配置，如增加内存、CPU 等。此外，还可以调整任务管理器的资源分配策略，如优先使用空闲节点等。 算子调优 ：如果任务延迟高，可以考虑调整算子的参数，如窗口时长、并发数等。窗口时长越短，计算量越大，可能会导致延迟增加，因此需要根据具体情况进行调整。同时，可以考虑使用更高效的算子，如 Reducer 的并行度可以调整为 taskNumber 的因子等。 数据优化 ：数据优化是提高 Flink 任务性能的重要手段。可以考虑使用数据压缩、数据筛选、数据重复消除等技术，以减少数据量和计算量。同时，还可以考虑使用批量处理、Checkpointing 等技术，以优化数据处理流程。 任务调度优化 ：任务调度优化也是提高 Flink 任务性能的重要手段。可以考虑使用 Flink 自带的调度器，如 FairScheduler、DynamicTaskAllocation 等，这些调度器可以根据不同的策略分配任务和管理器。此外，还可以使用自定义的调度器，如基于优先级、基于资源使用情况等调度器，以优化任务调度。 错误处理 ：如果任务出现错误，可能会导致延迟增加。因此，需要设置正确的错误处理策略，如使用 try-catch 语句、设置错误处理延迟等，以避免错误导致的延迟增加。 综上所述，要解决 Flink 任务延迟高的问题，需要从资源调优、算子调优、数据优化、任务调度优化和错误处理等方面入手，以提高 Flink 任务的性能和可靠性。\nFlink是如何保证Exactly-once语义的？ 如果下级存储支持事务 ：\nFlink可以通过实现两阶段提交和状态保存来实现端到端的一致性语义。\n分为以下几个步骤：\n开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面 预提交（preCommit）将内存中缓存的数据写入文件并关闭 正式提交（commit）将之前写完的临时文件放入目标目录下。这代表着最终的数据会有一些延迟 丢弃（abort）丢弃临时文件 若失败发生在预提交成功后，正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。 下级存储不支持事务 ：\n端到端的exactly-once对sink的要求比较高，具体的实现主要有幂等写入和事务性写入两种方式。幂等写入的场景依赖于业务逻辑，更常见的是用事务性写入。而事务性写入又有预写日志（WAL）和两阶段提交（2PC）两种方式。\n如果外部系统不支持事务，那么可以使用预写日志的方式，把结果数据当成状态保存，然后在收到checkpoint完成的通知时，一次性写入sink系统。\n说说Flink的状态 在 Flink 中，状态是指在实时计算过程中，用于存储和处理数据的一种机制。状态可以分为两种基本类型：KeyedState 和 OperatorState。\nKeyedState ：是基于键（Key）的状态，通常和 KeyedStream 的操作相关。KeyedState 包含两种基本的状态：ValueState 和 MapState。ValueState 用于存储单一值的状态，而 MapState 用于存储映射关系。在实际生产中，通常使用 KeyedState 中的 ValueState 和 MapState。 OperatorState ：是基于算子（Operator）的状态，通常和非 KeyedStream 的操作相关。OperatorState 可以存储算子的内部状态，例如窗口状态、累加器等。 KeyedState 和 OperatorState 都是 Flink 中的状态类型，它们在实时计算中起到了重要的作用。KeyedState 通常用于处理基于键的数据，例如对某个键进行计数、聚合等操作；而 OperatorState 通常用于处理非基于键的数据，例如对数据进行窗口操作、排序等操作。\n在学习 Flink 中的状态时，需要了解状态的基本概念、分类、使用方式以及状态管理的相关概念。同时，需要掌握如何在程序中使用 KeyedState 和 OperatorState，以便在实时计算中处理数据。\n介绍一下Flink的CEP机制 Flink 的 CEP（Complex Event Processing，复杂事件处理）机制主要用于处理实时数据流中的复杂事件，以便实时地计算和响应这些事件。与传统的批处理方式不同，CEP 机制可以处理实时数据流中的事件，并根据事件的复杂逻辑进行实时计算和响应。\nFlink CEP 是在 Flink 中实现的复杂事件处理（CEP）库。CEP 允许在无休止的事件流中检测事件模式，让我们有机会掌握数据中重要的部分。一个或多个由简单事件构成的事件流通过一定的规则匹配，然后输出用户想得到的数据——满足规则的复杂事件。\nFlink 的 CEP 机制主要依赖于两个核心组件：Flink 的流处理框架和 CEP 库。Flink 的流处理框架提供了低延迟、高吞吐量的数据流处理能力，可以处理海量的实时数据。而 CEP 库则提供了处理复杂事件的逻辑，可以实现事件的过滤、聚合、路由等功能。通过这两个组件的结合，Flink 能够实现对实时数据流中复杂事件的实时处理和响应。\nFlink 的 CEP 机制具有以下特点：\n实时性 ：Flink 的 CEP 机制可以处理实时数据流中的事件，并实时计算和响应这些事件，具有非常低的延迟。 灵活性 ：CEP 库提供了灵活的事件处理逻辑，可以根据具体的业务需求定义事件的处理方式，例如：过滤、聚合、路由等。 可扩展性 ：Flink 的流处理框架具有优秀的水平扩展能力，可以根据数据流的规模和处理需求动态地增加或减少计算资源。 高可用性 ：Flink 的 CEP 机制支持故障恢复，可以在应用程序出现故障时自动恢复，避免数据丢失和影响。 流式处理 ：Flink 的 CEP 机制采用流式处理的方式处理实时数据流，可以实时地计算和响应事件，不需要先收集所有数据再进行批处理。 Flink 的 CEP 机制在实际应用中可以广泛应用于金融、物联网、物流等行业，例如：实时计算股票交易数据、实时监测传感器数据、实时路由物流信息等。了解 Flink 的 CEP 机制有助于更好地应对实时数据流中的复杂事件处理需求。\nFlink CEP 编程中当状态没有到达的时候会将数据保存在哪里 在 Flink CEP 编程中，当状态没有到达的时候，数据通常会被保存在内存中。这是因为在流式处理中，CEP 需要支持 EventTime，也就需要支持数据的迟到现象，这就需要使用 Watermark 机制来处理。对于未匹配成功的事件序列的处理，和迟到数据是类似的。在 Flink CEP 的处理逻辑中，状态没有满足的和迟到的数据，都会存储在一个 Map 数据结构中。\n这种内存存储数据的方式在处理延迟数据时是必要的，但也确实可能会对内存造成一定的损伤。为了降低内存占用，可以采取以下策略：\n合理设置状态的时间间隔 ：根据业务需求和数据处理的实际情况，合理设置状态的时间间隔，以减少内存中存储的数据量。 使用外部状态存储 ：将状态数据存储在外部状态存储中，如 Redis、HBase 等，以减轻内存压力。 优化 CEP 算法 ：对 CEP 算法进行优化，使其在处理延迟数据时能更有效地利用内存，降低内存占用。 合理设置 Flink 的并行度 ：根据实际硬件资源情况和数据处理需求，合理设置 Flink 的并行度，以平衡内存占用和处理速度之间的关系。 在 Flink CEP 编程中，当状态没有到达的时候，数据会被保存在内存中。为了降低内存占用，可以采取合理设置状态时间间隔、使用外部状态存储、优化 CEP 算法以及合理设置 Flink 并行度等策略。\nFlink的并行度是什么？Flink的并行度设置是怎么样的？ Flink 的并行度是指在执行算子时，可以同时处理的数据流分片的数量。通过设置并行度，可以充分利用集群中的多个 TaskSlots（任务槽）来执行多个数据流分片，从而提高计算性能。并行度这个概念很好理解，例如 Kafka Source，它的并行度默认就是它的分区数量。\nFlink 的并行度设置可以通过算子内部的参数或者外部的配置进行调整。\n下面是一些常见的设置方法：\n对于内置算子，如 Map、Filter、Reduce 等，可以通过算子函数的参数来设置并行度。例如，在 Map 算子中，可以使用 map_function.parallelism 参数来设置并行度。 对于自定义算子，可以通过实现 ParallelismAware 接口来设置并行度。在实现该接口的过程中，需要实现 get_parallelism 方法，该方法返回算子的并行度。 在 Flink 的配置文件（如 flink-config.yaml）中，可以设置整个任务的并行度。例如，可以使用 parallelism.task.num 参数来设置 TaskSlots 的数量，从而影响算子的并行度。 一般情况下，我们应该根据数据量来设置并行度。对于源算子（如 Kafka Source、HDFS Source 等），它们的并行度通常可以与分区数量保持一致，因为源算子通常不会产生太多的数据量。对于中间算子（如 Map、Filter 等），并行度可以根据数据量的大小进行适当调整。对于聚合算子（如 Reduce、Aggregate 等）和连接算子（如 Join 等），并行度通常需要根据数据量的大小和算子的压力来综合考虑。 合理地设置并行度可以充分发挥 Flink 的并行计算优势，提高数据处理的性能。\n说说Flink的分区策略 Flink 提供了多种分区策略以满足不同数据处理的需求。以下是详细的叙述：\nGlobalPartitioner ：将数据发到下游算子的第一个实例。这种分区器适用于数据处理过程中只需要一个实例处理的情况。 ShufflePartitioner ：将数据随机分发到下游算子。这种分区器适用于数据处理过程中需要对数据进行随机分发的情况，例如数据去重或数据混淆等。 RebalancePartitioner ：将数据循环发送到下游的实例。这种分区器适用于数据处理过程中需要对数据进行循环处理的情况，例如数据清洗或数据转换等。 RescalePartitioner ：根据上下游算子的并行度，循环输出到下游算子。这种分区器适用于数据处理过程中需要根据算子的并行度进行数据分配的情况，例如数据聚合或数据过滤等。 BroadcastPartitioner ：输出到下游算子的每个实例中。这种分区器适用于数据处理过程中需要将数据广播到所有实例中的情况，例如数据源或数据收集等。 ForwardPartitioner ：上下游算子并行度一样。这种分区器适用于数据处理过程中需要保持上下游算子的并行度一致的情况，例如数据窗口或数据排序等。 KeyGroupStreamPartitioner ：按 Key 的 Hash 值输出到下游算子。这种分区器适用于数据处理过程中需要根据 Key 的哈希值进行数据分区的情况，例如数据分组或数据汇总等。 KeyedStream ：根据 keyGroup 索引编号进行分区，会将数据按 Key 的 Hash 值输出到下游算子实例中。该分区器不是提供给用户来用的，而是 Flink 内部使用的。 CustomPartitionerWrapper ：用户自定义分区器。这种分区器需要用户自己实现 Partitioner 接口，来定义自己的分区逻辑。适用于数据处理过程中需要根据特定逻辑进行数据分区的情况。 Flink 提供了多种内置的分区器以满足常见的数据处理需求，同时也支持用户自定义分区器以满足特定需求。\n说说Flink的资源调度 Flink 的资源调度是基于 TaskManager 和 Task slot 的概念进行的。TaskManager 是 Flink 中最小的调度单元，负责管理和调度任务。而 Task slot 则是 TaskManager 中最细粒度的资源，代表了一个固定大小的资源子集。每个 TaskManager 会将其所占有的资源平分给它的 slot。通过调整 task slot 的数量，用户可以定义 task 之间是如何相互隔离的。\n每个 TaskManager 有一个 slot，也就意味着每个 task 运行在独立的 JVM 中。这样做的好处是，任务之间的隔离更加明确，一个任务出现问题不会影响到其他任务。同时，独立的 JVM 可以提供更好的资源管理和垃圾回收。\n而当 TaskManager 拥有多个 slot 时，多个 task 可以运行在同一个 JVM 中。这样做的好处是，可以共享 TCP 连接（基于多路复用）和心跳消息，从而减少数据的网络传输。此外，同一个 JVM 进程中的 task 还可以共享一些数据结构，从而减少每个 task 的消耗。\n在 Flink 中，每个 slot 可以接受单个 task，也可以接受多个连续 task 组成的 pipeline。例如，FlatMap 函数占用一个 taskslot，而 key Agg 函数和 sink 函数共用一个 taskslot。这种灵活的资源调度方式可以根据不同的任务需求进行优化和配置，提高系统的资源利用率和性能。\n总之，Flink 的资源调度是通过 TaskManager 和 Task slot 的概念来实现的，通过调整 task slot 的数量和分配方式，可以满足不同任务的需求，提高系统的资源利用率和性能。\n如下图所示，FlatMap函数占用一个taskslot，而key Agg函数和sink函数共用一个taskslot：\nFlink中有没有重启策略 Flink 中的重启策略用于在程序运行过程中发生故障时，如何重新启动算子以恢复程序的运行。重启策略可以在 flink-conf.yaml 中配置，也可以在应用代码中动态指定。\n以下是 Flink 中常见的四种重启策略：\n故障延迟重启策略 （Failure Delay Restart Strategy）：当一个算子失败时，该策略会等待一个固定的时间间隔（即延迟时间）后，重新启动该算子。如果在延迟时间内，同一个算子再次失败，则会重新计算延迟时间，并将其设置为之前的两倍。这个过程会一直重复，直到延迟时间达到一个最大的值（通常是 60 秒），此时 Flink 会放弃重启该算子，并将其标记为永久失败。 故障率重启策略 （Failure Rate Restart Strategy）：该策略基于算子的失败率来决定是否重新启动算子。当一个算子的失败率超过一个预设的阈值时，Flink 会重新启动该算子。这个策略适用于那些可能因为数据异常或程序 BUG 导致频繁失败的算子。 没有重启策略 （No Restart Strategy）：当一个算子失败时，该策略不会重新启动该算子，而是直接跳过该算子，继续执行后面的算子。这个策略适用于那些可以在失败后被忽略的算子，例如那些只是输出数据的算子。 Fallback 重启策略 （Fallback Restart Strategy）：当一个算子失败时，该策略会尝试重新启动该算子，如果重启失败，则会 fallback 到之前的版本，即不会重新启动该算子。这个策略适用于那些可能因为程序升级导致失败的算子，以便在重启失败时能够回滚到之前的版本。 如果没有启用 checkpointing，则使用无重启（no restart）策略。如果启用了 checkpointing，但没有配置重启策略，则使用固定间隔（fixed-delay）策略。在固定间隔策略中，Flink 会等待一个固定的时间间隔后，重新启动失败的算子。这个时间间隔可以通过 flink-conf.yaml 中的 restart.delay 配置项来设置。\nFlink假如遇到程序异常重启怎么办 Flink 的分布式缓存有什么作用？如何使用？ 说说Flink中的广播变量，使用广播变量的时候需要注意什么 说说Flink Operator Chains Flink什么情况下才会把Operator chain在一起形成算子链 Flink序列化如何实现 Flink需要依赖于Hadoop吗 Flink组件栈有哪些 Flink支持哪些机器学习和图处理库 说说Flink 运行时组件 Flink的API可分为哪几层 Flink中应用在table API中的UDF有几种 说说Flink SQL的实现原理 说说Flink任务提交流程 Flink-On-Yarn常见的提交模式有哪些，分别有什么优缺点？ 说说Flink的执行图 说说Flink的CBO，逻辑执行计划和物理执行计划 什么是Flink的全局快照？为什么需要全局快照？ Flink维表关联怎么做 Flink如何海量key去重 说说Flink的RPC ","date":"2025-03-12T11:52:29+08:00","image":"https://sherlock-lin.github.io/p/flink%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/leaf-8687801_1280_hu4746464102952735896.jpg","permalink":"https://sherlock-lin.github.io/p/flink%E9%9D%A2%E8%AF%95%E4%B8%80%E6%96%87%E9%80%9A/","title":"Flink面试一文通"},{"content":"概述 二叉树是计算机科学中常见的数据结构，其便利方式包括前序遍历、中序遍历、后续遍历和层序遍历。不同遍历方式适用于不同问题，合理选择合适的遍历方式可以大大提高算法的效率和可读性。\n二叉树遍历代码框架 递归框架 迭代框架 递归方式虽然直观，但在数据规模较大时可能导致栈溢出，因此可以使用迭代方式来遍历二叉树。\n前序遍历\n层序遍历\n题型分析 前序遍历 需要先处理当前节点，再处理子节点，例如：\n树的构造问题：根据前序遍历和中序遍历重建二叉树 树的序列化与反序列化：先访问根节点可以用于将树编码成字符串 105. 从前序与中序遍历序列构造二叉树 297. 二叉树的序列化与反序列化 144. 二叉树的前序遍历 589. N 叉树的前序遍历 1008. 前序遍历构造二叉搜索树 中序遍历 94. 二叉树的中序遍历 230. 二叉搜索树中第 K 小的元素 99. 恢复二叉搜索树 173. 二叉搜索树迭代器 897. 递增顺序搜索树 后序遍历 145. 二叉树的后序遍历 124. 二叉树中的最大路径和 543. 二叉树的直径 337. 打家劫舍 III（树形DP） 236. 二叉树的最近公共祖先 层序遍历 102. 二叉树的层序遍历 107. 二叉树的层序遍历 II 199. 二叉树的右视图 637. 二叉树的层平均值 111. 二叉树的最小深度 ","date":"2025-03-12T11:51:39+08:00","image":"https://sherlock-lin.github.io/p/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86%E4%B8%80%E6%96%87%E9%80%9A/grebe-7972183_1280_hu8909851503123052803.jpg","permalink":"https://sherlock-lin.github.io/p/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86%E4%B8%80%E6%96%87%E9%80%9A/","title":"二叉树遍历一文通"},{"content":"概述 识别问题是否使用DP 最优子结构：问题的最优解可以由子问题的最优解推导得出。 重叠子问题：相同子问题被重复计算，适合用DP进行优化。 定义状态变量：找到递归关系，明确DP数组的含义 确定状态转移方程：通过归纳法找到状态转移公式。 确定初始条件：找出边界情况的最优解。 选择计算顺序： 自顶向下 递归+记忆化搜索 自底向上 迭代+DP数组 优化空间复杂度(可选)：通过滚动数组或状态压缩减少内存占用。 动态规划的代码框架 递归+记忆化搜索 迭代+动态规划数组 滚动数组优化(空间优化) 题型分析 线性动态规划 线性动态规划指的是状态按照一维顺序递推的动态规划问题，通常适用于数组或序列问题。\n常见的线性DP主要包括：\n前缀型DP(如最大数组和) 计数型DP(如爬楼梯、不同路径) 最优子结构型DP(如最小路径和) 线性DP通用解题步骤\n53. 最大子数组和 70. 爬楼梯 64. 最小路径和 62. 不同路径 343. 整数拆分 300. 最长递增子序列 121. 买卖股票的最佳时机 198. 打家劫舍 213. 打家劫舍 II 337. 打家劫舍 III 2560. 打家劫舍 IV 区间动态规划 区间动态规划是一类用于处理序列或区间的最优划分问题的动态规划，区间DP的特点有：\n适用于求解子区间的最优解，然后通过合并子区间推导出更大区间的最优解。 采用双层循环遍历区间长度，通常使用三层循环遍历区间端点。 适用于合并问题、切割问题等。 区间DP通用解题步骤\n1000. 合并石头的最低成本 312. 戳气球 1541. 平衡括号字符串的最少插入次数 背包问题 背包问题是一类组合优化问题，核心在于如何在有限容量的情况下选择物品，使得总价值最大或者总方法数最多。\n根据不同约束条件，背包问题可以分为以下几类：\n0-1背包：每个物品只能选0或1次 完全背包：每个物品可以选无限次 多重背包：每个物品最多选有限次 多组背包：物品被分为多个组，每个组最多选一个 多维背包：物品有多个属性限制，如重量、体积等 变种背包：如零钱兑换、目标和、子集和问题等 子序列类动态规划 子序列问题问题通常涉及序列的部分元素组合，通用解题思路如下\n300. 最长递增子序列 1143. 最长公共子序列 72. 编辑距离 516. 最长回文子序列 354. 俄罗斯套娃信封问题 划分类动态规划 划分类动态规划是一种将问题拆分为多个子区间或子短的动态规划方法，通常用于求解\n解题思路如下\n132. 分割回文串 II 410. 分割数组的最大值 698. 划分为k个相等的子集 ","date":"2025-03-10T21:16:22+08:00","image":"https://sherlock-lin.github.io/p/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B8%80%E6%96%87%E9%80%9A/matt-le-SJSpo9hQf7s-unsplash_hu10664154974910995856.jpg","permalink":"https://sherlock-lin.github.io/p/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B8%80%E6%96%87%E9%80%9A/","title":"动态规划一文通"},{"content":"概述 双指针是一种常见的算法技巧，适用于数组、字符串、链表等数据结构，主要用于优化时间复杂度，减少不必要的遍历。在O(n*n)复杂度的波阿里解法中，很多问题可以通过双指针优化为O(n)甚至更优的解法。\n双指针主要分为以下几类\n左右指针：左右指针就是两个指针相向而行或者相背而行，用于排序数组/字符串，常见问题：两数之和、回文判断等 快慢指针：就是两个指针同向而行，一快一慢；用于寻找循环、确定中间位置等，常见问题有链表环检测、寻找链表中点等 滑动窗口：快慢指针的特例，用于处理子数组问题，如最长子串、不含重复字符的子数组 双指针的代码框架 左右指针代码框架 快慢指针代码框架 滑动窗口代码框架 题型分析 左右指针 对于有序数组、字符串相关的问题，左右指针分别从两端向中间靠拢，逐步逼近答案。\n1. 两数之和: 如果支持返回目标数据则可以通过左右指针方式解决，如果是只支持通过下标方式则只能通过Hash 167. 两数之和 II - 输入有序数组: 通过左右指针完成即可 15. 三数之和: 左右指针思路，先对数据进行排序，然后遍历第一个数，剩下的两个数用左右指针来查找，需要注意的是可能存在重复数据导致重复结果，因此查找到数据后三个索引都要做去重判断 125. 验证回文串 344. 反转字符串 11. 盛最多水的容器:先移动高度低的那边，因为移动高度低的即便两者中间有更高的，两个更高的才能算出更大的面积; 不需要保留两边最大的高度，同时比较要用数组具体的值来比而不是比下标 42. 接雨水:记得有三种，暴力、备忘录、双指针(取左右两边最大值中的最小值减去当前柱的高度就是防水的容量) 快慢指针 适用于链表、数组，常用于寻找环、寻找链表的中点等，对于单链表来说，大部分技巧都属于快慢指针\n141. 环形链表: 快慢指针，如果相遇的话说明链表是存在环的 142. 环形链表 II: 快慢指针从头节点出发，快指针走两步，慢指针走一步，两个指针相遇时，将快指针重置到头节点，然后快指针走一步，慢指针走一步，再次相遇时就是入口节点 876. 链表的中间结点: 慢指针一次走一步，快指针一次走两步 160. 相交链表: A+B=B+A的思路，由于相交后的路程是相同的，差异是在于两条链表相交前的长度差异，那就让两个指针在遍历完各自的链表后，再交换遍历一次，这样他们一定会在交换遍历时候相遇 83. 删除排序链表中的重复元素 202. 快乐数 234. 回文链表 143. 重排链表 287. 寻找重复数 26. 删除有序数组中的重复项 27. 移除元素: 滑动窗口的条件非常重要 283. 移动零: 跟移动元素相同，只不过条件从是否符合指定元素变为是否为0 滑动窗口 适用于字符串、子数组问题，在优化子数组大小、寻找最长/最短子串时使用。\n3. 无重复字符的最长子串 438. 找到字符串中所有字母异位词 76. 最小覆盖子串 209. 长度最小的子数组 424. 替换后的最长重复字符 567. 字符串的排列 30. 串联所有单词的子串 206. 反转链表: 滑动窗口思路，三个指针同时往后一步步遍历，每移动一次就变更这个三个指针所负责节点的指向，变更完后三个指针再继续往后移动 左右指针 左右指针就是两个指针相向而行或者相背而行，用于排序数组/字符串，常见问题：两数之和、回文判断等\n167. 两数之和 II - 输入有序数组 167. 两数之和 II - 输入有序数组\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public int[] twoSum(int[] numbers, int target) { if (numbers.length \u0026lt;= 1) { return new int [] {-1, -1}; } int len = numbers.length; int left = 0, right = len-1; while(left \u0026lt; right) { int sum = numbers[left]+numbers[right]; if (sum == target) { return new int []{left+1, right+1}; } else if (sum \u0026gt; target) { right--; } else { left++; } } return new int [] {-1, -1}; } } 15. 三数之和 15. 三数之和\n左右指针思路，先对数据进行排序，然后遍历第一个数，剩下的两个数用左右指针来查找，需要注意的是可能存在重复数据导致重复结果，因此查找到数据后三个索引都要做去重判断。\n核心思想：排序以及轮询第一个数，后面两个数通过左右指针进行计算\n注意事项：1. 在命中数据是，要通过三个while循环分别过滤掉三个数下一步中的重复元素，并且左右指针要双向同时进行移动，因为在i不变的情况下，left变right不变的话，数据的和肯定是无法再次命中0的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; threeSum(int[] nums) { int length=nums.length; if (length \u0026lt; 3) return Collections.emptyList(); Arrays.sort(nums); int left=0, right=length-1; List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); for (int i=0; i\u0026lt;=length-3; i++) { left=i+1; right = length-1; while(left\u0026lt;right) { if (nums[i]+nums[left]+nums[right] == 0) { result.add(Arrays.asList(nums[i], nums[left], nums[right])); while(i \u0026lt; length-1 \u0026amp;\u0026amp; nums[i]==nums[i+1])i++; while(left+1 \u0026lt; length-1 \u0026amp;\u0026amp; nums[left] == nums[left+1]) left++; while(right \u0026gt; i \u0026amp;\u0026amp; nums[right] == nums[right-1])right--; left++; right--; } else if (nums[i]+nums[left]+nums[right] \u0026gt; 0) { right--; } else { left++; } } } return result; } } 验证回文字符串(字符串每个元素都是字母) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public boolean isPalindrome(String s) { if (s == null || s.length() == 1) { return true; } char[] charArray = s.toCharArray(); int left = 0, right = charArray.length; while(left \u0026lt; right) { if (charArray[left] != right) { return false; } left++; right--; } return true; } } 125. 验证回文串 125. 验证回文串\n先将回文串做清洗，仅保留字母和数字，再通过左右指针进行判断，判断的时候要记得同时转换成小写比较\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public boolean isPalindrome(String s) { StringBuffer sgood = new StringBuffer(); int length = s.length(); for (int i = 0; i \u0026lt; length; i++) { char ch = s.charAt(i); if (Character.isLetterOrDigit(ch)) { sgood.append(Character.toLowerCase(ch)); } } int n = sgood.length(); int left = 0, right = n - 1; while (left \u0026lt; right) { if (Character.toLowerCase(sgood.charAt(left)) != Character.toLowerCase(sgood.charAt(right))) { return false; } ++left; --right; } return true; } } 344. 反转字符串 344. 反转字符串\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public void reverseString(char[] s) { if (s.length == 0) { return; } int left = 0, right = s.length-1; while(left \u0026lt; right) { char tempChar = s[left]; s[left] = s[right]; s[right] = tempChar; left++; right--; } return; } } 11. 盛最多水的容器 11. 盛最多水的容器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Solution { public int maxArea(int[] height) { if (height.length == 0) return 0; int left=0, right = height.length-1, result=0; while(left \u0026lt; right) { result = Math.max(Math.min(height[left], height[right])*(right-left), result); if (height[left] \u0026gt; height[right]) { right--; } else { left++; } } return result; } } 42. 接雨水 42. 接雨水\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Solution { public int trap(int[] height) { if(height.length == 0) return 0; int len = height.length; int []lMax = new int[len]; int []rMax = new int[len]; int ret=0; lMax[0] = height[0]; rMax[len-1] = height[len-1]; for(int i=1; i\u0026lt;len; i++){ lMax[i]=Math.max(height[i], lMax[i-1]); } for(int i=len-2; i\u0026gt;0; i--) { rMax[i]=Math.max(height[i], rMax[i+1]); } for(int i=1; i\u0026lt;len-1; i++) { ret+=Math.min(lMax[i], rMax[i])-height[i]; } return ret; } } 快慢指针 141. 环形链表 141. 环形链表\n快慢指针即可，遍历链表过程中判断两个指针是否相等。由于快慢指针的步差是1步，并且链表的节点数是整数，所以如果存在环，则两个指针一定会相遇。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class Solution { public boolean hasCycle(ListNode head) { if(head == null || head.next ==null) { return false; } ListNode fast = head.next, slow = head; while(fast != null \u0026amp;\u0026amp; fast.next != null) { if(fast == slow) { return true; } fast = fast.next.next; slow = slow.next; } return false; } } 142. 环形链表 II 142. 环形链表 II\n双指针\n注意事项：快慢指针的初始状态一定都是要指向头节点\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public class Solution { public ListNode detectCycle(ListNode head) { if(head == null || head.next == null) { return null; } ListNode fast = head, slow = head; while(fast != null \u0026amp;\u0026amp; fast.next != null) { fast = fast.next.next; slow = slow.next; if(fast == slow) { fast = head; while(fast!= slow) { fast = fast.next; slow = slow.next; } return fast; } } return null; } } KC的含义就是指针顺着环走了整整K圈，并且当前位于环的入口节点；因此KC-t可以看作 (K-1)C+(C-t),也就是从当前节点再走C-t的距离就能到达入口节点，恰巧从链表头节点走a距离也是到达入口节点。因此如果两个指针此时同时分别从相遇点和头节点出发并以1的步长遍历，那么它们就一定会在环的入口点相遇。\n876. 链表的中间结点 876. 链表的中间结点\n慢指针一次走一步，快指针一次走两步，即便链表是偶数时依然可以找到对应的节点，可通过归纳法来分析出\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public ListNode middleNode(ListNode head) { if (head == null) { return head; } ListNode fast = head, slow = head; while(fast != null \u0026amp;\u0026amp; fast.next != null) { slow = slow.next; fast = fast.next.next; } return slow; } } 160. 相交链表 160. 相交链表\n核心思想：相交链表一定是Y型的，由于相交后的路程是相同的，差异是在于两条链表相交前的长度差异，那就让两个指针在遍历完各自的链表后，再交换遍历一次，这样他们一定会在交换遍历时候相遇。也就是A+B=B+A的思路\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { ListNode nodeA = headA, nodeB = headB; while (nodeA != nodeB) { if (nodeA != null) nodeA = nodeA.next; else nodeA = headB; if (nodeB != null) nodeB = nodeB.next; else nodeB = headA; } return nodeA; } } 83. 删除排序链表中的重复元素 83. 删除排序链表中的重复元素\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public ListNode deleteDuplicates(ListNode head) { if(head == null) { return head; } ListNode fast = head, slow = head; while(fast!= null) { if(fast.val != slow.val) { slow.next = fast; slow = slow.next; } fast = fast.next; } slow.next = null; return head; } } 202. 快乐数 202. 快乐数\n只要有死循环，通过快慢指针就一定会相遇。因此除了链表环，像这种会在某几个计算流程中陷入循环的场景，也可以通过双指针的方式进行判断。\n太强了，想到了之前做的那个 带环链表找入口的题。 也是一个slow，一个fast。fast速度是slow的两倍。 这样只要有循环，两者总会相遇。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public int getNext(int n) { int totalSum = 0; while (n \u0026gt; 0) { int d = n % 10; n = n / 10; totalSum += d * d; } return totalSum; } public boolean isHappy(int n) { int slowRunner = n; int fastRunner = getNext(n); while (fastRunner != 1 \u0026amp;\u0026amp; slowRunner != fastRunner) { slowRunner = getNext(slowRunner); fastRunner = getNext(getNext(fastRunner)); } return fastRunner == 1; } } 234. 回文链表 234. 回文链表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Solution { public boolean isPalindrome(ListNode head) { if (head == null || head.next == null) return true; ListNode endOfFirstHalf = endOfFirstHalf(head); ListNode endOfFirstHalfNode = reverseList(endOfFirstHalf.next); Boolean result = true; while (result \u0026amp;\u0026amp; endOfFirstHalfNode != null) { if (head.val != endOfFirstHalfNode.val) result =false; head = head.next; endOfFirstHalfNode = endOfFirstHalfNode.next; } return result; } public ListNode endOfFirstHalf(ListNode head) { if (head == null) return head; ListNode fast = head, slow = head; while (fast.next != null \u0026amp;\u0026amp; fast.next.next != null) { fast = fast.next.next; slow = slow.next; } return slow; } public ListNode reverseList(ListNode head) { if (head == null || head.next == null) return head; ListNode cur = head, pre = null; while (cur != null) { ListNode next = cur.next; cur.next = pre; pre = cur; cur = next; } return pre; } } 143. 重排链表 143. 重排链表\n需要搭配链表中间节点、反转链表功能来实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Solution { public void reorderList(ListNode head) { ListNode mid = middleNode(head); ListNode head2 = reverseList(mid); while (head2.next != null) { ListNode nxt = head.next; ListNode nxt2 = head2.next; head.next = head2; head2.next = nxt; head = nxt; head2 = nxt2; } } // 876. 链表的中间结点 private ListNode middleNode(ListNode head) { ListNode slow = head, fast = head; while (fast != null \u0026amp;\u0026amp; fast.next != null) { slow = slow.next; fast = fast.next.next; } return slow; } // 206. 反转链表 private ListNode reverseList(ListNode head) { ListNode pre = null, cur = head; while (cur != null) { ListNode nxt = cur.next; cur.next = pre; pre = cur; cur = nxt; } return pre; } } 287. 寻找重复数 287. 寻找重复数\n看作是\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Solution { public int findDuplicate(int[] nums) { int slow = 0, fast = 0; do { slow = nums[slow]; fast = nums[nums[fast]]; } while (slow!=fast); slow = 0; while (slow != fast) { slow = nums[slow]; fast = nums[fast]; } return slow; } } 26. 删除有序数组中的重复项 26. 删除有序数组中的重复项\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public int removeDuplicates(int[] nums) { if(nums.length == 0) return 0; int fast=0, slow=0, len = nums.length; while(fast\u0026lt;len) { if(nums[fast] != nums[slow]) { slow++; nums[slow]=nums[fast]; } fast++; } return slow+1; } } 27. 移除元素 27. 移除元素\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Solution { public int removeElement(int[] nums, int val) { if(nums.length == 0) return 0; int fast=0, slow=0, len=nums.length; while(fast\u0026lt;len) { if(nums[fast] != val){ nums[slow]=nums[fast]; slow++; } fast++; } return slow; } } 283. 移动零 283. 移动零\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public void moveZeroes(int[] nums) { if(nums.length == 0) return; int len=nums.length, fast=0, slow=0; while(fast\u0026lt;len) { if(nums[fast] != 0) { nums[slow] = nums[fast]; slow++; } fast++; } while(slow\u0026lt;len) { nums[slow++] = 0; } } } 滑动窗口 滑动窗口的题基本可以通过Hash缓存的方式来实现\n3. 无重复字符的最长子串 3. 无重复字符的最长子串\n外循环是左指针每次移动一步，内循环是右指针不断移动寻找不重复的子串，记住，这里内循环结束后，右指针不会复位。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Solution { public int lengthOfLongestSubstring(String s) { // 通过集合记录每个字符是否出现过 Set\u0026lt;Character\u0026gt; occ = new HashSet\u0026lt;Character\u0026gt;(); int n = s.length(); // 右指针，初始值为 -1，相当于我们在字符串的左边界的左侧，还没有开始移动 int rk = -1, ans = 0; for (int i = 0; i \u0026lt; n; ++i) { if (i != 0) { // 左指针向右移动一格，移除一个字符 occ.remove(s.charAt(i - 1)); } while (rk + 1 \u0026lt; n \u0026amp;\u0026amp; !occ.contains(s.charAt(rk + 1))) { // 不断地移动右指针 occ.add(s.charAt(rk + 1)); ++rk; } // 第 i 到 rk 个字符是一个极长的无重复字符子串 ans = Math.max(ans, rk - i + 1); } return ans; } } 438. 找到字符串中所有字母异位词 438. 找到字符串中所有字母异位词\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { public List\u0026lt;Integer\u0026gt; findAnagrams(String s, String p) { int n = s.length(), m = p.length(); List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (n \u0026lt; m) return res; int []pCnt = new int[26]; int []sCnt = new int[26]; for (int i = 0; i \u0026lt; m; i++) { pCnt[p.charAt(i)-\u0026#39;a\u0026#39;]++; } int left = 0; for (int right = 0; right \u0026lt; n; right++) { int curRight = s.charAt(right) - \u0026#39;a\u0026#39;; sCnt[curRight]++; while (sCnt[curRight] \u0026gt; pCnt[curRight]) { int curLeft = s.charAt(left) - \u0026#39;a\u0026#39;; sCnt[curLeft]--; left++; } if (right - left+1 == m) res.add(left); } return res; } } 76. 最小覆盖子串 76. 最小覆盖子串\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { public String minWindow(String s, String t) { int []map = new int [128]; for (char ch: t.toCharArray()) map[ch]++; int count = t.length(); int left = 0; int right = 0; int windowLength = Integer.MAX_VALUE; int strStart = 0; while (right \u0026lt; s.length()) { if (map[s.charAt(right++)]-- \u0026gt; 0) count--; while (count == 0) { if (right-left \u0026lt; windowLength) { windowLength = right - left; strStart = left; } if (map[s.charAt(left++)]++ == 0) count++; } } if (windowLength != Integer.MAX_VALUE) return s.substring(strStart, strStart + windowLength); return \u0026#34;\u0026#34;; } } 209. 长度最小的子数组 209. 长度最小的子数组\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public int minSubArrayLen(int target, int[] nums) { int len = nums.length, left = 0, right = 0, minLen = 0, curSum = 0; while (right \u0026lt; len) { curSum = curSum+nums[right]; while (curSum \u0026gt;= target) { if (right-left+1 \u0026lt; minLen || minLen == 0) { minLen = right-left+1; } curSum -= nums[left]; left++; } right++; } return minLen; } } 424. 替换后的最长重复字符 424. 替换后的最长重复字符\n没搞懂，先看后续的题，后面要回过头来搞懂\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 public class Solution { public int characterReplacement(String s, int k) { int len = s.length(); if (len \u0026lt; 2) { return len; } char[] charArray = s.toCharArray(); int left = 0; int right = 0; int res = 0; int maxCount = 0; int[] freq = new int[26]; // [left, right) 内最多替换 k 个字符可以得到只有一种字符的子串 while (right \u0026lt; len){ freq[charArray[right] - \u0026#39;A\u0026#39;]++; // 在这里维护 maxCount，因为每一次右边界读入一个字符，字符频数增加，才会使得 maxCount 增加 maxCount = Math.max(maxCount, freq[charArray[right] - \u0026#39;A\u0026#39;]); right++; if (right - left \u0026gt; maxCount + k){ // 说明此时 k 不够用 // 把其它不是最多出现的字符替换以后，都不能填满这个滑动的窗口，这个时候须要考虑左边界向右移动 // 移出滑动窗口的时候，频数数组须要相应地做减法 freq[charArray[left] - \u0026#39;A\u0026#39;]--; left++; } res = Math.max(res, right - left); } return res; } } 567. 字符串的排列 567. 字符串的排列\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Solution { public boolean checkInclusion(String s1, String s2) { int n = s1.length(), m = s2.length(); if (n \u0026gt; m) { return false; } int[] cnt1 = new int[26]; int[] cnt2 = new int[26]; for (int i = 0; i \u0026lt; n; ++i) { ++cnt1[s1.charAt(i) - \u0026#39;a\u0026#39;]; ++cnt2[s2.charAt(i) - \u0026#39;a\u0026#39;]; } if (Arrays.equals(cnt1, cnt2)) { return true; } for (int i = n; i \u0026lt; m; ++i) { ++cnt2[s2.charAt(i) - \u0026#39;a\u0026#39;]; --cnt2[s2.charAt(i - n) - \u0026#39;a\u0026#39;]; if (Arrays.equals(cnt1, cnt2)) { return true; } } return false; } } https://leetcode.cn/problems/permutation-in-string/solutions/599202/zi-fu-chuan-de-pai-lie-by-leetcode-solut-7k7u/ 30. 串联所有单词的子串 30. 串联所有单词的子串\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 public List\u0026lt;Integer\u0026gt; findSubstring(String s, String[] words) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); int wordNum = words.length; if (wordNum == 0) { return res; } int wordLen = words[0].length(); HashMap\u0026lt;String, Integer\u0026gt; allWords = new HashMap\u0026lt;String, Integer\u0026gt;(); for (String w : words) { int value = allWords.getOrDefault(w, 0); allWords.put(w, value + 1); } //将所有移动分成 wordLen 类情况 for (int j = 0; j \u0026lt; wordLen; j++) { HashMap\u0026lt;String, Integer\u0026gt; hasWords = new HashMap\u0026lt;String, Integer\u0026gt;(); int num = 0; //记录当前 HashMap2（这里的 hasWords 变量）中有多少个单词 //每次移动一个单词长度 for (int i = j; i \u0026lt; s.length() - wordNum * wordLen + 1; i = i + wordLen) { boolean hasRemoved = false; //防止情况三移除后，情况一继续移除 while (num \u0026lt; wordNum) { String word = s.substring(i + num * wordLen, i + (num + 1) * wordLen); if (allWords.containsKey(word)) { int value = hasWords.getOrDefault(word, 0); hasWords.put(word, value + 1); //出现情况三，遇到了符合的单词，但是次数超了 if (hasWords.get(word) \u0026gt; allWords.get(word)) { // hasWords.put(word, value); hasRemoved = true; int removeNum = 0; //一直移除单词，直到次数符合了 while (hasWords.get(word) \u0026gt; allWords.get(word)) { String firstWord = s.substring(i + removeNum * wordLen, i + (removeNum + 1) * wordLen); int v = hasWords.get(firstWord); hasWords.put(firstWord, v - 1); removeNum++; } num = num - removeNum + 1; //加 1 是因为我们把当前单词加入到了 HashMap 2 中 i = i + (removeNum - 1) * wordLen; //这里依旧是考虑到了最外层的 for 循环，看情况二的解释 break; } //出现情况二，遇到了不匹配的单词，直接将 i 移动到该单词的后边（但其实这里 //只是移动到了出现问题单词的地方，因为最外层有 for 循环， i 还会移动一个单词 //然后刚好就移动到了单词后边） } else { hasWords.clear(); i = i + num * wordLen; num = 0; break; } num++; } if (num == wordNum) { res.add(i); } //出现情况一，子串完全匹配，我们将上一个子串的第一个单词从 HashMap2 中移除 if (num \u0026gt; 0 \u0026amp;\u0026amp; !hasRemoved) { String firstWord = s.substring(i, i + wordLen); int v = hasWords.get(firstWord); hasWords.put(firstWord, v - 1); num = num - 1; } } } return res; } https://leetcode.cn/problems/substring-with-concatenation-of-all-words/solutions/9092/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-by-w-6/\n206. 反转链表 206. 反转链表: 滑动窗口思路，三个指针同时往后一步步遍历，每移动一次就变更这个三个指针所负责节点的指向，变更完后三个指针再继续往后移动\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public ListNode reverseList(ListNode head) { if(head == null || head.next == null) { return head; } ListNode cur = head, pre = null; while(cur != null) { ListNode next = cur.next; cur.next = pre; pre = cur; cur = next; } return pre; } } ","date":"2025-03-10T20:26:31+08:00","image":"https://sherlock-lin.github.io/p/%E5%8F%8C%E6%8C%87%E9%92%88%E4%B8%80%E6%96%87%E9%80%9A/the-creative-exchange-d2zvqp3fpro-unsplash_hu5876398126655421130.jpg","permalink":"https://sherlock-lin.github.io/p/%E5%8F%8C%E6%8C%87%E9%92%88%E4%B8%80%E6%96%87%E9%80%9A/","title":"双指针一文通"},{"content":"概述 回溯算法是一种暴力搜索算法，适合于组合、排列、子集、路径搜索等问题，其核心思想是尝试(选择一个可能的解)，回溯(如果发现不可行则撤回)，然后继续尝试其他可能性。\n回溯算法的代码框架 回溯算法一般包含：\n状态变量：记录搜索进度(如当前路径、可选集合等) 终止条件：满足条件时，将当前路径加入解集 递归搜索：枚举当前状态所有可能的选择 回溯(撤销选择)：恢复到上一步状态，继续尝试其他可能解 通用代码框架如下\n题型分析 回溯算法常见应用的场景有组合问题、排列问题、子集问题、棋盘问题、路径搜索这五种，这里例举出各个场景下经典的LeetCode题目。\n组合问题 组合问题涉及从给定集合中选取k个元素的所有可能组合，不考虑顺序。常用递归+剪枝来优化搜索。可以参考下面的方法论\n递归搜索，并通过start控制范围，防止重复选取 组合问题一般不能重复选择同一元素，避免无效搜索 77. 组合: 通过Deque队列从尾部加数据，递归算好后又从尾部进行移除 优质题解 39. 组合总和 40. 组合总和 II 216. 组合总和 III 377. 组合总和 Ⅳ 254 因子组合 17. 电话号码的字母组合 401. 二进制手表 1079. 活字印刷 1268. 搜索推荐系统 排列问题 排列问题要求输出所有可能的排列，需要记录已使用的元素，防止重复。\n方法论：\n递归过程中维护used数组，防止重复使用元素 当所有元素都被选取时，保存当前排列 46. 全排列 47. 全排列 II 60. 排列序列 784. 字母大小写全排列 996. 平方数组的数目 526. 优美的排列 267 回文排列II 子集问题 子集问题的目标是找到所有可能的子集(幂集)，可通过递归+回溯构造所有可能的解。\n方法论：\n递归时，每次都加入当前路径，从而生成所有可能的子集。 处理重复元素时，可先排序，然后跳过重复选项。 78. 子集 90. 子集 II 79. 单词搜索 491. 非递减子序列 1239. 串联字符串的最大长度 131. 分割回文串 332. 重新安排行程 473. 火柴拼正方形 967. 连续差相同的数字 1079. 活字印刷 棋盘问题 这类问题通常涉及在棋盘上放置元素(如皇后、数独)，需要合法性检查。\n方法论：\nN皇后问题中，每行只能放置一个皇后，递归按行搜索，并通过合法性检查进行剪枝。 数独求解中，使用回溯+递归搜索空位，并逐步填充数字。 51. N 皇后 52. N 皇后 II 37. 解数独 36. 有效的数独 130. 被围绕的区域 994. 腐烂的橘子 529. 扫雷游戏 489 扫地机器人 302 包含全部黑色像素的最小矩阵 1274 矩形内船只的数目 路径搜索 路径搜索问题通常涉及在网格或图中搜索特定路径，DFS是主要手段。\n方法论：\n采用DFS+回溯搜索所有可能路径 使用visited标记已访问的单元格，避免重复搜索 剪枝优化：若当前搜索方向无解，则及时返回 79. 单词搜索 212. 单词搜索 II 130. 被围绕的区域 200. 岛屿数量 1254. 统计封闭岛屿的数目 417. 太平洋大西洋水流问题 529. 扫雷游戏 980. 不同路径 III 1219. 黄金矿工 847. 访问所有节点的最短路径 组合问题 77. 组合 77. 组合\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combine(int n, int k) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res= new ArrayList\u0026lt;\u0026gt;(); if(k \u0026lt;= 0 || n \u0026lt; k) { return res; } Deque\u0026lt;Integer\u0026gt; path = new ArrayDeque\u0026lt;\u0026gt;(); dfs(n,k,1,path,res); return res; } private void dfs(int n, int k, int begin, Deque\u0026lt;Integer\u0026gt; path, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res) { //递归终止条件时：path的长度等于k if(path.size() == k) { res.add(new ArrayList\u0026lt;\u0026gt;(path)); return; } //遍历可能的搜索起点 for(int i=begin; i\u0026lt;=n; i++) { //向路径变量里添加一个数 path.addLast(i); // 下一轮搜索，设置的搜索起点要加 1，因为组合数理不允许出现重复的元素 dfs(n, k, i+1, path, res); // 重点理解这里：深度优先遍历有回头的过程，因此递归之前做了什么，递归之后需要做相同操作的逆向操作 path.removeLast(); } } } 39. 组合总和 39. 组合总和\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; result = new ArrayList(); List\u0026lt;Integer\u0026gt;temp = new ArrayList(); int traceSum = 0; public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum(int[] candidates, int target) { if (candidates.length == 0) return new ArrayList(); backtrace(candidates, 0, target); return result; } void backtrace(int []nums, int index, int target) { if(traceSum == target) { result.add(new ArrayList(temp)); } if(traceSum \u0026gt; target) return; for(int i=index; i\u0026lt;nums.length; i++) { temp.add(nums[i]); traceSum += nums[i]; backtrace(nums, i, target); temp.remove(temp.size()-1); traceSum -= nums[i]; } } } 40. 组合总和 II 40. 组合总和 II\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Solution { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; result = new ArrayList(); List\u0026lt;Integer\u0026gt; trace = new ArrayList(); int sum = 0; public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum2(int[] candidates, int target) { Arrays.sort(candidates); backtrace(candidates, 0, target); return result; } void backtrace(int []nums, int index, int target) { if(sum == target) { result.add(new ArrayList(trace)); return; } if(sum \u0026gt; target) return; for(int i=index; i\u0026lt;nums.length; i++) { if(i\u0026gt;index \u0026amp;\u0026amp; nums[i]==nums[i-1]) continue; sum += nums[i]; trace.add(nums[i]); backtrace(nums, i+1, target); sum -= nums[i]; trace.remove(trace.size()-1); } } } 排列问题 46. 全排列 46. 全排列\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Solution { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; ret = new LinkedList\u0026lt;\u0026gt;(); boolean []used; public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permute(int[] nums) { LinkedList\u0026lt;Integer\u0026gt; track = new LinkedList\u0026lt;\u0026gt;(); used = new boolean[nums.length]; backtrack(nums, track); return ret; } private void backtrack(int[] nums, LinkedList\u0026lt;Integer\u0026gt; track) { if(track.size() == nums.length) { ret.add(new LinkedList(track)); return; } for(int i=0; i\u0026lt;nums.length; i++) { if(used[i])continue; track.add(nums[i]); used[i] = true; backtrack(nums, track); track.removeLast(); used[i] = false; } } } 47. 全排列 II 47. 全排列 II\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; ret = new LinkedList\u0026lt;\u0026gt;(); boolean []used; public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permuteUnique(int[] nums) { LinkedList\u0026lt;Integer\u0026gt; track = new LinkedList\u0026lt;\u0026gt;(); used = new boolean[nums.length]; Arrays.sort(nums); backtrack(nums, track); return ret; } private void backtrack(int[] nums, LinkedList\u0026lt;Integer\u0026gt; track) { if(track.size() == nums.length) { ret.add(new LinkedList(track)); return; } for(int i=0; i\u0026lt;nums.length; i++) { if(used[i]) continue; if(i\u0026gt;0 \u0026amp;\u0026amp; nums[i] == nums[i-1] \u0026amp;\u0026amp; used[i-1]) continue; track.add(nums[i]); used[i] = true; backtrack(nums, track); track.removeLast(); used[i] = false; } } } ","date":"2025-03-10T17:11:56+08:00","image":"https://sherlock-lin.github.io/p/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%E4%B8%80%E6%96%87%E9%80%9A/gannet-9003524_1280_hu7120391022597348638.jpg","permalink":"https://sherlock-lin.github.io/p/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%E4%B8%80%E6%96%87%E9%80%9A/","title":"回溯算法一文通"},{"content":"概述 从小我们就被教育，学习做事要踏踏实实的，要有三十年磨一剑的匠心精神，因此在面对一些重大项目或者学习一些毕竟经典的知识时，可能会下意识的想去放慢步伐，担心学太快无法消化，但最后几个月过去了项目、学习还没有什么进展，最后事情、学习都有些不上不下的，最后成本也投入了不少但是收益几乎为零，因此在这里提出\u0026quot;快速闭环原则\u0026quot;来解决这个棘手的问题。\n快速闭环原则 快速闭环原则的本质就是，通过合理的对庞大的任务进行拆分，拆分成多个最小可执行任务，通过不断快速执行完这些小任务来达到推动大任务进度的效果，这个过程有以下几个优点\n灵活性：完成小任务的过程中，你会接触到更多的信息，这些信息可以反过来验证大目标的合理性并作出相对应的调整，避免一次性憋大招，最终做出一个跟预期不匹配的结果，从而浪费时间跟精力 动力：如果长期盯着一个目标去做，那么由于长时间得不到反馈，身心都容易疲惫；而通过将大任务拆解成多个小任务，通过二十分钟迅速完成，来得到即时的正面反馈，也就是多巴胺，从而能让身心都得到小阶段的满足，从而有更大的动力去迎接下一个小任务 番茄闹钟：通过搭配番茄闹钟，可以将每天的时间拆分成多个25分钟的番茄闹钟，番茄闹钟之间都提供五分钟的休息时间，这跟人注意力集中时间刚好对应得上。每个人注意力集中时间差不多就是25分钟，那么通过每个番茄闹钟去完成一个小任务，是比较合理的一个方式。如果有些小任务无法在25分钟内完成，那么说明这个小任务拆解得还不够细粒度，应该再进一步拆分，当然如果有少个别特殊情况，可以通过多个番茄闹钟完成，但是一定要注意，即便是通过多个闹钟完成，每个闹钟结束时，一定要有阶段性的成果 结果导向：每个小任务一定要是奔着结果去的，这个结果一定要是实质上的东西，代码、博客、笔记、一些验证性的数据都可以，但一定要有实质的东西来支撑着 总结 本篇文章也是通过一个番茄闹钟来完成的，虽然粗糙但是核心思想已经表达出来了，有句话很适合整体的思想，“先快速做出一堆屎，然后再去优化它”，本质上也是一样的，至少这个过程中是不断围绕着最终产品去做的，无论哪个阶段这个产品都有一个实质上的形态，让自己以及别人都能够看得见，从而有更多的信心以及动力去将它完成。\n","date":"2025-03-09T13:38:13+08:00","image":"https://sherlock-lin.github.io/p/%E5%BF%AB%E9%80%9F%E9%97%AD%E7%8E%AF%E5%8E%9F%E5%88%99/lizard-8007238_1280_hu7957813414589977256.jpg","permalink":"https://sherlock-lin.github.io/p/%E5%BF%AB%E9%80%9F%E9%97%AD%E7%8E%AF%E5%8E%9F%E5%88%99/","title":"快速闭环原则"},{"content":"概述 无论是批处理、流计算，还是数据分析、机器学习，只要是在Spark作业中，我们总能见到OOM（Out Of Memory，内存溢出）的身影。一旦出现OOM，作业就会中断，应用的业务功能也都无法执行。因此，及时处理OOM问题是我们日常开发中一项非常重要的工作。\n但是，Spark报出的OOM问题可以说是五花八门，常常让人找不到头绪。比如，我们经常遇到，数据集按照尺寸估算本该可以完全放进内存，但Spark依然会报OOM异常。这个时候，不少同学都会参考网上的做法，把spark.executor.memory不断地调大、调大、再调大，直到内心崩溃也无济于事，最后只能放弃。\n那么，当我们拿到OOM这个“烫手的山芋”的时候该怎么办呢？我们最先应该弄清楚的是“到底哪里出现了OOM”。只有准确定位出现问题的具体区域，我们的调优才能有的放矢。具体来说，这个“哪里”，我们至少要分3个方面去看。\n发生OOM的LOC（Line Of Code），也就是代码位置在哪？ OOM发生在Driver端，还是在Executor端？ 如果是发生在Executor端，OOM到底发生在哪一片内存区域？ 定位出错代码的位置非常重要但也非常简单，我们只要利用Stack Trace就能很快找到抛出问题的LOC。因此，更关键的是，我们要明确出问题的到底是Driver端还是Executor端，以及是哪片内存区域。Driver和Executor产生OOM的病灶不同，我们自然需要区别对待。\n所以今天这一讲，我们就先来说说Driver端的OOM问题和应对方法。由于内存在Executor端被划分成了不同区域，因此，对于Executor端怪相百出的OOM，我们还要结合案例来分类讨论。最后，我会带你整理出一套应对OOM的“武功秘籍”，让你在面对OOM的时候，能够见招拆招、有的放矢！\nDriver端的OOM 我们先来说说Driver端的OOM。Driver的主要职责是任务调度，同时参与非常少量的任务计算，因此Driver的内存配置一般都偏低，也没有更加细分的内存区域。\n因为Driver的内存就是囫囵的那么一块，所以Driver端的OOM问题自然不是调度系统的毛病，只可能来自它涉及的计算任务，主要有两类：\n创建小规模的分布式数据集：使用parallelize、createDataFrame等API创建数据集 收集计算结果：通过take、show、collect等算子把结果收集到Driver端 因此Driver端的OOM逃不出2类病灶：\n创建的数据集超过内存上限 收集的结果集超过内存上限 第一类病灶不言自明，咱们不细说了。看到第二类病灶，想必你第一时间想到的就是万恶的collect。确实，说到OOM就不得不提collect。collect算子会从Executors把全量数据拉回到Driver端，因此，如果结果集尺寸超过Driver内存上限，它自然会报OOM。\n由开发者直接调用collect算子而触发的OOM问题其实很好定位，比较难定位的是间接调用collect而导致的OOM。那么，间接调用collect是指什么呢？还记得广播变量的工作原理吗？\n广播变量在创建的过程中，需要先把分布在所有Executors的数据分片拉取到Driver端，然后在Driver端构建广播变量，最后Driver端把封装好的广播变量再分发给各个Executors。第一步的数据拉取其实就是用collect实现的。如果Executors中数据分片的总大小超过Driver端内存上限也会报OOM。在日常的调优工作中，你看到的表象和症状可能是：\n1 java.lang.OutOfMemoryError: Not enough memory to build and broadcast 但实际的病理却是Driver端内存受限，没有办法容纳拉取回的结果集。找到了病因，再去应对Driver端的OOM就很简单了。我们只要对结果集尺寸做适当的预估，然后再相应地增加Driver侧的内存配置就好了。调节Driver端侧内存大小我们要用到spark.driver.memory配置项，预估数据集尺寸可以用“先Cache，再查看执行计划”的方式，示例代码如下。\n1 2 3 4 5 6 7 8 9 val df: DataFrame = _ df.cache.count val plan = df.queryExecution.logical val estimated: BigInt = spark .sessionState .executePlan(plan) .optimizedPlan .stats .sizeInBytes Executor端的OOM 我们再来说说Executor端的OOM。我们知道，执行内存分为4个区域：Reserved Memory、User Memory、Storage Memory和Execution Memory。这4个区域中都有哪些区域会报OOM异常呢？哪些区域压根就不存在OOM的可能呢？\n在Executors中，与任务执行有关的内存区域才存在OOM的隐患。其中，Reserved Memory大小固定为300MB，因为它是硬编码到源码中的，所以不受用户控制。而对于Storage Memory来说，即便数据集不能完全缓存到MemoryStore，Spark也不会抛OOM异常，额外的数据要么落盘（MEMORY_AND_DISK）、要么直接放弃（MEMORY_ONLY）。\n因此，当Executors出现OOM的问题，我们可以先把Reserved Memory和Storage Memory排除，然后锁定Execution Memory和User Memory去找毛病。\nUser Memory的OOM 在内存管理那一讲，我们说过User Memory用于存储用户自定义的数据结构，如数组、列表、字典等。因此，如果这些数据结构的总大小超出了User Memory内存区域的上限，你可能就会看到下表示例中的报错。\n1 2 3 java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf java.lang.OutOfMemoryError: Java heap space at java.lang.reflect.Array.newInstance 如果你的数据结构是用于分布式数据转换，在计算User Memory内存消耗时，你就需要考虑Executor的线程池大小。还记得下面的这个例子吗？\n1 2 3 4 val dict = List(“spark”, “tune”) val words = spark.sparkContext.textFile(“~/words.csv”) val keywords = words.filter(word =\u0026gt; dict.contains(word)) keywords.map((_, 1)).reduceByKey(_ + _).collect 自定义的列表dict会随着Task分发到所有Executors，因此多个Task中的dict会对User Memory产生重复消耗。如果把dict尺寸记为#size，Executor线程池大小记为#threads，那么dict对User Memory的总消耗就是：#size * #threads。一旦总消耗超出User Memory内存上限，自然就会产生OOM问题。\n那么，解决User Memory 端 OOM的思路和Driver端的并无二致，也是先对数据结构的消耗进行预估，然后相应地扩大User Memory的内存配置。不过，相比Driver，User Memory内存上限的影响因素更多，总大小由spark.executor.memory * （ 1 - spark.memory.fraction）计算得到。\nExecution Memory的OOM 要说OOM的高发区，非Execution Memory莫属。久行夜路必撞鬼，在分布式任务执行的过程中，Execution Memory首当其冲，因此出错的概率相比其他内存区域更高。关于Execution Memory的OOM，我发现不少同学都存在这么一个误区：只要数据量比执行内存小就不会发生OOM，相反就会有一定的几率触发OOM问题。\n实际上，数据量并不是决定OOM与否的关键因素，数据分布与Execution Memory的运行时规划是否匹配才是。这么说可能比较抽象，你还记得黄小乙的如意算盘吗？为了提高老乡们种地的热情和积极性，他制定了个转让协议，所有老乡申请的土地面积介于1/N/2和1/N之间。因此，如果有的老乡贪多求快，买的种子远远超过1/N上限能够容纳的数量，这位老乡多买的那部分种子都会被白白浪费掉。\n同样的，我们可以把Execution Memory看作是土地，把分布式数据集看作是种子，一旦**分布式任务的内存请求超出1/N这个上限，**Execution Memory就会出现OOM问题。而且，相比其他场景下的OOM问题，Execution Memory的OOM要复杂得多，它不仅仅与内存空间大小、数据分布有关，还与Executor线程池和运行时任务调度有关。抓住了引起OOM问题最核心的原因，对于Execution Memory OOM的诸多表象，我们就能从容应对了。下面，我们就来看两个平时开发中常见的实例：数据倾斜和数据膨胀。为了方便说明，在这两个实例中，计算节点的硬件配置是一样的，都是2个CPU core，每个core有两个线程，内存大小为1GB，并且spark.executor.cores设置为3，spark.executor.memory设置为900MB。\n根据配置项那一讲我们说过的不同内存区域的计算公式，在默认配置下，我们不难算出Execution Memory和Storage Memory内存空间都是180MB。而且，因为我们的例子里没有RDD缓存，所以Execution Memory内存空间上限是360MB。\n实例1：数据倾斜 我们先来看第一个数据倾斜的例子。节点在Reduce阶段拉取数据分片，3个Reduce Task对应的数据分片大小分别是100MB和300MB。显然，第三个数据分片存在轻微的数据倾斜。由于Executor线程池大小为3，因此每个Reduce Task最多可获得360MB * 1 / 3 = 120MB的内存空间。Task1、Task2获取到的内存空间足以容纳分片1、分片2，因此可以顺利完成任务。\nTask3的数据分片大小远超内存上限，即便Spark在Reduce阶段支持Spill和外排，120MB的内存空间也无法满足300MB数据最基本的计算需要，如PairBuffer和AppendOnlyMap等数据结构的内存消耗，以及数据排序的临时内存消耗等等。\n这个例子的表象是数据倾斜导致OOM，但实质上是Task3的内存请求超出1/N上限。因此，针对以这个案例为代表的数据倾斜问题，我们至少有2种调优思路：\n消除数据倾斜，让所有的数据分片尺寸都不大于100MB 调整Executor线程池、内存、并行度等相关配置，提高1/N上限到300MB 每一种思路都可以衍生出许多不同的方法，就拿第2种思路来说，要满足1/N的上限，最简单地，我们可以把spark.executor.cores设置成1，也就是Executor线程池只有一个线程“并行”工作。这个时候，每个任务的内存上限都变成了360MB，容纳300MB的数据分片绰绰有余。\n当然，线程池大小设置为1是不可取的，刚刚只是为了说明调优的灵活性。延续第二个思路，你需要去平衡多个方面的配置项，在充分利用CPU的前提下解决OOM的问题。比如：\n维持并发度、并行度不变，增大执行内存设置，提高1/N上限到300MB 维持并发度、执行内存不变，使用相关配置项来提升并行度将数据打散，让所有的数据分片尺寸都缩小到100MB以内 关于线程池、内存和并行度之间的平衡与设置，我在CPU视角那一讲做过详细的介绍，你可以去回顾一下。至于怎么消除数据倾斜，你可以好好想想，再把你的思路分享出来。\n实例2：数据膨胀 我们再来看第二个数据膨胀的例子。节点在Map阶段拉取HDFS数据分片，3个Map Task对应的数据分片大小都是100MB。按照之前的计算，每个Map Task最多可获得120MB的执行内存，不应该出现OOM问题才对。\n尴尬的地方在于，磁盘中的数据进了JVM之后会膨胀。在我们的例子中，数据分片加载到JVM Heap之后翻了3倍，原本100MB的数据变成了300MB，因此，OOM就成了一件必然会发生的事情。\n在这个案例中，表象是数据膨胀导致OOM，但本质上还是Task2和Task3的内存请求超出1/N上限。因此，针对以这个案例为代表的数据膨胀问题，我们还是有至少2种调优思路：\n把数据打散，提高数据分片数量、降低数据粒度，让膨胀之后的数据量降到100MB左右 加大内存配置，结合Executor线程池调整，提高1/N上限到300MB 总结 想要高效解决五花八门的OOM问题，最重要的就是准确定位问题出现的区域，这样我们的调优才能有的放矢，我建议你按照两步进行。\n首先，定位OOM发生的代码位置，你通过Stack Trace就能很快得到答案。\n其次，定位OOM是发生在Driver端还是在Executor端。如果是发生在Executor端，再定位具体发生的区域。\n发生在Driver端的OOM可以归结为两类：\n创建的数据集超过内存上限 收集的结果集超过内存上限 应对Driver端OOM的常规方法，是先适当预估结果集尺寸，然后再相应增加Driver侧的内存配置。\n发生在Executors侧的OOM只和User Memory和Execution Memory区域有关，因为它们都和任务执行有关。其中，User Memory区域OOM的产生的原因和解决办法与Driver别无二致，你可以直接参考。\n而Execution Memory区域OOM的产生的原因是数据分布与Execution Memory的运行时规划不匹配，也就是分布式任务的内存请求超出了1/N上限。解决Execution Memory区域OOM问题的思路总的来说可以分为3类：\n消除数据倾斜，让所有的数据分片尺寸都小于1/N上限 把数据打散，提高数据分片数量、降低数据粒度，让膨胀之后的数据量降到1/N以下 加大内存配置，结合Executor线程池调整，提高1/N上限 ","date":"2025-03-09T13:08:43+08:00","image":"https://sherlock-lin.github.io/p/oom%E9%83%BD%E6%98%AF%E8%B0%81%E7%9A%84%E9%94%85/winter-8433267_1280_hu11580229835300275763.jpg","permalink":"https://sherlock-lin.github.io/p/oom%E9%83%BD%E6%98%AF%E8%B0%81%E7%9A%84%E9%94%85/","title":"OOM都是谁的锅"},{"content":"概述 在数据分析领域，数据关联（Joins）是Shuffle操作的高发区，二者如影随形从。可以说，有Joins的地方，就有Shuffle。但面对Shuffle，开发者应当“能省则省、能拖则拖”。拖指的就是，把应用中会引入Shuffle的操作尽可能地往后面的计算步骤去拖。那具体该怎么省呢？在数据关联场景中，广播变量就可以轻而易举地省去Shuffle。所以今天这一讲，我们就先说一说广播变量的含义和作用，再说一说它是如何帮助开发者省去Shuffle操作的。\n广播常量数据 接下来，咱们借助一个小例子，来讲一讲广播变量的含义与作用。这个例子和Word Count有关，它可以说是分布式编程里的Hello world了，Word Count就是用来统计文件中全部单词的，你肯定已经非常熟悉了，所以，我们例子中的需求增加了一点难度，我们要对指定列表中给定的单词计数。\n1 2 3 4 val dict = List(“spark”, “tune”) val words = spark.sparkContext.textFile(“~/words.csv”) val keywords = words.filter(word =\u0026gt; dict.contains(word)) keywords.map((_, 1)).reduceByKey(_ + _).collect 按照这个需求，同学小A实现了如上的代码，一共有4行，我们逐一来看。第1行在Driver端给定待查单词列表dict；第2行以textFile API读取分布式文件，内容包含一列，存储的是常见的单词；第3行用列表dict中的单词过滤分布式文件内容，只保留dict中给定的单词；第4行调用reduceByKey对单词进行累加计数。\n学习过调度系统之后，我们知道，第一行代码定义的dict列表连带后面的3行代码会一同打包到Task里面去。这个时候，Task就像是一架架小飞机，携带着这些“行李”，飞往集群中不同的Executors。对于这些“行李”来说，代码的“负重”较轻，可以忽略不计，而数据的负重占了大头，成了最主要的负担。\n你可能会说：“也还好吧，dict列表又不大，也没什么要紧的”。但是，如果我们假设这个例子中的并行度是10000，那么，Driver端需要通过网络分发总共10000份dict拷贝。这个时候，集群内所有的Executors需要消耗大量内存来存储这10000份的拷贝，对宝贵的网络和内存资源来说，这已经是一笔不小的浪费了。更何况，如果换做一个更大的数据结构，Task分发所引入的网络与内存开销会更可怕。换句话说，统计计数的业务逻辑还没有开始执行，Spark就已经消耗了大量的网络和存储资源，这简直不可理喻。因此，我们需要对示例中的代码进行优化，从而跳出这样的窘境。\n但是，在着手优化之前，我们不妨先来想一想，现有的问题是什么，我们要达到的目的是什么。结合刚刚的分析，我们不难发现，Word Count的核心痛点在于，数据结构的分发和存储受制于并行，并且是以Task为粒度的，因此往往频次过高。痛点明确了，调优的目的也就清晰了，我们需要降低数据结构分发的频次。要达到这个目的，我们首先想到的就是降低并行度。不过，牵一发而动全身，并行度一旦调整，其他与CPU、内存有关的配置项都要跟着适配，这难免把调优变复杂了。实际上，要降低数据结构的分发频次，我们还可以考虑广播变量。\n**广播变量是一种分发机制，它一次性封装目标数据结构，以Executors为粒度去做数据分发。**换句话说，在广播变量的工作机制下，数据分发的频次等同于集群中的Executors个数。通常来说，集群中的Executors数量都远远小于Task数量，相差两到三个数量级是常有的事。那么，对于第一版的Word Count实现，如果我们使用广播变量的话，会有哪些变化呢？\n代码的改动很简单，主要有两个改动：第一个改动是用broadcast封装dict列表，第二个改动是在访问dict列表的地方改用broadcast.value替代。\n1 2 3 4 5 val dict = List(“spark”, “tune”) val bc = spark.sparkContext.broadcast(dict) val words = spark.sparkContext.textFile(“~/words.csv”) val keywords = words.filter(word =\u0026gt; bc.value.contains(word)) keywords.map((_, 1)).reduceByKey(_ + _).collect 你可能会说：“这个改动看上去也没什么呀！”别着急，我们先来分析一下，改动之后的代码在运行时都有哪些变化。\n在广播变量的运行机制下，封装成广播变量的数据，由Driver端以Executors为粒度分发，每一个Executors接收到广播变量之后，将其交给BlockManager管理。由于广播变量携带的数据已经通过专门的途径存储到BlockManager中，因此分发到Executors的Task不需要再携带同样的数据。\n这个时候，你可以把广播变量想象成一架架专用货机，专门为Task这些小飞机运送“大件行李”。Driver与每一个Executors之间都开通一条这样的专用货机航线，统一运载负重较大的“数据行李”。有了专用货机来帮忙，Task小飞机只需要携带那些负重较轻的代码就好了。等这些Task小飞机在Executors着陆，它们就可以到Executors的公用仓库BlockManager里去提取它们的“大件行李”。\n总之，在广播变量的机制下，dict列表数据需要分发和存储的次数锐减。我们假设集群中有20个Executors，不过任务并行度还是10000，那么，Driver需要通过网络分发的dict列表拷贝就会由原来的10000份减少到20份。同理，集群范围内所有Executors需要存储的dict拷贝，也由原来的10000份，减少至20份。这个时候，引入广播变量后的开销只是原来Task分发的1/500！\n广播分布式数据集 那在刚刚的示例代码中，广播变量封装的是Driver端创建的普通变量：字符串列表。除此之外，广播变量也可以封装分布式数据集。我们来看这样一个例子。在电子商务领域中，开发者往往用事实表来存储交易类数据，用维度表来存储像物品、用户这样的描述性数据。事实表的特点是规模庞大，数据体量随着业务的发展不断地快速增长。维度表的规模要比事实表小很多，数据体量的变化也相对稳定。\n假设用户维度数据以Parquet文件格式存储在HDFS文件系统中，业务部门需要我们读取用户数据并创建广播变量以备后用，我们该怎么做呢？很简单，几行代码就可以搞定！\n1 2 3 val userFile: String = “hdfs://ip:port/rootDir/userData” val df: DataFrame = spark.read.parquet(userFile) val bc_df: Broadcast[DataFrame] = spark.sparkContext.broadcast(df) 首先，我们用Parquet API读取HDFS分布式数据文件生成DataFrame，然后用broadcast封装DataFrame。从代码上来看，这种实现方式和封装普通变量没有太大差别，它们都调用了broadcast API，只是传入的参数不同。但如果不从开发的视角来看，转而去观察运行时广播变量的创建过程的话，我们就会发现，分布式数据集与普通变量之间的差异非常显著。从普通变量创建广播变量，由于数据源就在Driver端，因此，只需要Driver把数据分发到各个Executors，再让Executors把数据缓存到BlockManager就好了。\n但是，从分布式数据集创建广播变量就要复杂多了，具体的过程如下图所示。\n与普通变量相比，分布式数据集的数据源不在Driver端，而是来自所有的Executors。Executors中的每个分布式任务负责生产全量数据集的一部分，也就是图中不同的数据分区。因此，步骤1就是**Driver从所有的Executors拉取这些数据分区，然后在本地构建全量数据。**步骤2与从普通变量创建广播变量的过程类似。 Driver把汇总好的全量数据分发给各个Executors，Executors将接收到的全量数据缓存到存储系统的BlockManager中。不难发现，相比从普通变量创建广播变量，从分布式数据集创建广播变量的网络开销更大。原因主要有二：一是，前者比后者多了一步网络通信；二是，前者的数据体量通常比后者大很多。\n如何用广播变量克制Shuffle 你可能会问：“Driver从Executors拉取DataFrame的数据分片，揉成一份全量数据，然后再广播出去，抛开网络开销不说，来来回回得费这么大劲，图啥呢？”这是一个好问题，因为以广播变量的形式缓存分布式数据集，正是克制Shuffle杀手锏。\nShuffle Joins 为什么这么说呢？我还是拿电子商务场景举例。有了用户的数据之后，为了分析不同用户的购物习惯，业务部门要求我们对交易表和用户表进行数据关联。这样的数据关联需求在数据分析领域还是相当普遍的。\n1 2 3 val transactionsDF: DataFrame = _ val userDF: DataFrame = _ transactionsDF.join(userDF, Seq(“userID”), “inner”) 因为需求非常明确，同学小A立即调用Parquet数据源API，读取分布式文件，创建交易表和用户表的DataFrame，然后调用DataFrame的Join方法，以userID作为Join keys，用内关联（Inner Join）的方式完成了两表的数据关联。\n在分布式环境中，交易表和用户表想要以userID为Join keys进行关联，就必须要确保一个前提：交易记录和与之对应的用户信息在同一个Executors内。也就是说，如果用户黄小乙的购物信息都存储在Executor 0，而个人属性信息缓存在Executor 2，那么，在分布式环境中，这两种信息必须要凑到同一个进程里才能实现关联计算。\n在不进行任何调优的情况下，Spark默认采用Shuffle Join的方式来做到这一点。Shuffle Join的过程主要有两步。\n第一步就是对参与关联的左右表分别进行Shuffle，Shuffle的分区规则是先对Join keys计算哈希值，再把哈希值对分区数取模。由于左右表的分区数是一致的，因此Shuffle过后，一定能够保证userID相同的交易记录和用户数据坐落在同一个Executors内。\nShuffle完成之后，第二步就是在同一个Executors内，Reduce task就可以对userID一致的记录进行关联操作。但是，由于交易表是事实表，数据体量异常庞大，对TB级别的数据进行Shuffle，想想都觉得可怕！因此，上面对两个DataFrame直接关联的代码，还有很大的调优空间。我们该怎么做呢？话句话说，对于分布式环境中的数据关联来说，要想确保交易记录和与之对应的用户信息在同一个Executors中，我们有没有其他办法呢？\n克制Shuffle的方式 还记得之前业务部门要求我们把用户表封装为广播变量，以备后用吗？现在它终于派上用场了！\n1 2 3 4 5 6 7 import org.apache.spark.sql.functions.broadcast val transactionsDF: DataFrame = _ val userDF: DataFrame = _ val bcUserDF = broadcast(userDF) transactionsDF.join(bcUserDF, Seq(“userID”), “inner”) Driver从所有Executors收集userDF所属的所有数据分片，在本地汇总用户数据，然后给每一个Executors都发送一份全量数据的拷贝。既然每个Executors都有userDF的全量数据，这个时候，交易表的数据分区待在原地、保持不动，就可以轻松地关联到一致的用户数据。如此一来，我们不需要对数据体量巨大的交易表进行Shuffle，同样可以在分布式环境中，完成两张表的数据关联。\n利用广播变量，我们成功地避免了海量数据在集群内的存储、分发，节省了原本由Shuffle引入的磁盘和网络开销，大幅提升运行时执行性能。当然，采用广播变量优化也是有成本的，毕竟广播变量的创建和分发，也是会带来网络开销的。但是，相比大表的全网分发，小表的网络开销几乎可以忽略不计。这种小投入、大产出，用极小的成本去博取高额的性能收益，真可以说是“四两拨千斤”！\n总结 在数据关联场景中，广播变量是克制Shuffle的杀手锏。掌握了它，我们就能以极小的成本，获得高额的性能收益。关键是我们要掌握两种创建广播变量的方式。\n第一种，从普通变量创建广播变量。在广播变量的运行机制下，普通变量存储的数据封装成广播变量，由Driver端以Executors为粒度进行分发，每一个Executors接收到广播变量之后，将其交由BlockManager管理。\n第二种，从分布式数据集创建广播变量，这就要比第一种方式复杂一些了。第一步，Driver需要从所有的Executors拉取数据分片，然后在本地构建全量数据；第二步，Driver把汇总好的全量数据分发给各个Executors，Executors再将接收到的全量数据缓存到存储系统的BlockManager中。\n结合这两种方式，我们在做数据关联的时候，把Shuffle Joins转换为Broadcast Joins，就可以用小表广播来代替大表的全网分发，真正做到克制Shuffle。\n","date":"2025-03-07T13:38:25+08:00","image":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3spark%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu6307248181568134095.jpg","permalink":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3spark%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/","title":"详解Spark广播变量"},{"content":"概述 RDD (Spark1.0) —\u0026gt; Dataframe(Spark1.3) —\u0026gt; Dataset(Spark1.6)，如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。\n这三者之间的关系如下图\n是什么 RDD RDD叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表了一个弹性的、不可变的、可分区、里面的元素可进行计算的集合。\nRDD是一个懒执行的不可变的可以支持Lambda表达式的并行数据集合。 RDD的最大好处就是简单，API的人性化程度很高。 RDD的劣势是性能限制，它是一个JVM驻内存对象，这也就决定了存在GC的限制和数据增加时Java序列化成本的升高。 DataFrame 与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待，DataFrame也是懒执行的。\n上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。\nDataFrame性能上比RDD要高，主要有两方面原因：\n定制化内存管理：数据以二进制的方式存在于非堆内存，节省了大量空间之外，还摆脱了GC的限制\n优化的执行计划：查询计划通过Spark catalyst optimiser进行优化，为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。得到的优化执行计划在转换成物 理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。对于普通开发者而言，查询优化器的意义在于，即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。Dataframe的劣势在于在编译期缺少类型安全检查，导致运行时出错.\nDataFrame 带有schema元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型，便于Spark SQL的操作 支持嵌套数据类型（struct、array和Map），从易用性来说，DataFrame提供的是一套高层的关系操作，比函数式的RDD API更加友好 因为优化的查询执行计划，导致DataFrame执行效率优于RDD RDD 无法得到所存数据元素的具体结构，SparkCore只能在Stage层面进行简单、通用的流水线优化 操作门槛高 DataSet 是Dataframe API的一个扩展，是Spark最新的数据抽象 用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。 Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。 样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。 Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为 Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。 DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person]. DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。\n三者共性和差异 共性 1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利 2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过. 3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出 4、三者都有partition的概念 5、三者有许多共同的函数，如filter，排序等 6、在对DataFrame和Dataset进行操作许多操作都需要这个包进行支持 import spark.implicits._ 7、DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型\n差异 1、RDD一般和spark mlib同时使用 2、RDD不支持sparksql操作 3、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值 4、DataFrame与Dataset一般不与spark ml同时使用 5、DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作 6、DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然\n三者之间转换方式 ","date":"2025-02-25T14:40:07+08:00","image":"https://sherlock-lin.github.io/p/rdd%E5%92%8Cdataframe%E5%92%8Cdataset%E4%B8%89%E8%80%85%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB/trees-8136806_1280_hu4473522823771471937.png","permalink":"https://sherlock-lin.github.io/p/rdd%E5%92%8Cdataframe%E5%92%8Cdataset%E4%B8%89%E8%80%85%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"RDD和DataFrame和DataSet三者间的区别"},{"content":"概述 在数据分析领域，数据关联可以说是最常见的计算场景了。因为使用的频率很高，所以Spark为我们准备了非常丰富的关联形式，包括Inner Join、Left Join、Right Join、Anti Join、Semi Join等等。\n搞懂不同关联形式的区别与作用，可以让我们快速地实现业务逻辑。不过，这只是基础，要想提高数据关联场景下Spark应用的执行性能，更为关键的是我们要能够深入理解Join的实现原理。\n所以今天这一讲，我们先来说说，单机环境中Join都有哪几种实现方式，它们的优劣势分别是什么。理解了这些实现方式，我们再结合它们一起探讨，分布式计算环境中Spark都支持哪些Join策略。对于不同的Join策略，Spark是怎么做取舍的。\nJoin的实现方式详解 到目前为止，数据关联总共有3种Join实现方式。按照出现的时间顺序，分别是嵌套循环连接（NLJ，Nested Loop Join ）、排序归并连接（SMJ，Shuffle Sort Merge Join）和哈希连接（HJ，Hash Join）。接下来，我们就借助一个数据关联的场景，来分别说一说这3种Join实现方式的工作原理。\n假设，现在有事实表orders和维度表users。其中，users表存储用户属性信息，orders记录着用户的每一笔交易。两张表的Schema如下：\n1 2 3 4 5 6 7 8 9 10 // 订单表orders关键字段 userId, Int itemId, Int price, Float quantity, Int // 用户表users关键字段 id, Int name, String type, String //枚举值，分为头部用户和长尾用户 我们的任务是要基于这两张表做内关联（Inner Join），同时把用户名、单价、交易额等字段投影出来。具体的SQL查询语句如下表：\n1 2 3 //SQL查询语句 select orders.quantity, orders.price, orders.userId, users.id, users.name from orders inner join users on orders.userId = users.id 那么，对于这样一个关联查询，在3种不同的Join实现方式下，它是如何完成计算的呢？\nNLJ的工作原理 对于参与关联的两张数据表，我们通常会根据它们扮演的角色来做区分。其中，体量较大、主动扫描数据的表，我们把它称作外表或是驱动表；体量较小、被动参与数据扫描的表，我们管它叫做内表或是基表。那么，NLJ是如何关联这两张数据表的呢？\n**NLJ是采用“嵌套循环”的方式来实现关联的。**也就是说，NLJ会使用内、外两个嵌套的for循环依次扫描外表和内表中的数据记录，判断关联条件是否满足，比如例子中的orders.userId = users.id，如果满足就把两边的记录拼接在一起，然后对外输出。\n在这个过程中，外层的for循环负责遍历外表中的每一条数据，如图中的步骤1所示。而对于外表中的每一条数据记录，内层的for循环会逐条扫描内表的所有记录，依次判断记录的Join Key是否满足关联条件，如步骤2所示。假设，外表有M行数据，内表有N行数据，那么NLJ算法的计算复杂度是O(M * N)。不得不说，尽管NLJ实现方式简单而又直接，但它的执行效率实在让人不敢恭维。\nSMJ的工作原理 正是因为NLJ极低的执行效率，所以在它推出之后没多久之后，就有人用排序、归并的算法代替NLJ实现了数据关联，这种算法就是SMJ。**SMJ的思路是先排序、再归并。**具体来说，就是参与Join的两张表先分别按照Join Key做升序排序。然后，SMJ会使用两个独立的游标对排好序的两张表完成归并关联。\nSMJ刚开始工作的时候，内外表的游标都会先锚定在两张表的第一条记录上，然后再对比游标所在记录的Join Key。对比结果以及后续操作主要分为3种情况：\n外表Join Key等于内表Join Key，满足关联条件，把两边的数据记录拼接并输出，然后把外表的游标滑动到下一条记录 外表Join Key小于内表Join Key，不满足关联条件，把外表的游标滑动到下一条记录 外表Join Key大于内表Join Key，不满足关联条件，把内表的游标滑动到下一条记录 SMJ正是基于这3种情况，不停地向下滑动游标，直到某张表的游标滑到头，即宣告关联结束。对于SMJ中外表的每一条记录，由于内表按Join Key升序排序，且扫描的起始位置为游标所在位置，因此SMJ算法的计算复杂度为O(M + N)。\n不过，SMJ计算复杂度的降低，仰仗的是两张表已经事先排好序。要知道，排序本身就是一项非常耗时的操作，更何况，为了完成归并关联，参与Join的两张表都需要排序。因此，SMJ的计算过程我们可以用“先苦后甜”来形容。苦的是要先花费时间给两张表做排序，甜的是有序表的归并关联能够享受到线性的计算复杂度。\nHJ的工作原理 考虑到SMJ对排序的要求比较苛刻，所以后来又有人提出了效率更高的关联算法：HJ。HJ的设计初衷非常明确：把内表扫描的计算复杂度降低至O(1)。把一个数据集合的访问效率提升至O(1)，也只有Hash Map能做到了。也正因为Join的关联过程引入了Hash计算，所以它叫HJ。\nHJ的计算分为两个阶段，分别是Build阶段和Probe阶段。在Build阶段，基于内表，算法使用既定的哈希函数构建哈希表，如上图的步骤1所示。哈希表中的Key是Join Key应用（Apply）哈希函数之后的哈希值，表中的Value同时包含了原始的Join Key和Payload。\n在Probe阶段，算法遍历每一条数据记录，先是使用同样的哈希函数，以动态的方式（On The Fly）计算Join Key的哈希值。然后，用计算得到的哈希值去查询刚刚在Build阶段创建好的哈希表。如果查询失败，说明该条记录与维度表中的数据不存在关联关系；如果查询成功，则继续对比两边的Join Key。如果Join Key一致，就把两边的记录进行拼接并输出，从而完成数据关联。\n分布式环境下的Join 掌握了这3种最主要的数据关联实现方式的工作原理之后，在单机环境中，无论是面对常见的Inner Join、Left Join、Right Join，还是不常露面的Anti Join、Semi Join，你都能对数据关联的性能调优做到游刃有余了。\n不过，你也可能会说：“Spark毕竟是个分布式系统，光学单机实现有什么用呀？”\n所谓万变不离其宗，实际上，相比单机环境，分布式环境中的数据关联在计算环节依然遵循着NLJ、SMJ和HJ这3种实现方式，只不过是增加了网络分发这一变数。在Spark的分布式计算环境中，数据在网络中的分发主要有两种方式，分别是Shuffle和广播。那么，不同的网络分发方式，对于数据关联的计算又都有哪些影响呢？\n如果采用Shuffle的分发方式来完成数据关联，那么外表和内表都需要按照Join Key在集群中做全量的数据分发。因为只有这样，两个数据表中Join Key相同的数据记录才能分配到同一个Executor进程，从而完成关联计算，如下图所示。\n如果采用广播机制的话，情况会大有不同。在这种情况下，Spark只需要把内表（基表）封装到广播变量，然后在全网进行分发。由于广播变量中包含了内表的全量数据，因此体量较大的外表只要“待在原地、保持不动”，就能轻松地完成关联计算，如下图所示。\n不难发现，结合Shuffle、广播这两种网络分发方式和NLJ、SMJ、HJ这3种计算方式，对于分布式环境下的数据关联，我们就能组合出6种Join策略，如下图所示。\n这6种Join策略，对应图中6个青色圆角矩形，从上到下颜色依次变浅，它们分别是Cartesian Product Join、Shuffle Sort Merge Join和Shuffle Hash Join。也就是采用Shuffle机制实现的NLJ、SMJ和HJ，以及Broadcast Nested Loop Join、Broadcast Sort Merge Join和Broadcast Hash Join。\n**从执行性能来说，6种策略从上到下由弱变强。**相比之下，CPJ的执行效率是所有实现方式当中最差的，网络开销、计算开销都很大，因而在图中的颜色也是最深的。BHJ是最好的分布式数据关联机制，网络开销和计算开销都是最小的，因而颜色也最浅。此外，你可能也注意到了，Broadcast Sort Merge Join被标记成了灰色，这是因为Spark并没有选择支持Broadcast + Sort Merge Join这种组合方式。\n那么问题来了，明明是6种组合策略，为什么Spark偏偏没有支持这一种呢？要回答这个问题，我们就要回过头来对比SMJ与HJ实现方式的差异与优劣势。\n相比SMJ，HJ并不要求参与Join的两张表有序，也不需要维护两个游标来判断当前的记录位置，只要基表在Build阶段构建的哈希表可以放进内存，HJ算法就可以在Probe阶段遍历外表，依次与哈希表进行关联。\n当数据能以广播的形式在网络中进行分发时，说明被分发的数据，也就是基表的数据足够小，完全可以放到内存中去。这个时候，相比NLJ、SMJ，HJ的执行效率是最高的。因此，在可以采用HJ的情况下，Spark自然就没有必要再去用SMJ这种前置开销比较大的方式去完成数据关联。\nSpark如何选择Join策略？ 那么，在不同的数据关联场景中，对于这5种Join策略来说，也就是CPJ、BNLJ、SHJ、SMJ以及BHJ，Spark会基于什么逻辑取舍呢？我们来分两种情况进行讨论，分别是等值Join，和不等值Join。\n等值Join下，Spark如何选择Join策略？ 等值Join是指两张表的Join Key是通过等值条件连接在一起的。在日常的开发中，这种Join形式是最常见的，如t1 inner join t2 on t1.id = t2.id。\n**在等值数据关联中，Spark会尝试按照BHJ \u0026gt; SMJ \u0026gt; SHJ的顺序依次选择Join策略。**在这三种策略中，执行效率最高的是BHJ，其次是SHJ，再次是SMJ。其中，SMJ和SHJ策略支持所有连接类型，如全连接、Anti Join等等。BHJ尽管效率最高，但是有两个前提条件：一是连接类型不能是全连接（Full Outer Join）；二是基表要足够小，可以放到广播变量里面去。\n那为什么SHJ比SMJ执行效率高，排名却不如SMJ靠前呢？这是个非常好的问题。我们先来说结论，相比SHJ，Spark优先选择SMJ的原因在于，SMJ的实现方式更加稳定，更不容易OOM。\n回顾HJ的实现机制，在Build阶段，算法根据内表创建哈希表。在Probe阶段，为了让外表能够成功“探测”（Probe）到每一个Hash Key，哈希表要全部放进内存才行。坦白说，这个前提还是蛮苛刻的，仅这一点要求就足以让Spark对其望而却步。要知道，在不同的计算场景中，数据分布的多样性很难保证内表一定能全部放进内存。\n而且在Spark中，SHJ策略要想被选中必须要满足两个先决条件，这两个条件都是对数据尺寸的要求。**首先，外表大小至少是内表的3倍。其次，内表数据分片的平均大小要小于广播变量阈值。**第一个条件的动机很好理解，只有当内外表的尺寸悬殊到一定程度时，HJ的优势才会比SMJ更显著。第二个限制的目的是，确保内表的每一个数据分片都能全部放进内存。\n和SHJ相比，SMJ没有这么多的附加条件，无论是单表排序，还是两表做归并关联，都可以借助磁盘来完成。内存中放不下的数据，可以临时溢出到磁盘。单表排序的过程，我们可以参考Shuffle Map阶段生成中间文件的过程。在做归并关联的时候，算法可以把磁盘中的有序数据用合理的粒度，依次加载进内存完成计算。这个粒度可大可小，大到以数据分片为单位，小到逐条扫描。\n正是考虑到这些因素，相比SHJ，Spark SQL会优先选择SMJ。事实上，在配置项spark.sql.join.preferSortMergeJoin默认为True的情况下，Spark SQL会用SMJ策略来兜底，确保作业执行的稳定性，压根就不会打算去尝试SHJ。开发者如果想通过配置项来调整Join策略，需要把这个参数改为False，这样Spark SQL才有可能去尝试SHJ。\n不等值Join下，Spark如何选择Join策略？ 接下来，我们再来说说不等值Join，它指的是两张表的Join Key是通过不等值条件连接在一起的。不等值Join其实我们在以前的例子中也见过，比如像查询语句t1 inner join t2 on t1.date \u0026gt; t2.beginDate and t1.date \u0026lt;= t2.endDate，其中的关联关系是依靠不等式连接在一起的。\n**由于不等值Join只能使用NLJ来实现，因此Spark SQL可选的Join策略只剩下BNLJ和CPJ。**在同一种计算模式下，相比Shuffle，广播的网络开销更小。显然，在两种策略的选择上，Spark SQL一定会按照BNLJ \u0026gt; CPJ的顺序进行尝试。当然，BNLJ生效的前提自然是内表小到可以放进广播变量。如果这个条件不成立，那么Spark SQL只好委曲求全，使用笨重的CPJ策略去完成关联计算。\n开发者能做些什么？ 最后，我们再来聊聊，面对上述的5种Join策略，开发者还能做些什么呢？通过上面的分析，我们不难发现，Spark SQL对于这些策略的取舍也基于一些既定的规则。所谓计划赶不上变化，预置的规则自然很难覆盖多样且变化无常的计算场景。因此，当我们掌握了不同Join策略的工作原理，结合我们对于业务和数据的深刻理解，完全可以自行决定应该选择哪种Join策略。\n在最新发布的3.0版本中，Spark为开发者提供了多样化的Join Hints，允许你把专家经验凌驾于Spark SQL的选择逻辑之上。**在满足前提条件的情况下，如等值条件、连接类型、表大小等等，Spark会优先尊重开发者的意愿，去选取开发者通过Join Hints指定的Join策略。**关于Spark 3.0支持的Join Hints关键字，以及对应的适用场景，我把它们总结到了如上的表格中，你可以直接拿来参考。\n简单来说，你可以使用两种方式来指定Join Hints，一种是通过SQL结构化查询语句，另一种是使用DataFrame的DSL语言，都很方便。至于更全面的讲解，你可以去第13讲看看，这里我就不多说了。\n小结 这一讲，我们从数据关联的实现原理，到Spark SQL不同Join策略的适用场景，掌握这些关键知识点，对于数据关联场景中的性能调优至关重要。\n首先，你需要掌握3种Join实现机制的工作原理。为了方便你对比，我把它们总结在了下面的表格里。\n掌握了3种关联机制的实现原理，你就能更好地理解Spark SQL的Join策略。结合数据的网络分发方式（Shuffle和广播），Spark SQL支持5种Join策略，按照执行效率排序就是BHJ \u0026gt; SHJ \u0026gt; SMJ \u0026gt; BNLJ \u0026gt; CPJ。同样，为了方便对比，你也可以直接看下面的表格。\n最后，当你掌握了不同Join策略的工作原理，结合对于业务和数据的深刻理解，实际上你可以自行决定应该选择哪种Join策略，不必完全依赖Spark SQL的判断。\nSpark为开发者提供了多样化的Join Hints，允许你把专家经验凌驾于Spark SQL的选择逻辑之上。比如，当你确信外表比内表大得多，而且内表数据分布均匀，使用SHJ远比默认的SMJ效率高得多的时候，你就可以通过指定Join Hints来强制Spark SQL按照你的意愿去选择Join策略。\n","date":"2025-02-21T14:15:39+08:00","image":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3joinhints%E7%89%B9%E6%80%A7/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu6307248181568134095.jpg","permalink":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3joinhints%E7%89%B9%E6%80%A7/","title":"详解JoinHints特性"},{"content":"概述 距离Spark 3.0版本的发布已经将近一年的时间了，这次版本升级添加了自适应查询执行（AQE）、动态分区剪裁（DPP）和扩展的 Join Hints 等新特性。利用好这些新特性，可以让我们的性能调优如虎添翼。因此，我会用三讲的时间和你聊聊它们。今天，我们先来说说AQE。\nSpark为什么需要AQE 在2.0版本之前，Spark SQL仅仅支持启发式、静态的优化过程，启发式的优化又叫RBO（Rule Based Optimization，基于规则的优化），它往往基于一些规则和策略实现，如谓词下推、列剪枝，这些规则和策略来源于数据库领域已有的应用经验。也就是说，启发式的优化实际上算是一种经验主义。经验主义的弊端就是不分青红皂白、胡子眉毛一把抓，对待相似的问题和场景都使用同一类套路。Spark社区正是因为意识到了RBO的局限性，因此在2.2版本中推出了CBO（Cost Based Optimization，基于成本的优化）。\nCBO的特点是“实事求是”，基于数据表的统计信息（如表大小、数据列分布）来选择优化策略。CBO支持的统计信息很丰富，比如数据表的行数、每列的基数（Cardinality）、空值数、最大值、最小值和直方图等等。因为有统计数据做支持，所以CBO选择的优化策略往往优于RBO选择的优化规则。\n但是，CBO也面临三个方面的窘境：“窄、慢、静”。窄指的是适用面太窄，CBO仅支持注册到Hive Metastore的数据表，但在大量的应用场景中，数据源往往是存储在分布式文件系统的各类文件，如Parquet、ORC、CSV等等。慢指的是统计信息的搜集效率比较低。对于注册到Hive Metastore的数据表，开发者需要调用ANALYZE TABLE COMPUTE STATISTICS语句收集统计信息，而各类信息的收集会消耗大量时间。静指的是静态优化，这一点与RBO一样。CBO结合各类统计信息制定执行计划，一旦执行计划交付运行，CBO的使命就算完成了。换句话说，如果在运行时数据分布发生动态变化，CBO先前制定的执行计划并不会跟着调整、适配。\nAQE是什么 考虑到RBO和CBO的种种限制，Spark在3.0版本推出了AQE（Adaptive Query Execution，自适应查询执行）。如果用一句话来概括，AQE是Spark SQL的一种动态优化机制，在运行时，每当Shuffle Map阶段执行完毕，AQE都会结合这个阶段的统计信息，基于既定的规则动态地调整、修正尚未执行的逻辑计划和物理计划，来完成对原始查询语句的运行时优化。\n从定义中，我们不难发现，AQE优化机制触发的时机是Shuffle Map阶段执行完毕。也就是说，AQE优化的频次与执行计划中Shuffle的次数一致。反过来说，如果你的查询语句不会引入Shuffle操作，那么Spark SQL是不会触发AQE的。对于这样的查询，无论你怎么调整AQE相关的配置项，AQE也都爱莫能助。\n对于AQE的定义，我相信你还有很多问题，比如，AQE依赖的统计信息具体是什么？既定的规则和策略具体指什么？接下来，我们一一来解答。\n**首先，AQE赖以优化的统计信息与CBO不同，这些统计信息并不是关于某张表或是哪个列，而是Shuffle Map阶段输出的中间文件。**学习过Shuffle的工作原理之后，我们知道，每个Map Task都会输出以data为后缀的数据文件，还有以index为结尾的索引文件，这些文件统称为中间文件。每个data文件的大小、空文件数量与占比、每个Reduce Task对应的分区大小，所有这些基于中间文件的统计值构成了AQE进行优化的信息来源。\n其次，结合Spark SQL端到端优化流程图我们可以看到，AQE从运行时获取统计信息，在条件允许的情况下，优化决策会分别作用到逻辑计划和物理计划。\n**AQE既定的规则和策略主要有4个，分为1个逻辑优化规则和3个物理优化策略。**我把这些规则与策略，和相应的AQE特性，以及每个特性仰仗的统计信息，都汇总到了如下的表格中。\n如何用好AQE 那么，AQE是如何根据Map阶段的统计信息以及这4个规则与策略，来动态地调整和修正尚未执行的逻辑计划和物理计划的呢？这就要提到AQE的三大特性，也就是Join策略调整、自动分区合并，以及自动倾斜处理，我们需要借助它们去分析AQE动态优化的过程。它们的基本概念我们在第9讲说过，这里我再带你简单回顾一下。\nJoin策略调整：如果某张表在过滤之后，尺寸小于广播变量阈值，这张表参与的数据关联就会从Shuffle Sort Merge Join降级（Demote）为执行效率更高的Broadcast Hash Join。 自动分区合并：在Shuffle过后，Reduce Task数据分布参差不齐，AQE将自动合并过小的数据分区。 自动倾斜处理：结合配置项，AQE自动拆分Reduce阶段过大的数据分区，降低单个Reduce Task的工作负载。 接下来，我们就一起来分析这3个特性的动态优化过程。\nJoin策略调整 我们先来说说Join策略调整，这个特性涉及了一个逻辑规则和一个物理策略，它们分别是DemoteBroadcastHashJoin和OptimizeLocalShuffleReader。\n**DemoteBroadcastHashJoin规则的作用，是把Shuffle Joins降级为Broadcast Joins。需要注意的是，这个规则仅适用于Shuffle Sort Merge Join这种关联机制，其他机制如Shuffle Hash Join、Shuffle Nested Loop Join都不支持。**对于参与Join的两张表来说，在它们分别完成Shuffle Map阶段的计算之后，DemoteBroadcastHashJoin会判断中间文件是否满足如下条件：\n中间文件尺寸总和小于广播阈值spark.sql.autoBroadcastJoinThreshold 空文件占比小于配置项spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin 只要有任意一张表的统计信息满足这两个条件，Shuffle Sort Merge Join就会降级为Broadcast Hash Join。说到这儿，你可能会问：“既然DemoteBroadcastHashJoin逻辑规则可以把Sort Merge Join转换为Broadcast Join，那同样用来调整Join策略的OptimizeLocalShuffleReader规则又是干什么用的呢？看上去有些多余啊！”\n不知道你注意到没有，我一直强调，AQE依赖的统计信息来自于Shuffle Map阶段生成的中间文件。这意味什么呢？这就意味着AQE在开始优化之前，Shuffle操作已经执行过半了！\n我们来举个例子，现在有两张表：事实表Order和维度表User，它们的查询语句和初始的执行计划如下。\n1 2 3 4 5 6 //订单表与用户表关联 select sum(order.price * order.volume), user.id from order inner join user on order.userId = user.id where user.type = ‘Head Users’ group by user.id 由于两张表大都到超过了广播阈值，因此Spark SQL在最初的执行计划中选择了Sort Merge Join。AQE需要同时结合两个分支中的Shuffle（Exchange）输出，才能判断是否可以降级为Broadcast Join，以及用哪张表降级。这就意味着，不论大表还是小表都要完成Shuffle Map阶段的计算，并且把中间文件落盘，AQE才能做出决策。\n你可能会说：“根本不需要大表做Shuffle呀，AQE只需要去判断小表Shuffle的中间文件就好啦”。可问题是，AQE可分不清哪张是大表、哪张是小表。在Shuffle Map阶段结束之前，数据表的尺寸大小对于AQE来说是“透明的”。因此，AQE必须等待两张表都完成Shuffle Map的计算，然后统计中间文件，才能判断降级条件是否成立，以及用哪张表做广播变量。\n在常规的Shuffle计算流程中，Reduce阶段的计算需要跨节点访问中间文件拉取数据分片。如果遵循常规步骤，即便AQE在运行时把Shuffle Sort Merge Join降级为Broadcast Join，大表的中间文件还是需要通过网络进行分发。这个时候，AQE的动态Join策略调整也就失去了实用价值。原因很简单，负载最重的大表Shuffle计算已经完成，再去决定切换到Broadcast Join已经没有任何意义。\n在这样的背景下，OptimizeLocalShuffleReader物理策略就非常重要了。既然大表已经完成Shuffle Map阶段的计算，这些计算可不能白白浪费掉。采取OptimizeLocalShuffleReader策略可以省去Shuffle常规步骤中的网络分发，Reduce Task可以就地读取本地节点（Local）的中间文件，完成与广播小表的关联操作。\n不过，需要我们特别注意的是，OptimizeLocalShuffleReader物理策略的生效与否由一个配置项决定。这个配置项是spark.sql.adaptive.localShuffleReader.enabled，尽管它的默认值是True，但是你千万不要把它的值改为False。否则，就像我们刚才说的，AQE的Join策略调整就变成了形同虚设。\n说到这里，你可能会说：“这么看，AQE的Join策略调整有些鸡肋啊！毕竟Shuffle计算都已经过半，Shuffle Map阶段的内存消耗和磁盘I/O是半点没省！”确实，Shuffle Map阶段的计算开销是半点没省。但是，OptimizeLocalShuffleReader策略避免了Reduce阶段数据在网络中的全量分发，仅凭这一点，大多数的应用都能获益匪浅。因此，对于AQE的Join策略调整，我们可以用一个成语来形容：“亡羊补牢、犹未为晚”。\n自动分区合并 接下来，我们再来说说自动分区合并。分区合并的原理比较简单，在Reduce阶段，当Reduce Task从全网把数据分片拉回，AQE按照分区编号的顺序，依次把小于目标尺寸的分区合并在一起。目标分区尺寸由以下两个参数共同决定。这部分我们在第10讲详细讲过，如果不记得，你可以翻回去看一看。\nspark.sql.adaptive.advisoryPartitionSizeInBytes，由开发者指定分区合并后的推荐尺寸。 spark.sql.adaptive.coalescePartitions.minPartitionNum，分区合并后，分区数不能低于该值。 除此之外，我们还要注意，在Shuffle Map阶段完成之后，AQE优化机制被触发，CoalesceShufflePartitions策略“无条件”地被添加到新的物理计划中。读取配置项、计算目标分区大小、依序合并相邻分区这些计算逻辑，在Tungsten WSCG的作用下融合进“手写代码”于Reduce阶段执行。\n自动倾斜处理 与自动分区合并相反，自动倾斜处理的操作是“拆”。在Reduce阶段，当Reduce Task所需处理的分区尺寸大于一定阈值时，利用OptimizeSkewedJoin策略，AQE会把大分区拆成多个小分区。倾斜分区和拆分粒度由以下这些配置项决定。关于它们的含义与作用，我们在第10讲说过，你可以再翻回去看一看。\nspark.sql.adaptive.skewJoin.skewedPartitionFactor，判定倾斜的膨胀系数 spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes，判定倾斜的最低阈值 spark.sql.adaptive.advisoryPartitionSizeInBytes，以字节为单位，定义拆分粒度 自动倾斜处理的拆分操作也是在Reduce阶段执行的。在同一个Executor内部，本该由一个Task去处理的大分区，被AQE拆成多个小分区并交由多个Task去计算。这样一来，Task之间的计算负载就可以得到平衡。但是，这并不能解决不同Executors之间的负载均衡问题。\n我们来举个例子，假设有个Shuffle操作，它的Map阶段有3个分区，Reduce阶段有4个分区。4个分区中的两个都是倾斜的大分区，而且这两个倾斜的大分区刚好都分发到了Executor 0。通过下图，我们能够直观地看到，尽管两个大分区被拆分，但横向来看，整个作业的主要负载还是落在了Executor 0的身上。Executor 0的计算能力依然是整个作业的瓶颈，这一点并没有因为分区拆分而得到实质性的缓解。\n另外，在数据关联的场景中，对于参与Join的两张表，我们暂且把它们记做数据表1和数据表2，如果表1存在数据倾斜，表2不倾斜，那在关联的过程中，AQE除了对表1做拆分之外，还需要对表2对应的数据分区做复制，来保证关联关系不被破坏。\n在这样的运行机制下，如果两张表都存在数据倾斜怎么办？这个时候，事情就开始变得逐渐复杂起来了。对于上图中的表1和表2，我们假设表1还是拆出来两个分区，表2因为倾斜也拆出来两个分区。这个时候，为了不破坏逻辑上的关联关系，表1、表2拆分出来的分区还要各自复制出一份，如下图所示。\n如果现在问题变得更复杂了，左表拆出M个分区，右表拆出N各分区，那么每张表最终都需要保持M x N份分区数据，才能保证关联逻辑的一致性。当M和N逐渐变大时，AQE处理数据倾斜所需的计算开销将会面临失控的风险。\n**总的来说，当应用场景中的数据倾斜比较简单，比如虽然有倾斜但数据分布相对均匀，或是关联计算中只有一边倾斜，我们完全可以依赖AQE的自动倾斜处理机制。但是，当我们的场景中数据倾斜变得复杂，比如数据中不同Key的分布悬殊，或是参与关联的两表都存在大量的倾斜，我们就需要衡量AQE的自动化机制与手工处理倾斜之间的利害得失。**关于手工处理倾斜，我们留到第28讲再去展开。\n总结 AQE是Spark SQL的一种动态优化机制，它的诞生解决了RBO、CBO，这些启发式、静态优化机制的局限性。想要用好AQE，我们就要掌握它的特点，以及它支持的三种优化特性的工作原理和使用方法。\n如果用一句话来概括AQE的定义，就是每当Shuffle Map阶段执行完毕，它都会结合这个阶段的统计信息，根据既定的规则和策略动态地调整、修正尚未执行的逻辑计划和物理计划，从而完成对原始查询语句的运行时优化。也因此，只有当你的查询语句会引入Shuffle操作的时候，Spark SQL才会触发AQE。\nAQE支持的三种优化特性分别是Join策略调整、自动分区合并和自动倾斜处理。\n关于Join策略调整，我们首先要知道DemoteBroadcastHashJoin规则仅仅适用于Shuffle Sort Merge Join这种关联机制，对于其他Shuffle Joins类型，AQE暂不支持把它们转化为Broadcast Joins。其次，为了确保AQE的Join策略调整正常运行，我们要确保spark.sql.adaptive.localShuffleReader.enabled配置项始终为开启状态。\n关于自动分区合并，我们要知道，在Shuffle Map阶段完成之后，结合分区推荐尺寸与分区数量限制，AQE会自动帮我们完成分区合并的计算过程。\n关于AQE的自动倾斜处理我们要知道，它只能以Task为粒度缓解数据倾斜，并不能解决不同Executors之间的负载均衡问题。针对场景较为简单的倾斜问题，比如关联计算中只涉及单边倾斜，我们完全可以依赖AQE的自动倾斜处理机制。但是，当数据倾斜问题变得复杂的时候，我们需要衡量AQE的自动化机制与手工处理倾斜之间的利害得失。\n","date":"2025-02-21T13:56:45+08:00","image":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3aqe%E4%B8%89%E5%A4%A7%E7%89%B9%E6%80%A7/paddle-boat-8253274_1280_hu10546650770668311781.jpg","permalink":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3aqe%E4%B8%89%E5%A4%A7%E7%89%B9%E6%80%A7/","title":"详解AQE三大特性"},{"content":"概述 DPP（Dynamic Partition Pruning，动态分区剪裁）是Spark 3.0版本中第二个引人注目的特性，它指的是在星型数仓的数据关联场景中，可以充分利用过滤之后的维度表，大幅削减事实表的数据扫描量，从整体上提升关联计算的执行性能。\n分区剪裁 我们先来看这个例子。在星型（Start Schema）数仓中，我们有两张表，一张是订单表orders，另一张是用户表users。显然，订单表是事实表（Fact），而用户表是维度表（Dimension）。业务需求是统计所有头部用户贡献的营业额，并按照营业额倒序排序。那这个需求该怎么实现呢？\n首先，我们来了解一下两张表的关键字段，看看查询语句应该怎么写。\n1 2 3 4 5 6 7 8 9 10 // 订单表orders关键字段 userId, Int itemId, Int price, Float quantity, Int // 用户表users关键字段 id, Int name, String type, String //枚举值，分为头部用户和长尾用户 给定上述数据表，我们只需把两张表做内关联，然后分组、聚合、排序，就可以实现业务逻辑，具体的查询语句如下。\n1 2 3 4 5 select (orders.price * order.quantity) as income, users.name from orders inner join users on orders.userId = users.id where users.type = ‘Head User’ group by users.name order by income desc 看到这样的查询语句，再结合Spark SQL那几讲学到的知识，我们很快就能画出它的逻辑执行计划。\n由于查询语句中事实表上没有过滤条件，因此，在执行计划的左侧，Spark SQL选择全表扫描的方式来投影出userId、price和quantity这些字段。相反，维度表上有过滤条件users.type = ‘Head User’，因此，Spark SQL可以应用谓词下推规则，把过滤操作下推到数据源之上，来减少必需的磁盘I/O开销。\n虽然谓词下推已经很给力了，但如果用户表支持分区剪裁（Partition Pruning），I/O效率的提升就会更加显著。那什么是分区剪裁呢？实际上，分区剪裁是谓词下推的一种特例，它指的是在分区表中下推谓词，并以文件系统目录为单位对数据集进行过滤。分区表就是通过指定分区键，然后使用partitioned by语句创建的数据表，或者是使用partitionBy语句存储的列存文件（如Parquet、ORC等）。\n相比普通数据表，分区表特别的地方就在于它的存储方式。对于分区键中的每一个数据值，分区表都会在文件系统中创建单独的子目录来存储相应的数据分片。拿用户表来举例，假设用户表是分区表，且以type字段作为分区键，那么用户表会有两个子目录，前缀分别是“Head User”和“Tail User”。数据记录被存储于哪个子目录完全取决于记录中type字段的值，比如：所有type字段值为“Head User”的数据记录都被存储到前缀为“Head User”的子目录。同理，所有type字段值为“Tail User”的数据记录，全部被存放到前缀为“Tail User”的子目录。\n不难发现，**如果过滤谓词中包含分区键，那么Spark SQL对分区表做扫描的时候，是完全可以跳过（剪掉）不满足谓词条件的分区目录，这就是分区剪裁。**例如，在我们的查询语句中，用户表的过滤谓词是“users.type = ‘Head User’”。假设用户表是分区表，那么对于用户表的数据扫描，Spark SQL可以完全跳过前缀为“Tail User”的子目录。\n通过与谓词下推作对比，我们可以直观地感受分区剪裁的威力。如图所示，上下两行分别表示用户表在不做分区和做分区的情况下，Spark SQL对于用户表的数据扫描。在不做分区的情况下，用户表所有的数据分片全部存于同一个文件系统目录，尽管Parquet格式在注脚（Footer)中提供了type字段的统计值，Spark SQL可以利用谓词下推来减少需要扫描的数据分片，但由于很多分片注脚中的type字段同时包含‘Head User’和‘Tail User’（第一行3个浅绿色的数据分片），因此，用户表的数据扫描仍然会涉及4个数据分片。\n相反，当用户表本身就是分区表时，由于type字段为‘Head User’的数据记录全部存储到前缀为‘Head User’的子目录，也就是图中第二行浅绿色的文件系统目录，这个目录中仅包含两个type字段全部为‘Head User’的数据分片。这样一来，Spark SQL可以完全跳过其他子目录的扫描，从而大幅提升I/O效率。\n你可能会说：“既然分区剪裁这么厉害，那么我是不是也可以把它应用到事实表上去呢？毕竟事实表的体量更大，相比维度表，事实表上I/O效率的提升空间更大。”没错，如果事实表本身就是分区表，且过滤谓词中包含分区键，那么Spark SQL同样会利用分区剪裁特性来大幅减少数据扫描量。\n不过，对于实际工作中的绝大多数关联查询来说，事实表都不满足分区剪裁所需的前提条件。比如说，要么事实表不是分区表，要么事实表上没有过滤谓词，或者就是过滤谓词不包含分区键。就拿电商场景的例子来说，查询中压根就没有与订单表相关的过滤谓词。因此，即便订单表本身就是分区表，Spark SQL也没办法利用分区剪裁特性。\n对于这样的关联查询，我们是不是只能任由Spark SQL去全量扫描事实表呢？要是在以前，我们还没什么办法。不过，有了Spark 3.0推出的DPP特性之后，情况就大不一样了。\n动态分区剪裁 我们刚才说了，DPP指的是在数据关联的场景中，Spark SQL利用维度表提供的过滤信息，减少事实表中数据的扫描量、降低I/O开销，从而提升执行性能。那么，DPP是怎么做到这一点的呢？它背后的逻辑是什么？为了方便你理解，我们还用刚刚的例子来解释。\n首先，过滤条件users.type = ‘Head User’会帮助维度表过滤一部分数据。与此同时，维度表的ID字段也顺带着经过一轮筛选，如图中的步骤1所示。经过这一轮筛选之后，保留下来的ID值，仅仅是维度表ID全集的一个子集。\n然后，在关联关系也就是orders.userId = users.id的作用下，过滤效果会通过users的ID字段传导到事实表的userId字段，也就是图中的步骤2。这样一来，满足关联关系的userId值，也是事实表userId全集中的一个子集。把满足条件的userId作为过滤条件，应用（Apply）到事实表的数据源，就可以做到减少数据扫描量，提升I/O效率，如图中的步骤3所示。\nDPP正是基于上述逻辑，把维度表中的过滤条件，通过关联关系传导到事实表，从而完成事实表的优化。虽然DPP的运作逻辑非常清晰，但并不是所有的数据关联场景都可以享受到DPP的优化机制，想要利用DPP来加速事实表数据的读取和访问，数据关联场景还要满足三个额外的条件。\n首先，DPP是一种分区剪裁机制，它是以分区为单位对事实表进行过滤。结合刚才的逻辑，维度表上的过滤条件会转化为事实表上Join Key的过滤条件。具体到我们的例子中，就是orders.userId这个字段。显然，DPP生效的前提是事实表按照orders.userId这一列预先做好了分区。因此，事实表必须是分区表，而且分区字段（可以是多个）必须包含Join Key。\n其次，过滤效果的传导，依赖的是等值的关联关系，比如orders.userId = users.id。因此，DPP仅支持等值Joins，不支持大于、小于这种不等值关联关系。\n此外，DPP机制得以实施还有一个隐含的条件：维度表过滤之后的数据集要小于广播阈值。\n拿维度表users来说，满足过滤条件users.type = ‘Head User’的数据集，要能够放进广播变量，DPP优化机制才能生效。为什么会这样呢？这就要提到DPP机制的实现原理了。\n结合刚才对于DPP实现逻辑的分析和推导，我们不难发现，实现DPP机制的关键在于，我们要让处理事实表的计算分支，能够拿到满足过滤条件的Join Key列表，然后用这个列表来对事实表做分区剪裁。那么问题来了，用什么办法才能拿到这个列表呢？\nSpark SQL选择了一种“一箭双雕”的做法：使用广播变量封装过滤之后的维度表数据。具体来说，在维度表做完过滤之后，Spark SQL在其上构建哈希表（Hash Table），这个哈希表的Key就是用于关联的Join Key。在我们的例子中，Key就是满足过滤users.type = ‘Head User’条件的users.id；Value是投影中需要引用的数据列，在之前订单表与用户表的查询中，这里的引用列就是users.name。\n哈希表构建完毕之后，Spark SQL将其封装到广播变量中，这个广播变量的作用有二。第一个作用就是给事实表用来做分区剪裁，如图中的步骤1所示，哈希表中的Key Set刚好可以用来给事实表过滤符合条件的数据分区。\n第二个作用就是参与后续的Broadcast Join数据关联，如图中的步骤2所示。这里的哈希表，本质上就是Hash Join中的Build Table，其中的Key、Value，记录着数据关联中所需的所有字段，如users.id、users.name，刚好拿来和事实表做Broadcast Hash Join。\n因此你看，鉴于Spark SQL选择了广播变量的实现方式，要想有效利用DPP优化机制，我们就必须要确保，过滤后的维度表刚好能放到广播变量中去。也因此，我们必须要谨慎对待配置项spark.sql.autoBroadcastJoinThreshold。\n总结 这一讲，我们围绕动态分区剪裁，学习了谓词下推和分区剪裁的联系和区别，以及动态分区剪裁的定义、特点和使用方法。\n相比于谓词下推，分区剪裁往往能更好地提升磁盘访问的I/O效率。\n这是因为，谓词下推操作往往是根据文件注脚中的统计信息完成对文件的过滤，过滤效果取决于文件中内容的“纯度”。分区剪裁则不同，它的分区表可以把包含不同内容的文件，隔离到不同的文件系统目录下。这样一来，包含分区键的过滤条件能够以文件系统目录为粒度对磁盘文件进行过滤，从而大幅提升磁盘访问的I/O效率。\n而动态分区剪裁这个功能主要用在星型模型数仓的数据关联场景中，它指的是在运行的时候，Spark SQL利用维度表提供的过滤信息，来减少事实表中数据的扫描量、降低I/O开销，从而提升执行性能。\n动态分区剪裁运作的背后逻辑，是把维度表中的过滤条件，通过关联关系传导到事实表，来完成事实表的优化。在数据关联的场景中，开发者要想利用好动态分区剪裁特性，需要注意3点：\n事实表必须是分区表，并且分区字段必须包含Join Key 动态分区剪裁只支持等值Joins，不支持大于、小于这种不等值关联关系 维度表过滤之后的数据集，必须要小于广播阈值，因此，开发者要注意调整配置项spark.sql.autoBroadcastJoinThreshold ","date":"2025-02-21T13:56:00+08:00","image":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3dpp%E7%89%B9%E6%80%A7/mountains-8585535_1280_hu4825469915763682895.jpg","permalink":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3dpp%E7%89%B9%E6%80%A7/","title":"详解DPP特性"},{"content":"一、背景 迁移Hive数仓历史数据到数据湖的流程中，会先将Hive迁移到Iceberg镜像表，待质量验证通过再同步到Iceberg正式表。本次验证的是就是从Iceberg镜像表迁移数据到Iceberg正式表的流程\n二、验证思路 操作步骤 启动SparkSQL环境\n创建Iceberg镜像表A并写入数据\n创建跟镜像表A相同表结构的正式表B\n通过Iceberg原生的add_files存储过程将镜像表A的数据迁移到正式表B\n分析步骤 分析迁移前后表A和表B的表元信息\n分析迁移前后表A和表B的文件布局\n分析迁移前后位于HDFS上文件的内容差异\n三、验证场景 本次从三个场景进行验证分析\n不移动数据文件，仅通过add_files方式进行验证 手动移动数据文件到正式表data目录下，再通过add_files进行数据加载 验证Iceberg分支特性应用于迁移场景的效果 四、验证场景一（仅add_files方式） 启动SparkSQL 1 2 3 4 5 6 7 spark-sql --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \\ --conf spark.sql.catalog.spark_catalog.type=hive \\ --conf spark.sql.catalog.hive=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.hive.type=hive \\ --conf spark.sql.catalog.hive.uri=thrift://127.0.0.1:9083 \\ --conf spark.sql.catalog.hive.warehouse=hdfs://warehouse 创建镜像表A并写入数据 1 2 3 4 5 6 7 8 9 use hive_prod.default; CREATE TABLE test_add_files_src (id bigint,data string,category string) USING iceberg PARTITIONED BY (category); INSERT INTO test_add_files_src VALUES (1, \u0026#39;a\u0026#39;,\u0026#39;test\u0026#39;), (2, \u0026#39;b\u0026#39;,\u0026#39;test\u0026#39;), (3, \u0026#39;c\u0026#39;,\u0026#39;now\u0026#39;); desc formatted test_add_files_src; 查看镜像表A的结构信息以及HDFS布局信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 id bigint data string category string # Partition Information # col_name data_type comment category string # Metadata Columns _spec_id int _partition struct\u0026lt;category:string\u0026gt; _file string _pos bigint _deleted boolean # Detailed Table Information Name hive_prod.default.test_add_files_src Type MANAGED Location hdfs://localhost:9000/user/hive/warehouse/test_add_files_src Provider iceberg Owner lin Table Properties [current-snapshot-id=4643790123769250304,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd] Time taken: 0.216 seconds, Fetched 21 row(s) ├── test_add_files_src │ ├── data │ │ ├── category=now │ │ │ └── 00000-3-9b80e070-0c5a-4b73-a874-b3bead890332-0-00002.parquet │ │ └── category=test │ │ └── 00000-3-9b80e070-0c5a-4b73-a874-b3bead890332-0-00001.parquet │ └── metadata │ ├── 00000-26c68598-db37-4cf9-a592-86eec8010e77.metadata.json │ ├── 00001-55d99ee8-0145-4af2-ba60-41ba2f9c2260.metadata.json │ ├── a7a44fcc-68ae-4341-bb34-f0f93f065d58-m0.avro │ └── snap-4643790123769250304-1-a7a44fcc-68ae-4341-bb34-f0f93f065d58.avro 创建正式表B 1 2 3 CREATE TABLE test_add_files_target (id bigint,data string,category string) USING iceberg PARTITIONED BY (category); 查看表B的结构信息以及HDFS布局(迁移前) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 id bigint data string category string # Partition Information # col_name data_type comment category string # Metadata Columns _spec_id int _partition struct\u0026lt;category:string\u0026gt; _file string _pos bigint _deleted boolean # Detailed Table Information Name hive_prod.default.test_add_files_target Type MANAGED Location hdfs://localhost:9000/user/hive/warehouse/test_add_files_target Provider iceberg Owner lin Table Properties [current-snapshot-id=none,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd] Time taken: 0.216 seconds, Fetched 21 row(s) ├── test_add_files_target │ └── metadata │ └── 00000-cb0ee426-8a4e-45be-8b35-fa9276cebc9d.metadata.json 执行迁移存储过程add_files 1 2 3 4 call spark_catalog.system.add_files( table =\u0026gt; \u0026#39;default.test_add_files_target\u0026#39;, source_table =\u0026gt; \u0026#39;`parquet`.`hdfs://localhost:9000/user/hive/warehouse/test_add_files_src/data`\u0026#39; ); 验证数据 1 select * from test_add_files_target; 通过查询可看到数据已经成功迁移到正式表B\n分析迁移后的两张表结构信息 1 2 desc formatted test_add_files_src; desc formatted test_add_files_target; 镜像表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 id bigint data string category string # Partition Information # col_name data_type comment category string # Metadata Columns _spec_id int _partition struct\u0026lt;category:string\u0026gt; _file string _pos bigint _deleted boolean # Detailed Table Information Name hive_prod.default.test_add_files_src Type MANAGED Location hdfs://localhost:9000/user/hive/warehouse/test_add_files_src Provider iceberg Owner lin Table Properties [current-snapshot-id=4643790123769250304,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd] Time taken: 0.128 seconds, Fetched 21 row(s) 正式表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 id bigint data string category string # Partition Information # col_name data_type comment category string # Metadata Columns _spec_id int _partition struct\u0026lt;category:string\u0026gt; _file string _pos bigint _deleted boolean # Detailed Table Information Name hive_prod.default.test_add_files_target Type MANAGED Location hdfs://localhost:9000/user/hive/warehouse/test_add_files_target Provider iceberg Owner lin Table Properties [current-snapshot-id=1222093753720340072,format=iceberg/parquet,format-version=2,schema.name-mapping.default=[ { \u0026#34;field-id\u0026#34; : 1, \u0026#34;names\u0026#34; : [ \u0026#34;id\u0026#34; ] }, { \u0026#34;field-id\u0026#34; : 2, \u0026#34;names\u0026#34; : [ \u0026#34;data\u0026#34; ] }, { \u0026#34;field-id\u0026#34; : 3, \u0026#34;names\u0026#34; : [ \u0026#34;category\u0026#34; ] } ],write.parquet.compression-codec=zstd] Time taken: 0.13 seconds, Fetched 21 row(s) 分析迁移后的HDFS上两张表文件布局 镜像表\n1 2 3 4 5 6 7 8 9 10 11 ├── test_add_files_src │ ├── data │ │ ├── category=now │ │ │ └── 00000-3-9b80e070-0c5a-4b73-a874-b3bead890332-0-00002.parquet │ │ └── category=test │ │ └── 00000-3-9b80e070-0c5a-4b73-a874-b3bead890332-0-00001.parquet │ └── metadata │ ├── 00000-26c68598-db37-4cf9-a592-86eec8010e77.metadata.json │ ├── 00001-55d99ee8-0145-4af2-ba60-41ba2f9c2260.metadata.json │ ├── a7a44fcc-68ae-4341-bb34-f0f93f065d58-m0.avro │ └── snap-4643790123769250304-1-a7a44fcc-68ae-4341-bb34-f0f93f065d58.avro 正式表\n1 2 3 4 5 6 7 ├── test_add_files_target │ └── metadata │ ├── 00000-cb0ee426-8a4e-45be-8b35-fa9276cebc9d.metadata.json │ ├── 00001-42c5de92-c6eb-4659-a0dc-50c4b9798ed1.metadata.json │ ├── 00002-33330ca5-c55f-439c-b24b-00c613343376.metadata.json │ ├── snap-1222093753720340072-1-01b8de06-fcdf-415a-a276-be5c22b85175.avro │ └── stage-10-task-406-manifest-4fb6c376-f6f9-42cb-aa1b-66e1161c137a.avro 检验镜像表数据时间戳 可以看到镜像表的数据文件没有被移动且也没有被修改过\n分析Iceberg正式表的清单数据 通过avro-tools工具进行编译成json文件方便查看\n1 avro-tools tojson ~/Downloads/stage-10-task-406-manifest-4fb6c376-f6f9-42cb-aa1b-66e1161c137a.avro 文件内容如下\n1 2 {\u0026#34;status\u0026#34;:1,\u0026#34;snapshot_id\u0026#34;:null,\u0026#34;data_file\u0026#34;:{\u0026#34;file_path\u0026#34;:\u0026#34;hdfs://localhost:9000/user/hive/warehouse/test_add_files_src/data/category=now/00000-3-9b80e070-0c5a-4b73-a874-b3bead890332-0-00002.parquet\u0026#34;,\u0026#34;file_format\u0026#34;:\u0026#34;PARQUET\u0026#34;,\u0026#34;partition\u0026#34;:{\u0026#34;category\u0026#34;:{\u0026#34;string\u0026#34;:\u0026#34;now\u0026#34;}},\u0026#34;record_count\u0026#34;:1,\u0026#34;file_size_in_bytes\u0026#34;:860,\u0026#34;block_size_in_bytes\u0026#34;:67108864,\u0026#34;column_sizes\u0026#34;:{\u0026#34;array\u0026#34;:[{\u0026#34;key\u0026#34;:1,\u0026#34;value\u0026#34;:40},{\u0026#34;key\u0026#34;:2,\u0026#34;value\u0026#34;:37},{\u0026#34;key\u0026#34;:3,\u0026#34;value\u0026#34;:39}]},\u0026#34;value_counts\u0026#34;:{\u0026#34;array\u0026#34;:[{\u0026#34;key\u0026#34;:1,\u0026#34;value\u0026#34;:1},{\u0026#34;key\u0026#34;:2,\u0026#34;value\u0026#34;:1},{\u0026#34;key\u0026#34;:3,\u0026#34;value\u0026#34;:1}]},\u0026#34;null_value_counts\u0026#34;:{\u0026#34;array\u0026#34;:[{\u0026#34;key\u0026#34;:1,\u0026#34;value\u0026#34;:0},{\u0026#34;key\u0026#34;:2,\u0026#34;value\u0026#34;:0},{\u0026#34;key\u0026#34;:3,\u0026#34;value\u0026#34;:0}]},\u0026#34;nan_value_counts\u0026#34;:{\u0026#34;array\u0026#34;:[]},\u0026#34;lower_bounds\u0026#34;:{\u0026#34;array\u0026#34;:[{\u0026#34;key\u0026#34;:1,\u0026#34;value\u0026#34;:\u0026#34;\\u0003\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34;},{\u0026#34;key\u0026#34;:2,\u0026#34;value\u0026#34;:\u0026#34;c\u0026#34;},{\u0026#34;key\u0026#34;:3,\u0026#34;value\u0026#34;:\u0026#34;now\u0026#34;}]},\u0026#34;upper_bounds\u0026#34;:{\u0026#34;array\u0026#34;:[{\u0026#34;key\u0026#34;:1,\u0026#34;value\u0026#34;:\u0026#34;\\u0003\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34;},{\u0026#34;key\u0026#34;:2,\u0026#34;value\u0026#34;:\u0026#34;c\u0026#34;},{\u0026#34;key\u0026#34;:3,\u0026#34;value\u0026#34;:\u0026#34;now\u0026#34;}]},\u0026#34;key_metadata\u0026#34;:null,\u0026#34;split_offsets\u0026#34;:null,\u0026#34;sort_order_id\u0026#34;:{\u0026#34;int\u0026#34;:0}}} {\u0026#34;status\u0026#34;:1,\u0026#34;snapshot_id\u0026#34;:null,\u0026#34;data_file\u0026#34;:{\u0026#34;file_path\u0026#34;:\u0026#34;hdfs://localhost:9000/user/hive/warehouse/test_add_files_src/data/category=test/00000-3-9b80e070-0c5a-4b73-a874-b3bead890332-0-00001.parquet\u0026#34;,\u0026#34;file_format\u0026#34;:\u0026#34;PARQUET\u0026#34;,\u0026#34;partition\u0026#34;:{\u0026#34;category\u0026#34;:{\u0026#34;string\u0026#34;:\u0026#34;test\u0026#34;}},\u0026#34;record_count\u0026#34;:2,\u0026#34;file_size_in_bytes\u0026#34;:915,\u0026#34;block_size_in_bytes\u0026#34;:67108864,\u0026#34;column_sizes\u0026#34;:{\u0026#34;array\u0026#34;:[{\u0026#34;key\u0026#34;:1,\u0026#34;value\u0026#34;:48},{\u0026#34;key\u0026#34;:2,\u0026#34;value\u0026#34;:42},{\u0026#34;key\u0026#34;:3,\u0026#34;value\u0026#34;:70}]},\u0026#34;value_counts\u0026#34;:{\u0026#34;array\u0026#34;:[{\u0026#34;key\u0026#34;:1,\u0026#34;value\u0026#34;:2},{\u0026#34;key\u0026#34;:2,\u0026#34;value\u0026#34;:2},{\u0026#34;key\u0026#34;:3,\u0026#34;value\u0026#34;:2}]},\u0026#34;null_value_counts\u0026#34;:{\u0026#34;array\u0026#34;:[{\u0026#34;key\u0026#34;:1,\u0026#34;value\u0026#34;:0},{\u0026#34;key\u0026#34;:2,\u0026#34;value\u0026#34;:0},{\u0026#34;key\u0026#34;:3,\u0026#34;value\u0026#34;:0}]},\u0026#34;nan_value_counts\u0026#34;:{\u0026#34;array\u0026#34;:[]},\u0026#34;lower_bounds\u0026#34;:{\u0026#34;array\u0026#34;:[{\u0026#34;key\u0026#34;:1,\u0026#34;value\u0026#34;:\u0026#34;\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34;},{\u0026#34;key\u0026#34;:2,\u0026#34;value\u0026#34;:\u0026#34;a\u0026#34;},{\u0026#34;key\u0026#34;:3,\u0026#34;value\u0026#34;:\u0026#34;test\u0026#34;}]},\u0026#34;upper_bounds\u0026#34;:{\u0026#34;array\u0026#34;:[{\u0026#34;key\u0026#34;:1,\u0026#34;value\u0026#34;:\u0026#34;\\u0002\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34;},{\u0026#34;key\u0026#34;:2,\u0026#34;value\u0026#34;:\u0026#34;b\u0026#34;},{\u0026#34;key\u0026#34;:3,\u0026#34;value\u0026#34;:\u0026#34;test\u0026#34;}]},\u0026#34;key_metadata\u0026#34;:null,\u0026#34;split_offsets\u0026#34;:null,\u0026#34;sort_order_id\u0026#34;:{\u0026#34;int\u0026#34;:0}}} 通过查看可以明确的看到，正式表的清单文件是指向的镜像表的data目录的数据文件\n五、验证场景二（移动data数据文件并add_files方式） 启动SparkSQL 1 2 3 4 5 6 7 spark-sql --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\ --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \\ --conf spark.sql.catalog.spark_catalog.type=hive \\ --conf spark.sql.catalog.hive=org.apache.iceberg.spark.SparkCatalog \\ --conf spark.sql.catalog.hive.type=hive \\ --conf spark.sql.catalog.hive.uri=thrift://127.0.0.1:9083 \\ --conf spark.sql.catalog.hive.warehouse=hdfs://warehouse 创建镜像表A并写入数据 1 2 3 4 5 6 7 8 9 use hive_prod.default; CREATE TABLE test_add_files_move_src (id bigint,data string,category string) USING iceberg PARTITIONED BY (category); INSERT INTO test_add_files_move_src VALUES (1, \u0026#39;a\u0026#39;,\u0026#39;test\u0026#39;), (2, \u0026#39;b\u0026#39;,\u0026#39;test\u0026#39;), (3, \u0026#39;c\u0026#39;,\u0026#39;now\u0026#39;), (4, \u0026#39;d\u0026#39;,\u0026#39;last\u0026#39;), (5, \u0026#39;e\u0026#39;,\u0026#39;now\u0026#39;); desc formatted test_add_files_move_src; 查看镜像表A的结构信息以及HDFS布局信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 id bigint data string category string # Partition Information # col_name data_type comment category string # Metadata Columns _spec_id int _partition struct\u0026lt;category:string\u0026gt; _file string _pos bigint _deleted boolean # Detailed Table Information Name hive_prod.default.test_add_files_move_src Type MANAGED Location hdfs://localhost:9000/user/hive/warehouse/test_add_files_move_src Provider iceberg Owner lin Table Properties [current-snapshot-id=8190726940251932496,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd] Time taken: 0.221 seconds, Fetched 21 row(s) ├── test_add_files_move_src │ ├── data │ │ ├── category=last │ │ │ └── 00000-5-55be80b4-c335-4eb6-a014-b011fdae1787-0-00003.parquet │ │ ├── category=now │ │ │ └── 00000-5-55be80b4-c335-4eb6-a014-b011fdae1787-0-00002.parquet │ │ └── category=test │ │ └── 00000-5-55be80b4-c335-4eb6-a014-b011fdae1787-0-00001.parquet │ └── metadata │ ├── 00000-5ebb08d6-400c-4c5c-8459-e894a4e2223f.metadata.json │ ├── 00001-a81bf99d-9a3c-493d-8300-f7ac664b396f.metadata.json │ ├── 6ef03522-ea3c-4019-a6d2-31356b523308-m0.avro │ └── snap-8190726940251932496-1-6ef03522-ea3c-4019-a6d2-31356b523308.avro 创建正式表B 1 2 3 4 5 CREATE TABLE test_add_files_move_target (id bigint,data string,category string) USING iceberg PARTITIONED BY (category); desc formatted test_add_files_move_target; 查看表B的结构信息以及HDFS布局(迁移前) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 id bigint data string category string # Partition Information # col_name data_type comment category string # Metadata Columns _spec_id int _partition struct\u0026lt;category:string\u0026gt; _file string _pos bigint _deleted boolean # Detailed Table Information Name hive_prod.default.test_add_files_move_target Type MANAGED Location hdfs://localhost:9000/user/hive/warehouse/test_add_files_move_target Provider iceberg Owner lin Table Properties [current-snapshot-id=none,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd] Time taken: 0.102 seconds, Fetched 21 row(s) ├── test_add_files_move_target │ └── metadata │ └── 00000-aee35392-0abe-4dfe-b4d1-929ca6a429ad.metadata.json 移动镜像表A的数据文件到正式表B的目录下 1 hadoop fs -mv /user/hive/warehouse/test_add_files_move_src/data /user/hive/warehouse/test_add_files_move_target/ 移动后的目录结构如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ├── test_add_files_move_src │ └── metadata │ ├── 00000-5ebb08d6-400c-4c5c-8459-e894a4e2223f.metadata.json │ ├── 00001-a81bf99d-9a3c-493d-8300-f7ac664b396f.metadata.json │ ├── 6ef03522-ea3c-4019-a6d2-31356b523308-m0.avro │ └── snap-8190726940251932496-1-6ef03522-ea3c-4019-a6d2-31356b523308.avro ├── test_add_files_move_target │ ├── data │ │ ├── category=last │ │ │ └── 00000-5-55be80b4-c335-4eb6-a014-b011fdae1787-0-00003.parquet │ │ ├── category=now │ │ │ └── 00000-5-55be80b4-c335-4eb6-a014-b011fdae1787-0-00002.parquet │ │ └── category=test │ │ └── 00000-5-55be80b4-c335-4eb6-a014-b011fdae1787-0-00001.parquet │ └── metadata │ └── 00000-aee35392-0abe-4dfe-b4d1-929ca6a429ad.metadata.json 迁移前验证(查表操作) 查询test_add_files_move_target表未显示数据，符合预期\n查询test_add_files_move_src表报错，符合预期\n执行迁移存储过程add_files 1 2 3 4 call spark_catalog.system.add_files( table =\u0026gt; \u0026#39;default.test_add_files_move_target\u0026#39;, source_table =\u0026gt; \u0026#39;`parquet`.`hdfs://localhost:9000/user/hive/warehouse/test_add_files_move_target/data`\u0026#39; ); 验证数据 1 select * from test_add_files_move_target; 通过查询可看到数据已经成功迁移到正式表B\n分析迁移后的两张表结构信息 1 2 desc formatted test_add_files_move_src; desc formatted test_add_files_move_target; 镜像表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 spark-sql (default)\u0026gt; desc formatted test_add_files_move_src; id bigint data string category string # Partition Information # col_name data_type comment category string # Metadata Columns _spec_id int _partition struct\u0026lt;category:string\u0026gt; _file string _pos bigint _deleted boolean # Detailed Table Information Name hive_prod.default.test_add_files_move_src Type MANAGED Location hdfs://localhost:9000/user/hive/warehouse/test_add_files_move_src Provider iceberg Owner lin Table Properties [current-snapshot-id=8190726940251932496,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd] Time taken: 0.165 seconds, Fetched 21 row(s) 正式表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 spark-sql (default)\u0026gt; desc formatted test_add_files_move_target; id bigint data string category string # Partition Information # col_name data_type comment category string # Metadata Columns _spec_id int _partition struct\u0026lt;category:string\u0026gt; _file string _pos bigint _deleted boolean # Detailed Table Information Name hive_prod.default.test_add_files_move_target Type MANAGED Location hdfs://localhost:9000/user/hive/warehouse/test_add_files_move_target Provider iceberg Owner lin Table Properties [current-snapshot-id=6170566202764885842,format=iceberg/parquet,format-version=2,schema.name-mapping.default=[ { \u0026#34;field-id\u0026#34; : 1, \u0026#34;names\u0026#34; : [ \u0026#34;id\u0026#34; ] }, { \u0026#34;field-id\u0026#34; : 2, \u0026#34;names\u0026#34; : [ \u0026#34;data\u0026#34; ] }, { \u0026#34;field-id\u0026#34; : 3, \u0026#34;names\u0026#34; : [ \u0026#34;category\u0026#34; ] } ],write.parquet.compression-codec=zstd] Time taken: 0.169 seconds, Fetched 21 row(s) 检验正式表数据文件时间戳 可以看到镜像表的数据文件没有被移动且也没有被修改过\n分析Iceberg正式表的清单数据 分析Iceberg正式表的stage数据 六、验证场景三（Iceberg分支方式） 待验证\n七、结论 通过验证发现通过Iceberg原生提供的add_files存储过程能够完成数据迁移的逻辑，底层是给正式表创建元数据文件，并通过元数据文件维护表数据的地址，地址指向的还是镜像表的地址。因此这个过程速度会比较快并且不会造成额外的资源损耗(计算资源以及存储资源)。 额外思考 通过add_files方式，底层相当于正式表的清单文件指向镜像表data目录下的数据文件，\n在完成迁移动作后此处是否增加后期的维护成本？ 如果迁移后的数据存在数据变更，那么Iceberg是在正式表目录下还是镜像表目录下新增数据文件？ 结合上述两个疑问，是否可以考虑在镜像表的质量验收通过之后，通过hdfs mv指令将镜像表下的数据移动到正式表的data目录下，再通过add_files进行加载\n","date":"2025-02-21T11:01:17+08:00","image":"https://sherlock-lin.github.io/p/iceberg%E9%95%9C%E5%83%8F%E8%A1%A8%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88%E9%AA%8C%E8%AF%81/book-8643905_1280_hu13968723467129298319.jpg","permalink":"https://sherlock-lin.github.io/p/iceberg%E9%95%9C%E5%83%8F%E8%A1%A8%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88%E9%AA%8C%E8%AF%81/","title":"Iceberg镜像表数据迁移方案验证"},{"content":"概述 shuffle就是将一组无规则的数据重组成具有一定规则的数据，在分布式计算场景下常常会通过shuffle将计算流程划分为map和reduce阶段，在reduce阶段前计算框架会将数据进行shuffle将具有相同规则的数据分发到同一个reduce节点来进行聚合计算，这个过程往往会伴随大量的磁盘和网络IO，因此在分布式计算链路中，shuffle环节的执行性能是最差的。\n在spark中，每执行一个action算子都会创建一个对应的Job任务，在对这个Job进行DAG调度的过程中，会判断算子之间是否存在ShuffleDependency 宽依赖，最终每个Job提交后都会生成一个ResultStage和若干个ShuffleMapStage，ResultStage表示这个Stage输出最终的结果，然后spark会通过shuffle为边界将Job划分为多个Stage。\n一张图了解下 Spark Shuffle 的迭代历史：\n简单来说Spark Shuffle 分为两种：一种是基于 Hash 的 Shuffle；另一种是基于 Sort 的 Shuffle。\nSpark初始版本(Hash)\n背景：MapReduce将sort作为shuffle的固定步骤，有许多并不需要排序的任务，MapReduce也会对其进行排序，造成不必要的开销。\n设计思路：在基于 Hash 的 Shuffle 实现方式中，每个 Mapper 阶段的 Task 会为每个 Reduce 阶段的 Task 生成一个文件，通常会产生大量的文件（即对应为 M*R 个中间文件，其中， M 表示 Mapper 阶段的 Task 个数， R 表示 Reduce 阶段的 Task 个数） 伴随大量的随机磁盘 I/O 操作与大量的内存开销。\n缺点：会导致大量的随机磁盘IO和大量的内存开销\nSpark0.8.1版本(文件合并机制)\n背景：解决Hash shuffle大量随机磁盘IO的问题\n设计：通过**Shuffle Consolidate 机制（即文件合并机制）**将Mapper端生成的中间文件进行合并处理，在配置属性 spark.shuffie.consolidateFiles=true后，Spark会将中间文件的生成方式修改为每个执行单位为每个 Reduce 阶段的 Task 生成一个文件。\n计算公式：执行单位对应为：每个 Mapper 端的 Cores 数／每个 Task 分配的 Cores 数（默认为 1) 。最终可以将文件个数从 MR 修改为 EC/T*R，其中， E 表示 Executors 个数， C 表示可用 Cores 个数， T 表示 Task 分配的 Cores 数。\n缺点：基于 Hash 的 Shuffle 的实现方式中，生成的中间结果文件的个数都会依赖于 Reduce 阶段的 Task 个数，即 Reduce 端的并行度，因此文件数仍然不可控，无法真正解决问题\nSpark1.1版本(Sort Shuffle)\n背景：解决Hash Shuffle中间文件不可控的问题\n设计：每个 Mapper 阶段的 Task 不会为每 Reduce 阶段的 Task 生成一个单独的文件，而是全部写到一个数据（Data）文件中，同时生成一个索引（Index）文件， Reduce 阶段的各个 Task 可以通过该索引文件获取相关的数据。避免产生大量文件的直接收益就是降低随机磁盘 I/0 与内存的开销。最终生成的文件个数减少到 2M ，其中 M 表示 Mapper 阶段的 Task 个数，每个 Mapper 阶段的 Task 分别生成两个文件（1 个数据文件、 1 个索引文件），最终的文件个数为 M 个数据文件与 M 个索引文件。因此，最终文件个数是 2M 个。\nSpark1.4版本(Tungsten-Sort)\n背景：Sorted-Based Shuffle 也有缺点，其缺点反而是它排序的特性，它强制要求数据在 Mapper 端必须先进行排序，所以导致它排序的速度有点慢。\n设计： Tungsten-Sort Shuffle 对排序算法进行了改进，优化了排序的速度。Tungsten-Sort Shuffle 已经并入了 Sorted-Based Shuffle，Spark 的引擎会自动识别程序需要的是 Sorted-Based Shuffle，还是 Tungsten-Sort Shuffle。\nHash Shuffle 解析 shuffle write 阶段，主要就是在一个 stage 结束计算之后，为了下一个 stage 可以执行 shuffle 类的算子（比如 reduceByKey），而将每个 task 处理的数据按 key 进行“划分”。所谓“划分”，就是对相同的 key 执行 hash 算法，从而将相同 key 都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游 stage 的一个 task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。\n下一个 stage 的 task 有多少个，当前 stage 的每个 task 就要创建多少份磁盘文件。比如下一个 stage 总共有 100 个 task，那么当前 stage 的每个 task 都要创建 100 份磁盘文件。如果当前 stage 有 50 个 task，总共有 10 个 Executor，每个 Executor 执行 5 个 task，那么每个 Executor 上总共就要创建 500 个磁盘文件，所有 Executor 上会创建 5000 个磁盘文件。由此可见，未经优化的 shuffle write 操作所产生的磁盘文件的数量是极其惊人的。\nshuffle read 阶段，通常就是一个 stage 刚开始时要做的事情。此时该 stage 的每一个 task 就需要将上一个 stage 的计算结果中的所有相同 key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行 key 的聚合或连接等操作。由于 shuffle write 的过程中，map task 给下游 stage 的每个 reduce task 都创建了一个磁盘文件，因此 shuffle read 的过程中，每个 reduce task 只要从上游 stage 的所有 map task 所在节点上，拉取属于自己的那一个磁盘文件即可。\nshuffle read 的拉取过程是一边拉取一边进行聚合的。每个 shuffle read task 都会有一个自己的 buffer 缓冲，每次都只能拉取与 buffer 缓冲相同大小的数据，然后通过内存中的一个 Map 进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到 buffer 缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。\nHashShuffleManager 工作原理如下图所示：\nHash Shuffle 解析(合并优化版本) 为了优化 HashShuffleManager 我们可以设置一个参数：spark.shuffle.consolidateFiles，该参数默认值为 false，将其设置为 true 即可开启优化机制，通常来说，如果我们使用 HashShuffleManager，那么都建议开启这个选项。\n开启 consolidate 机制之后，在 shuffle write 过程中，task 就不是为下游 stage 的每个 task 创建一个磁盘文件了，此时会出现shuffleFileGroup的概念，每个 shuffleFileGroup 会对应一批磁盘文件，磁盘文件的数量与下游 stage 的 task 数量是相同的。一个 Executor 上有多少个 cpu core，就可以并行执行多少个 task。而第一批并行执行的每个 task 都会创建一个 shuffleFileGroup，并将数据写入对应的磁盘文件内。\n当 Executor 的 cpu core 执行完一批 task，接着执行下一批 task 时，下一批 task 就会复用之前已有的 shuffleFileGroup，包括其中的磁盘文件，也就是说，此时 task 会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate 机制允许不同的 task 复用同一批磁盘文件，这样就可以有效将多个 task 的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升 shuffle write 的性能。\n假设第二个 stage 有 100 个 task，第一个 stage 有 50 个 task，总共还是有 10 个 Executor（Executor CPU 个数为 1），每个 Executor 执行 5 个 task。那么原本使用未经优化的 HashShuffleManager 时，每个 Executor 会产生 500 个磁盘文件，所有 Executor 会产生 5000 个磁盘文件的。但是此时经过优化之后，每个 Executor 创建的磁盘文件的数量的计算公式为：cpu core的数量 * 下一个stage的task数量，也就是说，每个 Executor 此时只会创建 100 个磁盘文件，所有 Executor 只会创建 1000 个磁盘文件。\n这个功能优点明显，但为什么 Spark 一直没有在基于 Hash Shuffle 的实现中将功能设置为默认选项呢，官方给出的说法是这个功能还欠稳定。\n优化后的 HashShuffleManager 工作原理如下图所示：\n基于 Hash 的 Shuffle 机制的优缺点\n优点：\n可以省略不必要的排序开销。 避免了排序所需的内存开销。 缺点：\n生产的文件过多，会对文件系统造成压力。 大量小文件的随机读写带来一定的磁盘开销。 数据块写入时所需的缓存空间也会随之增加，对内存造成压力。 SortShuffle 解析 SortShuffleManager 的运行机制主要分成三种：\n普通运行机制； bypass 运行机制，当 shuffle read task 的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为 200），就会启用 bypass 机制； Tungsten Sort 运行机制，开启此运行机制需设置配置项 spark.shuffle.manager=tungsten-sort。开启此项配置也不能保证就一定采用此运行机制（后面会解释）。 普通运行机制 在该模式下，数据会先写入一个内存数据结构中，此时根据不同的 shuffle 算子，可能选用不同的数据结构。如果是 reduceByKey 这种聚合类的 shuffle 算子，那么会选用 Map 数据结构，一边通过 Map 进行聚合，一边写入内存；如果是 join 这种普通的 shuffle 算子，那么会选用 Array 数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。\n在溢写到磁盘文件之前，会先根据 key 对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的 batch 数量是 10000 条，也就是说，排序好的数据，会以每批 1 万条数据的形式分批写入磁盘文件。写入磁盘文件是通过 Java 的 BufferedOutputStream 实现的。BufferedOutputStream 是 Java 的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘 IO 次数，提升性能。\n一个 task 将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge 过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个 task 就只对应一个磁盘文件，也就意味着该 task 为下游 stage 的 task 准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个 task 的数据在文件中的 start offset 与 end offset。\nSortShuffleManager 由于有一个磁盘文件 merge 的过程，因此大大减少了文件数量。比如第一个 stage 有 50 个 task，总共有 10 个 Executor，每个 Executor 执行 5 个 task，而第二个 stage 有 100 个 task。由于每个 task 最终只有一个磁盘文件，因此此时每个 Executor 上只有 5 个磁盘文件，所有 Executor 只有 50 个磁盘文件。\n普通运行机制的 SortShuffleManager 工作原理如下图所示：\nbypass 运行机制 Reducer 端任务数比较少的情况下，基于 Hash Shuffle 实现机制明显比基于 Sort Shuffle 实现机制要快，因此基于 Sort Shuffle 实现机制提供了一个带 Hash 风格的回退方案，就是 bypass 运行机制。对于 Reducer 端任务数少于配置属性spark.shuffle.sort.bypassMergeThreshold设置的个数时，使用带 Hash 风格的回退计划。\nbypass 运行机制的触发条件如下：\nshuffle map task 数量小于spark.shuffle.sort.bypassMergeThreshold=200参数的值。 不是聚合类的 shuffle 算子。 此时，每个 task 会为每个下游 task 都创建一个临时磁盘文件，并将数据按 key 进行 hash 然后根据 key 的 hash 值，将 key 写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。\n该过程的磁盘写机制其实跟未经优化的 HashShuffleManager 是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的 HashShuffleManager 来说，shuffle read 的性能会更好。\n而该机制与普通 SortShuffleManager 运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write 过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。\nbypass 运行机制的 SortShuffleManager 工作原理如下图所示：\nTungsten Sort Shuffle 运行机制 Tungsten Sort 是对普通 Sort 的一种优化，Tungsten Sort 会进行排序，但排序的不是内容本身，而是内容序列化后字节数组的指针(元数据)，把数据的排序转变为了指针数组的排序，实现了直接对序列化后的二进制数据进行排序。由于直接基于二进制数据进行操作，所以在这里面没有序列化和反序列化的过程。内存的消耗大大降低，相应的，会极大的减少的 GC 的开销。\nSpark 提供了配置属性，用于选择具体的 Shuffle 实现机制，但需要说明的是，虽然默认情况下 Spark 默认开启的是基于 SortShuffle 实现机制，但实际上，参考 Shuffle 的框架内核部分可知基于 SortShuffle 的实现机制与基于 Tungsten Sort Shuffle 实现机制都是使用 SortShuffleManager，而内部使用的具体的实现机制，是通过提供的两个方法进行判断的：\n对应非基于 Tungsten Sort 时，通过 SortShuffleWriter.shouldBypassMergeSort 方法判断是否需要回退到 Hash 风格的 Shuffle 实现机制，当该方法返回的条件不满足时，则通过 SortShuffleManager.canUseSerializedShuffle 方法判断是否需要采用基于 Tungsten Sort Shuffle 实现机制，而当这两个方法返回都为 false，即都不满足对应的条件时，会自动采用普通运行机制。\n因此，当设置了 spark.shuffle.manager=tungsten-sort 时，也不能保证就一定采用基于 Tungsten Sort 的 Shuffle 实现机制。\n要实现 Tungsten Sort Shuffle 机制需要满足以下条件：\nShuffle 依赖中不带聚合操作或没有对输出进行排序的要求。 Shuffle 的序列化器支持序列化值的重定位（当前仅支持 KryoSerializer Spark SQL 框架自定义的序列化器）。 Shuffle 过程中的输出分区个数少于 16777216 个。 实际上，使用过程中还有其他一些限制，如引入 Page 形式的内存管理模型后，内部单条记录的长度不能超过 128 MB （具体内存模型可以参考 PackedRecordPointer 类）。另外，分区个数的限制也是该内存模型导致的。\n所以，目前使用基于 Tungsten Sort Shuffle 实现机制条件还是比较苛刻的。\n","date":"2025-02-18T11:20:20+08:00","image":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3spark%E7%9A%84shuffle%E6%BC%94%E5%8F%98/lizard-8007238_1280_hu7957813414589977256.jpg","permalink":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3spark%E7%9A%84shuffle%E6%BC%94%E5%8F%98/","title":"详解Spark的shuffle演变"},{"content":"概述 Spark算子根据使用方式可以整体成如下\nTransformations\n数据转换\nmap\n最常用的算子没有之一，对传进来的数据进行处理转换，属于进一个元素处理一个，处理完后将这个元素进行输出\nmapPartitions/mapPartitionsWithIndex\nmap算子的拓展补充，跟map一样是对数据进行转换处理，例如在map处理逻辑中依赖一些外部资源如mysql连接场景，如果是在map中进行连接创建的话，会严重影响执行效率，而mapPartitions支持先遍历Partitions，基于Partition级别初始化依赖的资源，再进行对数据map转换处理，属于是对map算子中的共享变量优化的补充算子；mapPartitionsWithIndex相比mapPartitions来说仅仅多出一个数据分区索引，也就是分区编号，如果在业务逻辑中需要用到这个编号可以考虑使用mapPartitionsWithIndex算子。\nflatMap\n用于对数据进行扁平化处理，也就是原来集合中的每一条数据再次打散成多条数据，方面后面的聚合计算。简单的说，map是（元素） =\u0026gt; （元素），而flatMap是（元素） =\u0026gt; （集合）。\nfilter\n对数据进行过滤，仅保留计算结果为true的数据\n数据聚合\ngroupByKey\n以Key为单位，分组收集同一Key的Value列表\nsortByKey\n以Key为单位，对数据集进行排序\nreduceByKey\n给定reduce函数，以Key为单位，分组聚合同一个Key对应的Value列表，常见的如请求平均值、求和等\naggregateByKey\naggregateByKey相比reduceByKey有初始值，也是以Key为单位，进行分组聚合计算\n数据整合\nunion\n对类型完全一致的RDD做合并，不会去重\nintersection\n计算两个RDD的交集\njoin\n跟mysql的表join一样，用来做联表查询。union只能产生窄依赖，join既能产生窄依赖，也能产生宽依赖\ncogroup\n对具有相同键的元素进行联表查询。\ncartesian\n对两个RDD做笛卡尔积操作\n数据整理\nsample\n给定采样率，对RDD数据集做统计抽样\ndistinct\n对结果进行去重操作\n数据分布\nrepartition\n给定目标并行度N，重新调整RDD并行度\ncoalesce\n给定目标并行度，降低RDD并行度，coalesce底层也是调用的repartition算子来实现的\nActions\n数据收集\ncollect\n将RDD数据全量收集到Driver端上，一般用于遍历RDD中的每条数据\nfirst\n随机提取RDD数据集中任意一条记录\ntake\n随机提取RDD数据集中任意N条记录\ncount\n对数据条数进行统计\n数据持久化\nsaveAsTextFile\n直接通过Executors将RDD数据集物化到文件系统\nsaveAsSequenceFile/saveAsObjectFile\n跟saveAsTextFile一样，只是物化到文件系统的数据格式有些差异\n数据遍历\nforeach\n遍历RDD中的每条数据\n实战 1. map 最常用的算子没有之一，对传进来的数据进行处理转换，属于进一个元素处理一个，处理完后将这个元素进行输出。\n输出如下，可以看到所有数据都已经按照预期的乘于3倍\n2. flatMap 用于对数据进行扁平化处理，也就是原来集合中的每一条数据再次打散成多条数据，方面后面的聚合计算。简单的说，map是（元素） =\u0026gt; （元素），而flatMap是（元素） =\u0026gt; （集合）。\n输出如下，原先的(1,2)元素被取出来转换成一个集合的形式，并针对集合中每个元素乘与5\n3. filter 对数据进行过滤，仅保留计算结果为true的数据\n输出如下，可以看到根据过滤逻辑仅保留奇数下来\n4. groupBy 以Key为单位，分组收集同一Key的Value列表\n输出如下\nsortByKey 以Key为单位，对数据集进行排序\n输出如下\nreduceByKey 给定reduce函数，以Key为单位，分组聚合同一个Key对应的Value列表，常见的如请求平均值、求和等\n输出如下\ndistinct 对结果进行去重操作\n输出如下\n","date":"2025-01-06T19:50:12+08:00","image":"https://sherlock-lin.github.io/p/spark%E5%AE%9E%E6%88%9802_rdd%E7%AE%97%E5%AD%90%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/gull-8151932_1280_hu14164449129448124008.jpg","permalink":"https://sherlock-lin.github.io/p/spark%E5%AE%9E%E6%88%9802_rdd%E7%AE%97%E5%AD%90%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","title":"Spark实战02_RDD算子基本操作"},{"content":"一、引言 任何领域的技能都分为“道术器”三个层面\n1. 器 器对应的就是具体的工具如大数据组件Spark、Flink、Hadoop、MySQL，也可以是一门编程语言如Java、Go、Shell，甚至我们写的一个脚本、小程序都是工具，简单来说就是一切具像化、可直接使用的具体个体，并且能够发挥作用的东西都可以统称为器。\n2. 术 术对应的是技巧，这个技巧包括使用工具的技巧以及设计的技巧等，例如用Spark时在联小表join的时候我们可以考虑将小表进行广播来避免shuffle操作、在庞大数据量场景下可以考虑Hadoop这类能横向扩展磁盘的组件、在大数据场景下对分析性能有严格要求的场景可以考虑用Doris、CK等olap引擎等等，这类技巧可以称之为术。\n3. 道 道对应的是规律，如这个世界的日升日落、潮涨潮落、沧海桑田的规律一般，软件也是有自己的一套规律，或者说底层逻辑。当你遵循这套规律，你设计出来的东西能够放大收益，例如性能更高、资源更加节约、服务更加稳定等等。这也是今天想聊聊的软件设计之道。\n小结 从人类历史发展的宏观角度看过去、现在以及未来，有一个词贯穿着整个人类文明，那就是“效率”。即便是在软件设计包括大数据时代这个词也是核心，简单来说就是如何用更少的资源做成更多的事情，包括但不仅限于用有限的CPU、内存跑更多的任务、相同的计算存储资源能够比别人计算以及分析得更快、存储同样数量级的数据只需更少的资源等等。那么围绕“效率”这个核心来聊聊软件设计的道。\n二、概述 如今的组件层出不穷，新技术更是日新月异，会让不少入门不久的程序员觉得遥遥无期，但实际上无论是多牛X的软件技术，最终都是要运行在操作系统上面或者说最终都是要借助操作系统的CPU、内存、磁盘、网络来实现它的功能，那么无论是多好的设计，只要映射到跟操作系统的交互并抽象出来，就会发现很多软件解决方案就是自相似的，正所谓万变不离其宗，只要将这抽象出来的解决方案掌握，后续即便有再新的技术也仅需要往上靠就好了。\n以下是结合自身的知识栈列举出的软件设计之道\n内存替代磁盘/顺序写代替随机写\nkafka pagecache mysql InnoDB内存池 WAL LSM 移动少量数据替代移动大量数据\nMR、Spark、Flink本地化计算 广播变量 数据湖 资源轻量级使用\n协程 复用-池化技术(内存池、线程池、对象池、连接池、进程标识符) 进程是CPU的抽象，进程被CPU调用本质上是一种闭包的实现 SLAB mmap 零拷贝(分为追尾读和追赶读两种场景) 三、内存替代磁盘、顺序写代替随机写 1. 简析 软件计算的对象只有一个，就是数据，而数据是要有一个载体，常见的载体有内存、磁盘，它们的读写性能比例在下面这张图可看出来(仅供参考)\n从这张图我们能够得到以下几个有用的信息\n顺序读写内存的性能是最高的 顺序读写磁盘跟随机读写内存的性能几乎一致 随机读写磁盘性能最差，顺序读写磁盘性能是随机读写磁盘的千倍以上。相当于顺序往磁盘里写一千条数据等同于随机往磁盘里写一条数据，这个性能差距可想而知。 随机读写磁盘性能差原因原因：是因为机械硬盘采用传统的磁头探针结构，随机读写时需要频繁寻道，也就需要磁头和探针频繁的转动，而机械结构的磁头和探针的位置调整是十分费时的，这就严重影响到硬盘的寻址速度，进而影响到随机写入速度。\n2. Kafka PageCache 设计思想：内存替代磁盘\n由于读写硬盘的速度比读写内存要慢很多，所以为了避免每次读写文件时，都需要对硬盘进行读写操作，Linux 内核使用 页缓存（Page Cache） 机制来对文件中的数据进行缓存。而Kafka作为一个高性能、高吞吐的流平台，如果每条消息都要进行落盘操作，那就会严重受限于读写磁盘的性能，因此Kafka仅会将数据写到PageCache，然后就给生产者返回写入成功的状态，而稳定性是通过多副本机制来进行保证的。\n而PageCache的落盘时机通过Kafka的配置进行控制。\n1 2 3 log.flush.interval.messages //多少条消息，刷盘1次 默认值 LONG.MAX_VALUE log.flush.interval.ms //割多长时间，刷盘1次 LONG.MAX_VALUE log.flush.scheduler.interval.ms //周期性的刷盘，缺省3000，即3s。 3. MySQL InnoDB缓冲池 设计思想：内存代替磁盘。\nMysql的InnoDB引擎是基于磁盘存储数据的，并将其中的数据按照页的方式进行管理。而由于CPU速度与磁盘速度之间的鸿沟，InnoDB使用缓冲池技术来提高数据库的整体性能(不用操作系统提供的PageCache机制)。缓冲池简单来说就是一块内存区域，通过内存的速度来弥补磁盘速度较慢对数据库性能的影响。\n缓冲池中存储以下数据\ndata page insert buffer lock info index page 自适应哈希 数据字典信息 在读取数据时会先去查缓冲池，如果命中则读取该页，否则去磁盘读取该页并放到缓冲池中。而在写数据时，会修改缓冲池中的页，然后再以一定的频率刷新到磁盘上，从而大幅提升MySQL的读写性能。\n4. WAL 设计思想：批量顺序写磁盘代替随机写磁盘\n内存取代磁盘能够大幅提升性能，因此大部分存储相关的大数据组件都会选择将数据落到内存即可，但内存是没法提供持久化保证的，如果在数据落盘之前服务故障或者关机就会导致这部分数据丢失。针对这种问题一种方式是像kafka一样通过多副本机制+一致性算法来提供保证；另一种方式就是WAL方式。\nWAL的核心就是避免内存中的数据丢失，在数据写入时先通过顺序写的方式持久化这部分写请求(用于故障恢复)，然后这部分数据只写到内存并异步的进行刷盘操作，从而避免了随机写磁盘的低效操作。可以发现这个过程WAL有两个特点 1. 通过顺序写磁盘记录变动数据的高效操作保证了写在内存中数据的稳定性 2. 数据写在内存中再定期去刷盘，能够大幅提升刷盘的吞吐量，本质上也是将原先的落盘操作从 随机写磁盘化为顺序写磁盘。\nWAL本质上是通过两次顺序写磁盘取代一次随机写磁盘，它的应用场景的非常广泛，如MySQL、HDFS、HBase、Pulsar等，基本上跟存储相关的组件都会采用WAL的设计思路，生产上有一条通用的落地方案，就是将WAL的数据落到SSD固态硬盘中来提升数据写的性能，因为SSD固态硬盘性能比HDD硬盘高，但价格比内存低很多且能提供持久化能力，因此非常适合挂载到WAL日志持久化的目录。\n5. LSM 设计思想：批量顺序写磁盘代替随机写磁盘\n在大数据写入密集场景，LSM设计结构通过WAL保证数据可恢复，然后并发写到内存中，在达到某些阈值后就会进行刷盘将数据刷到Level0的文件中，每个SSTable中的数据都是局部有序的，在Level0的数据文件变多后，也会进行合并，并将合并后的结果写到Level1层的文件，一层一层这样，避免小文件过多影响读的性能以及后续维护的成本。\n通过上述结果看到数据会先写到内存中缓存一下，那为什么不直接通过顺序写的方式写到磁盘呢，这是因为单条写的性能肯定没有批量写的性能好，即便是像kafka这种也是通过PageCache来进行批量的顺序写的。而且在内存中缓冲一下的好处是，对于追尾读以及一些读取的场景，可以直接查询内存并进行返回，避免去查询磁盘提升查询效率和提升资源利用率，另一个好处是，LSM结构需要在内存中对数据进行排序。\n四、移动少量数据替代移动大量数据 1. 本地化计算 在大数据计算领域，有一句经典的话“移动计算优于移动数据”，因为计算逻辑也是数据所以本质上也是移动少量数据替代移动大量数据，因为在大数据领域，虽然机器能够横向扩展，但是如果移动大量数据的话对网络以及内存的负担会比较大，并且在业务高峰期时还容易影响到其他服务，而大数据计算的核心也是计算，就是通过CPU对数据运行程序逻辑进行处理，那既然在那台机器计算都一样，与其将数据移动到程序所在的机器，将程序移动到数据所在的节点的成本是最低并且对服务来说也是最稳定的。\n应用场景：Spark、Flink、MR、边缘计算等\n2. 广播变量 在大数据计算场景为了在任务之间隔离，基本上每个任务都会用一份全量临时数据，这个出发点是好的，但是在任务较多并且数据量稍微大些的场景，你会发现同一台机器上跑几十、上百甚至上千个任务时，这台机器的内存里会存放同样数量的数据，这无异是一种浪费。因此解决方案就是机器上仅存一份，各个任务共享这份数据仅可，当然这份数据往往是常量，不支持修改，从而大幅提升资源的有效使用率。\n除此之外，在大数据计算场景，常常会出现联表查询的时候，这个时候会容易触发shuffle操作，本质上就是将两张表都重新做打散，相同规则的数据聚合到同一个下游节点做计算，而通过将其中的小表全量广播到计算节点，就能有效避免shuffle，他们数据直接本地化计算即可。\n3. 数据湖 大数据发展到如今，大部分公司都是通过Hive来搭建数仓，但随着数仓的维护Hive的不足也逐渐暴露出来，主要面临以下几个痛点\nHive要通过显示分区来避免全表扫描，因此有时候要维护多层分区，这会导致小文件过多并且分区规则变化时需要将所有数据重新计算重新分区，并且对于使用者来说需要了解表结构并一定要在查询语句中显示的指定分区条件。 数据更新、删除支持较弱，因此Hive更适合做按天级别的更新，如小时分钟级别的更新需要依赖实时链路 数据多版本管理能力较差，一般都是用分区来做的但是非常重量级 不支持ACID Hive的这些场景的核心都是大部分操作都会导致数据的大量计算，本质上就是Hive设计之初对增量场景的能力支持较弱，元数据管理方面没有花太多精力。因此数据湖的出现填补了这方面的不足，例如Hudi、Iceberg等技术，在数据变动时，以增加元数据、数据文件的形式来记录，并通过清单列表文件跟踪变动情况，同时在元数据中记录分区情况以及列的极限值。主要的显著能力有如下\n通过元数据保存分区情况取代Hive的文件目录管理方式，在分区变动时仅需要变更元数据文件即可 有数据更新删除时，通过添加增量数据文件，同时也增加新的元数据文件维护变化的关系 由于清单列表文件中记录变动的情况，这天然的支持数据多版本 由于数据都是增量的并互不干扰，因此在新建过程中也是能很好的做数据原子性提交、回滚 可以看到数据湖每次只需要计算、处理必要的少量数据，而不是像Hive大部分情况下都去全量计算\n五、资源轻量级使用 1. 协程 随着用户量、数据量的暴增，并发处理场景越来越多，像很多编程语言如Java原生只支持多线程，都是通过多个线程并发去处理任务，但这面临以下问题\n线程的本质就是进程，只不过具有共享能力，因此对操作系统的资源占用是比较重的，每台机器能创建的线程数也是比较有限 线程之间的CPU上下文切换，这会导致CPU中寄存器的缓存不断被覆盖导致命中率大幅降低从而影响程序的并发处理性能 因此像协程就派上用场了，核心思路是既然之前每一个并发逻辑都独占一个线程，那么就让这些并发逻辑共享一个线程或者少量几个线程，从而从根本上解决线程资源占用以及高频上下文切换的问题。而协程的设计可以是语言原生支持的如Go语言，也可以是第三方库支持的如Java的Quasar等，但不变的是，它要解决时间片分配以及调度这些并发逻辑的事情。相当于在线程内部跑一套小型的CPU调度算法逻辑，除此之外，由于协程相比操作系统来说处于更上层，它能获得更多并发逻辑相关的数据来做更多的调度、计算的优化。因此在一些对资源管控比较严格或者在一些高并发的场景，采用协程的设计是要远远优于基于线程。\n2. 池化技术 完成一次请求处理，一般需要经历以下几个阶段\n客户端跟服务端建立TCP连接 服务端处理线程创建子线程去并行处理这个任务 子线程处理这个任务是需要创建一些对象去接受数据、处理逻辑 处理任务时会涉及从磁盘读取数据 可以看到处理一次请求的代价是庞大的，对网络、CPU、内存、磁盘都有不同程度的损耗，那在高并发场景这无疑是致命的，为了降低单次请求的成本，我们可以通过复用技术也就是池化能力将上述流程进行优化\n连接池：客户端提前跟服务端建立多个连接并放到池子里，需要发送请求时从池子里拿一个连接进行数据发送即可 线程池：服务端提前创建好具有多个子线程的线程池，在请求到达服务端时，直接从线程池里取一个线程去处理即可 对象池：对象池是一种特殊的工厂对象，具体实现就是连接池、线程池以及一些常用到的重量级的对象，可以预先创建好并放到池子里，避免处理任务时频繁去创建和释放 内存池：将会频繁读取的数据以页的形式存储在内存池中，相当于从原先的基于磁盘读写数据优化成了基于内存读写的效果 除了这些以外，像Linux还会通过复用进程标识符也就是进程ID来加速进程/线程的创建。\n3. SLAB 进程/线程处理任务时，经常会需要频繁跟Linux申请内存，而由于内存是全局共享的，分配内存和释放内存都需要加锁操作，因此如果高频申请内存会影响服务性能。因此无论是Linux还是JVM都提供了SLAB机制，简单来说就是分配给每个进程/线程一小块内存，这些线程在运行过程中是独享这块内存的，因此无需加锁，需要内存时就直接拿来用，用完了就放回来，从而提升进程处理性能，也避免了高频从共享内存申请/释放内存导致的一些异常问题。\n4. mmap 传统写数据，我们需要将数据从应用内存写到用户态内存，再切换到内核态将数据从用户态内存拷贝到内核态内存，最终再拷贝到磁盘。这个过程不仅对资源消耗较多，同时性能也比较糟糕。因此linux提供mmap机制，将磁盘映射到内核态的一段内存，用户程序直接通过指针往这块内存写数据，有linux自动将脏页刷到磁盘，从而避免了数据的多次拷贝，降低资源消耗以及提升处理性能。\n5. 零拷贝 在将本地上的数据发送到远程机器上时，常常要经历以下几个阶段\n将数据从磁盘拷贝到内核态内存 将数据从内核态内存拷贝到用户态内存 将数据从用户态内存拷贝到应用内存 将数据从应用内存拷贝到网卡缓冲区 这个过程中要涉及4次CPU上下文切换，并且一份数据要拷贝4次\n使用零拷贝技术后，CPU只需要向磁盘发送一次指令，磁盘就会通过DMA技术将数据从磁盘拷贝到网卡缓冲区并发送出去，从而提升处理的能力。\n总结 内存替代磁盘、顺序写代替随机写\n通过内存缓冲一层，不仅能够 “化” 随机写为顺序写，还 “化”单次顺序写为批量顺序写，更重要的是还能将内存缓冲当作“缓存”，用来加速查询性能以及磁盘资源损耗。\n移动少量数据替代移动大量数据\n通过移动必要的数据来化解移动大量数据的操作，不仅能够显著降低资源使用情况，更能提升大数据场景下任务处理能力。\n资源轻量级使用\n通过科学化、精细化的分配处理请求时依赖的各项操作资源来大幅降低处理请求的成本以及提升处理的性能。\n","date":"2024-12-18T18:44:58+08:00","image":"https://sherlock-lin.github.io/p/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E4%B9%8B%E9%81%93/image-20241212112322120_hu14434329111253270122.png","permalink":"https://sherlock-lin.github.io/p/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E4%B9%8B%E9%81%93/","title":"软件设计之道"},{"content":"由于集成gif到hugo会影响每次的编译速度，因此暂时需要客官移步到博主的csdn地址阅读 【Go教程】全网最经典、易懂、适合入门学习的学生成绩管理系统设计与实现\n","date":"2024-12-11T17:46:04+08:00","image":"https://sherlock-lin.github.io/p/go%E6%95%99%E7%A8%8B%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%8F%E5%85%B8%E6%98%93%E6%87%82%E9%80%82%E5%90%88%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AD%A6%E7%94%9F%E6%88%90%E7%BB%A9%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/image-20241211180040874_hu15700110173829593853.png","permalink":"https://sherlock-lin.github.io/p/go%E6%95%99%E7%A8%8B%E5%85%A8%E7%BD%91%E6%9C%80%E7%BB%8F%E5%85%B8%E6%98%93%E6%87%82%E9%80%82%E5%90%88%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AD%A6%E7%94%9F%E6%88%90%E7%BB%A9%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/","title":"【Go教程】全网最经典、易懂、适合入门学习的学生成绩管理系统设计与实现"},{"content":"一、概述 Apache Iceberg作为一款新兴的数据湖解决方案在实现上高度抽象，在存储上能够对接当前主流的HDFS，S3并且支持多种文件存储格式，例如Parquet、ORC、AVRO。相较于Hudi、Delta与Spark的强耦合，Iceberg可以与多种计算引擎对接，目前社区已经支持Spark/Flink读写Iceberg、Impala/Hive查询Iceberg。本文基于Apache Iceberg 1.7.x，介绍Iceberg文件的组织方式以及不同文件的存储格式。\n二、文件布局 从图中可以看到iceberg将数据进行分层管理，主要分为元数据管理层和数据存储层。元数据管理层又可以细分为三层：\nVersionMetadata Snapshot Manifest VersionMetadata存储当前版本的元数据信息（所有snapshot信息）；Snapshot表示当前操作的一个快照，每次commit都会生成一个快照，一个快照中包含多个Manifest，每个Manifest中记录了当前操作生成数据所对应的文件地址，也就是data files的地址。基于snapshot的管理方式，iceberg能够进行time travel（历史版本读取以及增量读取），并且提供了serializable isolation。数据存储层支持不同的文件格式，目前支持Parquet、ORC、AVRO。\n三、例子分析 本次例子是基于hadoop catalog为例(Iceberg也支持Hive catalog形式)，Iceberg表生成的数据目录结构如下所示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 ├── user │ ├── data │ │ ├── puffin │ │ │ ├── 4580221921120736517-10c39b78-c786-4536-8294-ac802f392fe7.puffin │ │ │ ├── 526437005861772456-09efafdd-5090-4548-b637-bf3d644d3b1b.puffin │ │ │ └── 8069721931047691859-dc9b7437-d23a-4981-8b88-81bd02c246b1.puffin │ │ ├── ts_day=2022-07-01 │ │ │ ├── 4-B-1-00172-22-2763128833788220964-00001.parquet │ │ │ ├── 4-B-4-00000-0-479688754665061397-00001.parquet │ │ │ └── 7-B-2-00000-0-5781928758206271752-00001.parquet │ │ └── ts_day=2022-07-02 │ │ ├── 4-B-12-00000-0-7590973027130486494-00001.parquet │ │ ├── 5-B-1-00071-21-3760745643944625433-00001.parquet │ │ ├── 5-B-14-00000-0-3550918434527665780-00001.parquet │ │ ├── 6-B-1-00046-20-6620432571688919260-00001.parquet │ │ ├── 6-B-8-00000-0-1887052925566974993-00001.parquet │ │ └── 7-B-10-00000-0-4565413093180536089-00001.parquet │ └── metadata │ ├── 08d593c9-ab07-4781-8124-543b48782e0d-m0.avro │ ├── bc549902-51ee-461e-af57-3658feb08351-m0.avro │ ├── bc549902-51ee-461e-af57-3658feb08351-m1.avro │ ├── dbea59c9-96a1-490b-9b61-3c4a1f844c81-m0.avro │ ├── dbea59c9-96a1-490b-9b61-3c4a1f844c81-m1.avro │ ├── snap-4580221921120736517-1-08d593c9-ab07-4781-8124-543b48782e0d.avro │ ├── snap-526437005861772456-1-dbea59c9-96a1-490b-9b61-3c4a1f844c81.avro │ ├── snap-8069721931047691859-1-bc549902-51ee-461e-af57-3658feb08351.avro │ ├── v1.metadata.json │ ├── v2.metadata.json │ ├── v3.metadata.json │ ├── v4.metadata.json │ └── version-hint.text 其中metadata目录存放元数据管理层的数据：\nversion-hint.text: 存储version.metadata.json的版本号，即下文的number version[number].metadata.json snap-[snapshotID]-[attemptID]-[commitUUID].avro(snapshot文件) [commitUUID]-m-[manifestCount].avro(manifest文件) data目录组织形式类似于hive，都是以分区进行目录组织（上图中id为分区列），最终数据可以使用不同文件格式进行存储：\n[sparkPartitionID]-[sparkTaskID]-[UUID]-[fileCount].[parquet | avro | orc] VersionMetadata VersionMetadata对应的文件是version[number].metadata.json文件， 本篇文章 解析metadata.json文件已做了详细的解析。简单来说，metadata.json文件保存了Iceberg table schema、partition、snapshot信息，partition中的transform信息使得Iceberg能够根据字段进行hidden partition，而无需像hive一样显示的指定分区字段。由于VersionMetadata中记录了每次snapshot的id以及create_time，我们可以通过时间或snapshotId查询相应snapshot的数据，实现Time Travel。\nSnapshot 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 // Snapshot: 2080639593951710914 // Location: hdfs://10.242.199.202:9000/hive/empty_order_item/metadata/snap-2080639593951710914-1-1f8279fb-5b2d-464c-af12-d9d6fbe9b5ae.avro // manifest entry { \u0026#34;manifest_path\u0026#34; : \u0026#34;hdfs://10.242.199.202:9000/hive/empty_order_item/metadata/1f8279fb-5b2d-464c-af12-d9d6fbe9b5ae-m1.avro\u0026#34;, \u0026#34;manifest_length\u0026#34; : 5291, \u0026#34;partition_spec_id\u0026#34; : 0, // 该manifest entry所属的snapshot \u0026#34;added_snapshot_id\u0026#34; : { \u0026#34;long\u0026#34; : 2080639593951710914 }, // 该manifest中添加的文件数量 \u0026#34;added_data_files_count\u0026#34; : { \u0026#34;int\u0026#34; : 4 }, // 创建该manifest时已经存在且 // 没有被这次创建操作删除的文件数量 \u0026#34;existing_data_files_count\u0026#34; : { \u0026#34;int\u0026#34; : 0 }, // 创建manifest删除的文件 \u0026#34;deleted_data_files_count\u0026#34; : { \u0026#34;int\u0026#34; : 0 }, // 该manifest中partition字段的范围 \u0026#34;partitions\u0026#34; : { \u0026#34;array\u0026#34; : [ { \u0026#34;contains_null\u0026#34; : false, \u0026#34;lower_bound\u0026#34; : { \u0026#34;bytes\u0026#34; : \u0026#34;\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34; }, \u0026#34;upper_bound\u0026#34; : { \u0026#34;bytes\u0026#34; : \u0026#34;\\u0004\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34; } } ] }, \u0026#34;added_rows_count\u0026#34; : { \u0026#34;long\u0026#34; : 4 }, \u0026#34;existing_rows_count\u0026#34; : { \u0026#34;long\u0026#34; : 0 }, \u0026#34;deleted_rows_count\u0026#34; : { \u0026#34;long\u0026#34; : 0 } } // manifest entry { \u0026#34;manifest_path\u0026#34; : \u0026#34;hdfs://10.242.199.202:9000/hive/empty_order_item/metadata/1f8279fb-5b2d-464c-af12-d9d6fbe9b5ae-m0.avro\u0026#34;, \u0026#34;manifest_length\u0026#34; : 5289, \u0026#34;partition_spec_id\u0026#34; : 0, \u0026#34;added_snapshot_id\u0026#34; : { \u0026#34;long\u0026#34; : 2080639593951710914 }, \u0026#34;added_data_files_count\u0026#34; : { \u0026#34;int\u0026#34; : 0 }, \u0026#34;existing_data_files_count\u0026#34; : { \u0026#34;int\u0026#34; : 0 }, \u0026#34;deleted_data_files_count\u0026#34; : { \u0026#34;int\u0026#34; : 4 }, \u0026#34;partitions\u0026#34; : { \u0026#34;array\u0026#34; : [ { \u0026#34;contains_null\u0026#34; : false, \u0026#34;lower_bound\u0026#34; : { \u0026#34;bytes\u0026#34; : \u0026#34;\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34; }, \u0026#34;upper_bound\u0026#34; : { \u0026#34;bytes\u0026#34; : \u0026#34;\\u0004\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34; } } ] }, \u0026#34;added_rows_count\u0026#34; : { \u0026#34;long\u0026#34; : 0 }, \u0026#34;existing_rows_count\u0026#34; : { \u0026#34;long\u0026#34; : 0 }, \u0026#34;deleted_rows_count\u0026#34; : { \u0026#34;long\u0026#34; : 4 } } 清单列表文件不仅仅是指向其他文件的简单指针；它是一个丰富的元数据文件，包含对 Apache Iceberg 中数据的有效管理和查询至关重要的详细信息。清单列表中的每个条目都对应一个清单文件，并包含描述该清单的状态和特征的各种字段。\n以下是清单文件内的重要字段：\nmanifest_path：该字段指定清单文件的位置。它是一个字符串，指向存储系统中存储清单的物理文件。 manifest_length：此字段表示清单文件的大小（以字节为单位）。了解大小有助于估算读取清单的成本，这对于优化查询执行非常重要。 partition_spec_id：随着架构的发展，Iceberg 中的每个表都可以随着时间的推移拥有多个分区规范。此字段跟踪用于编写清单的分区规范的 ID，允许 Iceberg 在读取数据时应用正确的分区逻辑。 content：此字段指定清单跟踪的内容类型，可以是数据文件 ( 0) 或删除文件 ( 1)。此区别对于合并和查询规划等操作至关重要，因为这些操作对数据和删除的处理方式不同。 sequence_number和min_sequence_number：这些字段是 Iceberg 版本控制系统的一部分。sequence_number表示清单添加到表中的时间，而min_sequence_number提供此清单跟踪的所有文件的最早序列号。这些字段对于了解数据的演变和实现时间旅行查询至关重要。 added_files_count、existing_files_count、deleted_files_count：这些字段提供清单中处于不同状态（已添加、现有或已删除）的文件数量。此元数据可帮助查询引擎确定清单是否与特定操作相关，从而可能跳过仅包含已删除文件或查询范围之外的文件的清单。 分区摘要：清单列表还可以包含分区字段的摘要，例如分区值的lower_bound和、和。这些摘要对于查询规划期间的分区修剪非常有用，因为它们允许查询引擎根据查询的过滤条件跳过不包含相关数据的整个清单。upper_bound``contains_null``contains_nan 清单列表是 Apache Iceberg 架构的重要组成部分，在高效、精确地管理大型数据集方面发挥着关键作用。通过跟踪与每个快照关联的清单文件，清单列表使 Iceberg 能够提供原子快照、时间旅行和优化查询执行等强大功能。\n通过其详细的元数据，清单列表允许查询引擎智能地决定要扫描哪些数据文件，从而显著减少不必要的 I/O 并提高查询性能。无论您处理的是数据湖还是复杂的分析平台，了解清单列表的运作方式都可以帮助您充分利用 Apache Iceberg 的潜力。\nManifest 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 // DataFileEntry { // 表示对应数据文件status // 0: EXISTING, 1: ADDED，2: DELETED \u0026#34;status\u0026#34; : 1, \u0026#34;snapshot_id\u0026#34; : { \u0026#34;long\u0026#34; : 2080639593951710914 }, \u0026#34;data_file\u0026#34; : { \u0026#34;file_path\u0026#34; : \u0026#34;hdfs://10.242.199.202:9000/hive/empty_order_item/data/id=1/00000-4-487b841b-13b4-4ae8-9238-f70674d5102e-00001.parquet\u0026#34;, \u0026#34;file_format\u0026#34; : \u0026#34;PARQUET\u0026#34;, // 对应的分区值 \u0026#34;partition\u0026#34; : { \u0026#34;id\u0026#34; : { \u0026#34;long\u0026#34; : 1 } }, // 文件中record数量 \u0026#34;record_count\u0026#34; : 1, // 文件大小 \u0026#34;file_size_in_bytes\u0026#34; : 1823, \u0026#34;block_size_in_bytes\u0026#34; : 67108864, // 不同column存储大小 \u0026#34;column_sizes\u0026#34; : { \u0026#34;array\u0026#34; : [ { \u0026#34;key\u0026#34; : 1, \u0026#34;value\u0026#34; : 52 }, { \u0026#34;key\u0026#34; : 2, \u0026#34;value\u0026#34; : 52 }, { \u0026#34;key\u0026#34; : 3, \u0026#34;value\u0026#34; : 52 }, { \u0026#34;key\u0026#34; : 4, \u0026#34;value\u0026#34; : 53 }, { \u0026#34;key\u0026#34; : 5, \u0026#34;value\u0026#34; : 51 }, { \u0026#34;key\u0026#34; : 6, \u0026#34;value\u0026#34; : 61 } ] }, // 不同列对应的value数量 \u0026#34;value_counts\u0026#34; : { \u0026#34;array\u0026#34; : [ { \u0026#34;key\u0026#34; : 1, \u0026#34;value\u0026#34; : 1 }, { \u0026#34;key\u0026#34; : 2, \u0026#34;value\u0026#34; : 1 }, { \u0026#34;key\u0026#34; : 3, \u0026#34;value\u0026#34; : 1 }, { \u0026#34;key\u0026#34; : 4, \u0026#34;value\u0026#34; : 1 }, { \u0026#34;key\u0026#34; : 5, \u0026#34;value\u0026#34; : 1 }, { \u0026#34;key\u0026#34; : 6, \u0026#34;value\u0026#34; : 1 } ] }, // 列值为null的数量 \u0026#34;null_value_counts\u0026#34; : { \u0026#34;array\u0026#34; : [ { \u0026#34;key\u0026#34; : 1, \u0026#34;value\u0026#34; : 0 }, { \u0026#34;key\u0026#34; : 2, \u0026#34;value\u0026#34; : 0 }, { \u0026#34;key\u0026#34; : 3, \u0026#34;value\u0026#34; : 0 }, { \u0026#34;key\u0026#34; : 4, \u0026#34;value\u0026#34; : 0 }, { \u0026#34;key\u0026#34; : 5, \u0026#34;value\u0026#34; : 0 }, { \u0026#34;key\u0026#34; : 6, \u0026#34;value\u0026#34; : 0 } ] }, // 不同列的范围 \u0026#34;lower_bounds\u0026#34; : { \u0026#34;array\u0026#34; : [ { \u0026#34;key\u0026#34; : 1, \u0026#34;value\u0026#34; : \u0026#34;\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34; }, { \u0026#34;key\u0026#34; : 2, \u0026#34;value\u0026#34; : \u0026#34;\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34; }, { \u0026#34;key\u0026#34; : 3, \u0026#34;value\u0026#34; : \u0026#34;\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34; }, { \u0026#34;key\u0026#34; : 4, \u0026#34;value\u0026#34; : \u0026#34;\\u0013ˆ\u0026#34; }, { \u0026#34;key\u0026#34; : 5, \u0026#34;value\u0026#34; : \u0026#34;\\u0002\\u0000\\u0000\\u0000\u0026#34; }, { \u0026#34;key\u0026#34; : 6, \u0026#34;value\u0026#34; : \u0026#34;table lamp\u0026#34; } ] }, \u0026#34;upper_bounds\u0026#34; : { \u0026#34;array\u0026#34; : [ { \u0026#34;key\u0026#34; : 1, \u0026#34;value\u0026#34; : \u0026#34;\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34; }, { \u0026#34;key\u0026#34; : 2, \u0026#34;value\u0026#34; : \u0026#34;\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34; }, { \u0026#34;key\u0026#34; : 3, \u0026#34;value\u0026#34; : \u0026#34;\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34; }, { \u0026#34;key\u0026#34; : 4, \u0026#34;value\u0026#34; : \u0026#34;\\u0013ˆ\u0026#34; }, { \u0026#34;key\u0026#34; : 5, \u0026#34;value\u0026#34; : \u0026#34;\\u0002\\u0000\\u0000\\u0000\u0026#34; }, { \u0026#34;key\u0026#34; : 6, \u0026#34;value\u0026#34; : \u0026#34;table lamp\u0026#34; } ] }, \u0026#34;key_metadata\u0026#34; : null, // parquet block offset/ orc stripe offset \u0026#34;split_offsets\u0026#34; : { \u0026#34;array\u0026#34; : [ 4 ] } } } { ... } Manifest管理多个data文件，一条DataFileEntry对应一个data文件，DataFileEntry中记录了所属partition，value bounds等信息，value_counts和null_value_counts可以用于过滤null列，例：column a所对应的value_count为3，且对应的null_value_count也为3，此时如果select a，则可以根据value_count-null_value_count=0判断a全为null直接返回而无需再进行parquet文件的查询；除此之外，可以根据value bounds进行过滤，加速查询。\n以下是清单文件中的一些关键字段：\nfile_path：该字段记录了数据文件在存储系统中的位置，是一个字符串，提供了文件的完整路径，确保 Iceberg 在需要的时候可以快速找到数据文件。 partition_data：此字段包含有关文件中数据的分区值的信息。分区是 Iceberg 架构的一个重要方面，因为它允许组织数据，使其更容易过滤和高效查询。该partition_data字段确保 Iceberg 可以在查询执行期间应用正确的分区逻辑。 file_format：此字段指定数据文件的格式（例如 Parquet、Avro、ORC）。了解格式至关重要，因为它决定了 Iceberg 如何读取和写入文件，以及如何优化针对数据的查询。 record_count：该record_count字段表示数据文件中包含的记录数。此元数据可帮助查询引擎估计数据集的大小并决定如何优化查询执行。 file_size_in_bytes：此字段提供数据文件的总大小（以字节为单位）。与 一样record_count，文件大小是了解数据规模和规划高效扫描的重要指标。 **value_counts、、null_value_counts：nan_value_counts**这些字段是提供文件中数据详细统计信息的指标。value_counts给出每列中的值总数，null_value_counts跟踪空值的数量，并nan_value_counts记录 NaN（非数字）值的数量。这些指标对于查询优化非常有用，因为它们可以帮助查询引擎根据相关数据的存在与否来确定是否应扫描文件。 lower_bounds和upper_bounds：这些字段存储数据文件中每列的最小值和最大值。查询引擎使用这些边界来执行最小/最大修剪 — 跳过不符合查询过滤条件的数据文件。例如，如果查询正在寻找特定日期范围内的数据，而数据文件的lower_bounds和upper_bounds超出范围，则查询引擎可以完全跳过读取该文件。 关键作用的字段\n高效的数据定位：file_path和file_format字段确保 Iceberg 可以快速定位并正确解释数据文件，无论它们存储或格式化在何处。 增强查询优化：诸如partition_data、record_count、file_size_in_bytes和各种计数（例如value_counts）之类的字段为 Iceberg 优化查询执行提供了必要的元数据。通过了解数据的大小、格式和结构，Iceberg 可以规划更高效的扫描，减少读取的数据量并加快查询速度。 数据修剪：lower_bounds 和 upper_bounds 字段对于查询优化尤为重要。它们使 Iceberg 能够在扫描开始之前修剪不必要的数据文件，确保只处理最相关的数据。 清单文件的内容使 Iceberg 能够控制大型数据集，确保高效管理数据集并尽快执行查询。通过利用这些元数据，Iceberg 可以提供现代数据湖所需的高性能和可扩展性。\npuffin 相比Manifest文件，puffin提供了更详细的数据文件的详细元信息，帮助分析/计算引擎更高效的检索必要的数据文件。\n","date":"2024-12-05T14:53:34+08:00","image":"https://sherlock-lin.github.io/p/%E8%A7%A3%E6%9E%90iceberg%E6%96%87%E4%BB%B6%E5%B8%83%E5%B1%80/image-20241204180525798_hu1822080974460878562.png","permalink":"https://sherlock-lin.github.io/p/%E8%A7%A3%E6%9E%90iceberg%E6%96%87%E4%BB%B6%E5%B8%83%E5%B1%80/","title":"解析Iceberg文件布局"},{"content":"概述 Spark进程模型中Driver负责解析用户代码、构建计算流图，然后将计算流图转化为分布式任务，并把任务分发给集群中的Executors交付运行。而Driver是怎么把计算图拆解为分布式任务，又是按照什么规则分发给Executors的呢？还有，Executors具体又是如何执行分布式任务的呢？\n分布式计算的精髓，在于如何把抽象的计算图，转化为实实在在的分布式计算任务，然后以并行计算的方式交付执行。因此深入探索调度系统是掌握Spark的必经之路。\n角色划分 Driver DAGScheduler：负责将DAG转换为执行阶段Stages TaskScheduler：负责将DAGScheduler划分好的计算任务分发给SchedulerBackend执行 Executors SchedulerBackend：是资源管理器的代理，用于适配Standalone、Yarn上的计算资源 从全局视角来看，DAGScheduler是任务调度的发起者，DAGScheduler以TaskSet为粒度，向TaskScheduler提交任务调度请求。TaskScheduler在初始化的过程中，会创建任务调度队列，任务调度队列用于缓存 DAGScheduler提交的TaskSets。TaskScheduler结合SchedulerBackend提供的 WorkerOffer，按照预先设置的调度策略依次对队列中的任务进行调度。\n简而言之，DAGScheduler手里有“活儿”，SchedulerBackend手里有“人力”，TaskScheduler的核心职能，就是把合适的“活儿”派发到合适的“人”的手里。由此可见，TaskScheduler承担的是承上启下、上通下达的关键角色，这也正是我们将“塔斯克”视为斯巴克建筑公司元老之一的重要原因。\nDAGScheduler 作为集团公司的“总架”（总架构师），戴格的核心职责，是把计算图DAG拆分为执行阶段Stages，Stages指的是不同的运行阶段，同时还要负责把Stages转化为任务集合TaskSets，也就是把“建筑图纸”转化成可执行、可操作的“建筑项目”。\n用一句话来概括从 DAG 到 Stages 的拆分过程，那就是：以 Actions 算子为起点，从后向前回溯 DAG，以 Shuffle 操作为边界去划分 Stages。\n具体来说，在Word Count的例子中，DAGScheduler最先提请执行的是Stage1。在提交的时候，DAGScheduler发现Stage1依赖的父Stage，也就是Stage0，还没有执行过，那么这个时候它会把Stage1的提交动作压栈，转而去提请执行Stage0。当Stage0执行完毕的时候，DAGScheduler通过出栈的动作，再次提请执行Stage 1。对于提请执行的每一个Stage，DAGScheduler根据Stage内RDD的partitions属性创建分布式任务集合TaskSet。TaskSet包含一个又一个分布式任务Task，RDD有多少数据分区，TaskSet就包含多少个Task。换句话说，Task与RDD的分区，是一一对应的。\nDAGScheduler的主要职责有三个：\n根据用户代码构建DAG； 以Shuffle为边界切割Stages； 基于Stages创建TaskSets，并将TaskSets提交给TaskScheduler请求调度。 SchedulerBackend SchedulerBackend用一个叫做ExecutorDataMap的数据结构，来记录每一个计算节点中Executors的资源状态。这里的ExecutorDataMap是一种HashMap，它的Key是标记 Executor 的字符串，Value是一种叫做ExecutorData的数据结构。ExecutorData用于封装Executor的资源状态，如RPC地址、主机地址、可用CPU核数和满配CPU核数等等，它相当于是对Executor做的“资源画像”。\nSchedulerBackend与集群内所有Executors中的ExecutorBackend保持周期性通信，双方通过LaunchedExecutor、RemoveExecutor、StatusUpdate等消息来互通有无、变更可用计算资源。拜肯德正是通过这些小弟发送的“信件”，来不停地更新自己手中的那本小册子，从而对集团人力资源了如指掌。\nTaskScheduler 一把手戴格有“活儿”，三把手拜肯德出“人力”，接下来，终于轮到牵线搭桥的塔斯克出马了。作为施工经理，塔斯克的核心职责是，给定拜肯德提供的“人力”，遴选出最合适的“活儿”并派发出去。而这个遴选的过程，就是任务调度的核心所在\nTaskScheduler是按照任务的本地倾向性，来遴选出TaskSet中适合调度的Tasks。这是什么意思呢？听上去比较抽象，我们还是从DAGScheduler在Stage内创建任务集TaskSet说起。\n我们刚刚说过，Task与RDD的partitions是一一对应的，在创建Task的过程中，DAGScheduler会根据数据分区的物理地址，来为Task设置locs属性。locs属性记录了数据分区所在的计算节点、甚至是Executor进程ID。举例来说，当我们调用textFile API从HDFS文件系统中读取源文件时，Spark会根据HDFS NameNode当中记录的元数据，获取数据分区的存储地址，例如node0:/rootPath/partition0-replica0，node1:/rootPath/partition0-replica1和node2:/rootPath/partition0-replica2。\n那么，DAGScheduler在为该数据分区创建Task0的时候，会把这些地址中的计算节点记录到Task0的locs属性。如此一来，当TaskScheduler需要调度Task0这个分布式任务的时候，根据Task0的locs属性，它就知道：“Task0所需处理的数据分区，在节点node0、node1、node2上存有副本，因此，如果WorkOffer是来自这3个节点的计算资源，那对Task0来说就是投其所好”。从这个例子我们就能更好地理解，每个任务都是自带本地倾向性的，换句话说，每个任务都有自己的“调度意愿”。像上面这种定向到计算节点粒度的本地性倾向，Spark中的术语叫做NODE_LOCAL。除了定向到节点，Task还可以定向到进程（Executor）、机架、任意地址，它们对应的术语分别是PROCESS_LOCAL、RACK_LOCAL和ANY。\n下图展示的是，TaskScheduler依据本地性倾向，依次进行任务调度的运行逻辑：\n不难发现，从PROCESS_LOCAL、NODE_LOCAL、到RACK_LOCAL、再到ANY，Task的本地性倾向逐渐从严苛变得宽松。TaskScheduler接收到WorkerOffer之后，也正是按照这个顺序来遍历TaskSet中的Tasks，优先调度本地性倾向为PROCESS_LOCAL的Task，而NODE_LOCAL次之，RACK_LOCAL为再次，最后是ANY。\nSpark调度系统的核心思想，是“数据不动、代码动。也就是说，在任务调度的过程中，为了完成分布式计算，Spark倾向于让数据待在原地、保持不动，而把计算任务（代码）调度、分发到数据所在的地方，从而消除数据分发引入的性能隐患。毕竟，相比分发数据，分发代码要轻量得多。\n结合WorkerOffer与任务的本地性倾向，塔斯克TaskScheduler挑选出了适合调度的“活儿”：Tasks。接下来，TaskScheduler就把这些Tasks通过LaunchTask消息，发送给好基友SchedulerBackend。人力资源总监SchedulerBackend拿到这些活儿之后，同样使用LaunchTask消息，把活儿进一步下发给分公司的小弟：ExecutorBackend。\nExecutorBackend ExecutorBackend拿到“活儿”之后，随即把活儿派发给分公司的建筑工人。这些工人，就是Executors线程池中一个又一个的CPU线程，每个线程负责处理一个Task。每当Task处理完毕，这些线程便会通过ExecutorBackend，向Driver端的SchedulerBackend发送StatusUpdate事件，告知Task执行状态。接下来，TaskScheduler与SchedulerBackend通过接力的方式，最终把状态汇报给DAGScheduler。\n对于同一个TaskSet当中的Tasks来说，当它们分别完成了任务调度与任务执行这两个环节时，也就是上图中步骤1到步骤9的计算过程，Spark调度系统就完成了DAG中某一个Stage的任务调度。不过，故事到这里并未结束。我们知道，一个DAG会包含多个Stages，一个Stage的结束即宣告下一个Stage的开始，而这也是戴格起初将DAG划分为Stages的意义所在。只有当所有的Stages全部调度、执行完毕，才表示一个完整的Spark作业宣告结束。\n总结 具体说来，任务调度分为如下5个步骤：\nDAGScheduler以Shuffle为边界，将开发者设计的计算图DAG拆分为多个执行阶段Stages，然后为每个Stage创建任务集TaskSet。 SchedulerBackend通过与Executors中的ExecutorBackend的交互来实时地获取集群中可用的计算资源，并将这些信息记录到ExecutorDataMap数据结构。 与此同时，SchedulerBackend根据ExecutorDataMap中可用资源创建WorkerOffer，以WorkerOffer为粒度提供计算资源。 对于给定WorkerOffer，TaskScheduler结合TaskSet中任务的本地性倾向，按照PROCESS_LOCAL、NODE_LOCAL、RACK_LOCAL和ANY的顺序，依次对TaskSet中的任务进行遍历，优先调度本地性倾向要求苛刻的Task。 被选中的Task由TaskScheduler传递给SchedulerBackend，再由SchedulerBackend分发到Executors中的ExecutorBackend。Executors接收到Task之后，即调用本地线程池来执行分布式任务。 ","date":"2024-12-04T14:32:32+08:00","image":"https://sherlock-lin.github.io/p/spark%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E8%AF%A6%E8%A7%A3/trees-8136806_1280_hu4473522823771471937.png","permalink":"https://sherlock-lin.github.io/p/spark%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E8%AF%A6%E8%A7%A3/","title":"Spark调度系统详解"},{"content":"一、概述 Apache Iceberg是一种数据湖格式，它的出现主要是用于解决大型数据湖相关的很多问题。如它允许模式演变、时间旅行查询和高效的数据分区等，同时保持与现有数据引擎的兼容性。Iceberg功能核心是文件 metadata.json，它是表元数据管理的核心。\n二、metadata.json作用 metadata.json有以下五个重要用途\n集中式表管理：metadata.json是Iceberg表元数据的单一真实来源，这包括schame的定义、分区策略和快照历史记录等等，这让计算引擎更容易理解以及与表交互。 schema演变：Iceberg表可以随时间进行schema演变，添加新列或修改现有列，跟踪metadata.json的变化，确保历史数据仍然可以访问，并且可以针对表历史记录中的任何点执行查询。 数据分区和组织：通过metadata.json定义分区方式，有助于优化数据存储和查询性能，分区策略可以更新(分区演变)，并且此文件会跟踪所有此类更改。 快照管理：Iceberg允许快照，快照本质上是表在不同时间节点的版本。元数据文件记录这些快照，从而实现时间旅行查询等功能，用户可以查询过去存在的表。 一致性和完整性：通过提供哪些数据应该存在哪里，以何种形态存在的明确参考，确保表上的所有操作都保持数据完整性。 metadata.json文件不仅是一个静态记录，而是一个随表而不断演化的动态文档，它是Apache Iceberg架构中不可获取的组成部分。在对表进行增删改、优化等操作时，Iceberg都会创建一个新的metadata.json来跟踪表的最新元信息。\n三、内容详解 表格式版本 字段：format-version 值：整数（1 或 2） 用途：标识当前表的格式版本，不同版本的表行为不同，例如在2版本时删数据支持MOR特性等 表UUID 字段：table-uuid 值：UUID 用途：Iceberg 中的每个表都有一个唯一标识符，即table-uuid。此 UUID 在创建表时生成，用于确保表的元数据在不同操作之间匹配，尤其是在元数据刷新后。如果不匹配，则表示存在潜在冲突或损坏，从而引发异常。 数据存储位置 字段：location 值：hdfs或者s3地址等，如 hdfs://localhost:9000/warehouse/db/user 用途：此字段指定表的数据文件、清单文件和元数据文件的存储基本位置。计算引擎通过此字段来确定新数据的放置位置，确保表的所有部分都位于正确的位置。 最近更新时间 字段：last-updated-ms 值：时间戳 用途：此字段记录元数据的上次更新时间。它在写入元数据文件之前更新，为最新更改的提交提供时间戳。 表schemas信息 字段：schemas 值：schema列表，schames是模式对象列表，每个对象都有一个唯一标识schema-id, 而current-schema-id指向的就是当前正在使用的schema对象ID 用途：Iceberg支持schema演变，列表schemas跟踪已用于表的所有模式，同时 current-schema-id指向最新的模式。此设置允许使用最初写入的schema模式查询历史数据，确保模式更改中的数据一致性和灵活性。 两者的结构关系如下图\n分区信息 字段：partition-specs 描述：它是完整分区对象的列表，每个对象都详细说明了如何对数据进行分区，default-spec-id指向当前表使用的分区对象ID 用途：Iceberg中的分区对于数据组织和查询优化至关重要，此字段定义如何将数据按分区划分，分区可以基于date、category等进行 快照 描述：last-sequence-number是一个单调递增的long，current-snapshot-id是最新快照的ID，snapshots是有效快照的列表，snapshot-log记录了当前快照的变化。 用途：这些字段管理表状态的历史记录。快照允许进行时间旅行查询，其中可以查询任何时间点存在的数据。这snapshot-log有助于跟踪当前快照的变化，有助于了解表随时间的演变。 元数据变更历史 字段：metadata-log\n描述：metadata-log是一个维护元数据历史变更的数组，或者说此字段维护了Iceberg表 metadata.json之间的继承依赖关系，通过这个关系能够方便维护管理元数据变更的历史\n这个字段的内容如下\n排序 描述：sort-orders是排序顺序对象的列表，default-sort-order-id指定默认排序顺序。 用途：这些字段定义数据在分区或表中的排序方式。虽然排序对于写入者来说更为重要，但它也会影响数据的读取方式，尤其是对于某些类型的查询而言，其中数据顺序很重要。 四、metadata.json例子 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 { \u0026#34;format-version\u0026#34;: 2, \u0026#34;table-uuid\u0026#34;: \u0026#34;f5d68306-5a68-46b7-a937-b01f14855392\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;hdfs://localhost:9000/warehouse/db/user\u0026#34;, \u0026#34;last-sequence-number\u0026#34;: 3, \u0026#34;last-updated-ms\u0026#34;: 1732178316826, \u0026#34;last-column-id\u0026#34;: 3, \u0026#34;current-schema-id\u0026#34;: 0, \u0026#34;schemas\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;struct\u0026#34;, \u0026#34;schema-id\u0026#34;: 0, \u0026#34;identifier-field-ids\u0026#34;: [ 1 ], \u0026#34;fields\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;required\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;int\u0026#34; }, { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;required\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;id\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;ts\u0026#34;, \u0026#34;required\u0026#34;: false, \u0026#34;type\u0026#34;: \u0026#34;timestamptz\u0026#34; } ] } ], \u0026#34;default-spec-id\u0026#34;: 0, \u0026#34;partition-specs\u0026#34;: [ { \u0026#34;spec-id\u0026#34;: 0, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ts_day\u0026#34;, \u0026#34;transform\u0026#34;: \u0026#34;day\u0026#34;, \u0026#34;source-id\u0026#34;: 3, \u0026#34;field-id\u0026#34;: 1000 } ] } ], \u0026#34;last-partition-id\u0026#34;: 1000, \u0026#34;default-sort-order-id\u0026#34;: 0, \u0026#34;sort-orders\u0026#34;: [ { \u0026#34;order-id\u0026#34;: 0, \u0026#34;fields\u0026#34;: [ ] } ], \u0026#34;properties\u0026#34;: { \u0026#34;owner\u0026#34;: \u0026#34;lin\u0026#34;, \u0026#34;mixed-format.primary-key-fields\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;flink.max-continuous-empty-commits\u0026#34;: \u0026#34;2147483647\u0026#34;, \u0026#34;table.create-timestamp\u0026#34;: \u0026#34;1732171642459\u0026#34;, \u0026#34;watermark.table\u0026#34;: \u0026#34;1732178316302\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;mixed_iceberg\u0026#34;, \u0026#34;write.parquet.compression-codec\u0026#34;: \u0026#34;zstd\u0026#34;, \u0026#34;table-format\u0026#34;: \u0026#34;MIXED_ICEBERG\u0026#34;, \u0026#34;write.metadata.delete-after-commit.enabled\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;mixed-format.change.identifier\u0026#34;: \u0026#34;db.user_change_\u0026#34;, \u0026#34;mixed-format.table-store\u0026#34;: \u0026#34;base\u0026#34; }, \u0026#34;current-snapshot-id\u0026#34;: 526437005861772456, \u0026#34;refs\u0026#34;: { \u0026#34;main\u0026#34;: { \u0026#34;snapshot-id\u0026#34;: 526437005861772456, \u0026#34;type\u0026#34;: \u0026#34;branch\u0026#34; } }, \u0026#34;snapshots\u0026#34;: [ { \u0026#34;sequence-number\u0026#34;: 1, \u0026#34;snapshot-id\u0026#34;: 4580221921120736517, \u0026#34;timestamp-ms\u0026#34;: 1732171680678, \u0026#34;summary\u0026#34;: { \u0026#34;operation\u0026#34;: \u0026#34;overwrite\u0026#34;, \u0026#34;replace-partitions\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;optimized-sequence.exist\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;added-data-files\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;added-records\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;added-files-size\u0026#34;: \u0026#34;2753\u0026#34;, \u0026#34;changed-partition-count\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;total-records\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;total-files-size\u0026#34;: \u0026#34;2753\u0026#34;, \u0026#34;total-data-files\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;total-delete-files\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;total-position-deletes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;total-equality-deletes\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;manifest-list\u0026#34;: \u0026#34;hdfs://localhost:9000/warehouse/db/user/metadata/snap-4580221921120736517-1-08d593c9-ab07-4781-8124-543b48782e0d.avro\u0026#34;, \u0026#34;schema-id\u0026#34;: 0 }, { \u0026#34;sequence-number\u0026#34;: 2, \u0026#34;snapshot-id\u0026#34;: 8069721931047691859, \u0026#34;parent-snapshot-id\u0026#34;: 4580221921120736517, \u0026#34;timestamp-ms\u0026#34;: 1732172970959, \u0026#34;summary\u0026#34;: { \u0026#34;operation\u0026#34;: \u0026#34;overwrite\u0026#34;, \u0026#34;base-optimized-time.exist\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;optimized-sequence.exist\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;snapshot.producer\u0026#34;: \u0026#34;OPTIMIZE\u0026#34;, \u0026#34;added-data-files\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;deleted-data-files\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;added-records\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;deleted-records\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;added-files-size\u0026#34;: \u0026#34;1903\u0026#34;, \u0026#34;removed-files-size\u0026#34;: \u0026#34;918\u0026#34;, \u0026#34;changed-partition-count\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;total-records\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;total-files-size\u0026#34;: \u0026#34;3738\u0026#34;, \u0026#34;total-data-files\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;total-delete-files\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;total-position-deletes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;total-equality-deletes\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;manifest-list\u0026#34;: \u0026#34;hdfs://localhost:9000/warehouse/db/user/metadata/snap-8069721931047691859-1-bc549902-51ee-461e-af57-3658feb08351.avro\u0026#34;, \u0026#34;schema-id\u0026#34;: 0 }, { \u0026#34;sequence-number\u0026#34;: 3, \u0026#34;snapshot-id\u0026#34;: 526437005861772456, \u0026#34;parent-snapshot-id\u0026#34;: 8069721931047691859, \u0026#34;timestamp-ms\u0026#34;: 1732178316693, \u0026#34;summary\u0026#34;: { \u0026#34;operation\u0026#34;: \u0026#34;overwrite\u0026#34;, \u0026#34;base-optimized-time.exist\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;optimized-sequence.exist\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;snapshot.producer\u0026#34;: \u0026#34;OPTIMIZE\u0026#34;, \u0026#34;added-data-files\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;deleted-data-files\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;added-records\u0026#34;: \u0026#34;7\u0026#34;, \u0026#34;deleted-records\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;added-files-size\u0026#34;: \u0026#34;3832\u0026#34;, \u0026#34;removed-files-size\u0026#34;: \u0026#34;1835\u0026#34;, \u0026#34;changed-partition-count\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;total-records\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;total-files-size\u0026#34;: \u0026#34;5735\u0026#34;, \u0026#34;total-data-files\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;total-delete-files\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;total-position-deletes\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;total-equality-deletes\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;manifest-list\u0026#34;: \u0026#34;hdfs://localhost:9000/warehouse/db/user/metadata/snap-526437005861772456-1-dbea59c9-96a1-490b-9b61-3c4a1f844c81.avro\u0026#34;, \u0026#34;schema-id\u0026#34;: 0 } ], \u0026#34;statistics\u0026#34;: [ { \u0026#34;snapshot-id\u0026#34;: 526437005861772456, \u0026#34;statistics-path\u0026#34;: \u0026#34;hdfs://localhost:9000/warehouse/db/user/data/puffin/526437005861772456-09efafdd-5090-4548-b637-bf3d644d3b1b.puffin\u0026#34;, \u0026#34;file-size-in-bytes\u0026#34;: 384, \u0026#34;file-footer-size-in-bytes\u0026#34;: 265, \u0026#34;blob-metadata\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;optimized-sequence\u0026#34;, \u0026#34;snapshot-id\u0026#34;: 526437005861772456, \u0026#34;sequence-number\u0026#34;: 3, \u0026#34;fields\u0026#34;: [ ] }, { \u0026#34;type\u0026#34;: \u0026#34;base-optimized-time\u0026#34;, \u0026#34;snapshot-id\u0026#34;: 526437005861772456, \u0026#34;sequence-number\u0026#34;: 3, \u0026#34;fields\u0026#34;: [ ] } ] }, { \u0026#34;snapshot-id\u0026#34;: 8069721931047691859, \u0026#34;statistics-path\u0026#34;: \u0026#34;hdfs://localhost:9000/warehouse/db/user/data/puffin/8069721931047691859-dc9b7437-d23a-4981-8b88-81bd02c246b1.puffin\u0026#34;, \u0026#34;file-size-in-bytes\u0026#34;: 351, \u0026#34;file-footer-size-in-bytes\u0026#34;: 267, \u0026#34;blob-metadata\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;optimized-sequence\u0026#34;, \u0026#34;snapshot-id\u0026#34;: 8069721931047691859, \u0026#34;sequence-number\u0026#34;: 2, \u0026#34;fields\u0026#34;: [ ] }, { \u0026#34;type\u0026#34;: \u0026#34;base-optimized-time\u0026#34;, \u0026#34;snapshot-id\u0026#34;: 8069721931047691859, \u0026#34;sequence-number\u0026#34;: 2, \u0026#34;fields\u0026#34;: [ ] } ] }, { \u0026#34;snapshot-id\u0026#34;: 4580221921120736517, \u0026#34;statistics-path\u0026#34;: \u0026#34;hdfs://localhost:9000/warehouse/db/user/data/puffin/4580221921120736517-10c39b78-c786-4536-8294-ac802f392fe7.puffin\u0026#34;, \u0026#34;file-size-in-bytes\u0026#34;: 195, \u0026#34;file-footer-size-in-bytes\u0026#34;: 146, \u0026#34;blob-metadata\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;optimized-sequence\u0026#34;, \u0026#34;snapshot-id\u0026#34;: 4580221921120736517, \u0026#34;sequence-number\u0026#34;: 1, \u0026#34;fields\u0026#34;: [ ] } ] } ], \u0026#34;snapshot-log\u0026#34;: [ { \u0026#34;timestamp-ms\u0026#34;: 1732171680678, \u0026#34;snapshot-id\u0026#34;: 4580221921120736517 }, { \u0026#34;timestamp-ms\u0026#34;: 1732172970959, \u0026#34;snapshot-id\u0026#34;: 8069721931047691859 }, { \u0026#34;timestamp-ms\u0026#34;: 1732178316693, \u0026#34;snapshot-id\u0026#34;: 526437005861772456 } ], \u0026#34;metadata-log\u0026#34;: [ { \u0026#34;timestamp-ms\u0026#34;: 1732171642473, \u0026#34;metadata-file\u0026#34;: \u0026#34;hdfs://localhost:9000/warehouse/db/user/metadata/v1.metadata.json\u0026#34; }, { \u0026#34;timestamp-ms\u0026#34;: 1732171680784, \u0026#34;metadata-file\u0026#34;: \u0026#34;hdfs://localhost:9000/warehouse/db/user/metadata/v2.metadata.json\u0026#34; }, { \u0026#34;timestamp-ms\u0026#34;: 1732172970991, \u0026#34;metadata-file\u0026#34;: \u0026#34;hdfs://localhost:9000/warehouse/db/user/metadata/v3.metadata.json\u0026#34; } ] } 五、查询/计算引擎如何使用metadata.json 计算引擎会根据 metadata.json构造查询规划，主要根据以下进行\n分区裁剪：根据partition-specs信息，引擎可以获得每个历史分区方案的详细信息，从而方便跟清单列表和清单文件中引用的分区ID相匹配，从而允许它从扫描计划中裁剪掉不必要的数据文件。 模式验证：在执行查询之前，引擎会检查current-schema-id指定的schame对象跟查询的表元信息是否对应的上 元数据日志记录：metadata-log给计算引擎提供元数据更改的历史记录，通过识别catalog引用哪个metadata.json来将表回滚到之前的状态。 MVCC：Iceberg通过序列号在并发事务时保持一致性，写入将在开始时对原来的序列号进行+1操作，并在提交时确认这个号码是否仍然还是序列中的下一个。如果另一个事务在原始写入提交之前已声明下一个序列号，则当前事务则可以重新尝试写入。 六、总结 Apache Iceberg的metadata.json给计算引擎提供了全新的指导，让计算引擎能够高效地管理、查询和演进(包括schema、分区等)大规模数据表。通过提供详细的元数据，从而允许计算引擎在查询规划到数据一致性的各个层面进行优化，从而使Iceberg表具有高性能和灵活性。\n","date":"2024-12-03T15:48:30+08:00","image":"https://sherlock-lin.github.io/p/%E8%A7%A3%E6%9E%90metadata.json%E6%96%87%E4%BB%B6/the-creative-exchange-d2zvqp3fpro-unsplash_hu5876398126655421130.jpg","permalink":"https://sherlock-lin.github.io/p/%E8%A7%A3%E6%9E%90metadata.json%E6%96%87%E4%BB%B6/","title":"解析metadata.json文件"},{"content":"前言 由于不同场景下需要用的JDK版本不同，常常需要在多个JDK版本之间进行切换，因此将这个操作步骤工具化是很有必要的\n痛点 在多个JDK版本之间切换，传统方式是在~/.bash_profile文件中配置以下内容\n1 2 3 4 5 6 #JAVA_HOME=\u0026#34;/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home\u0026#34; #JAVA_HOME=\u0026#34;/Users/lin/dev/java/jdk-11.0.13.jdk/Contents/Home\u0026#34; JAVA_HOME=\u0026#34;/Users/lin/dev/java/jdk-17.0.10.jdk/Contents/Home\u0026#34; PATH=$PATH:$JAVA_HOME/bin export PATH 在需要切换版本时通过以下步骤完成\nvim ~/.bash_profile 注释掉当前jdk版本，取消注释目标jdk版本 source ~/.bash_profile 通过 java -version 确认jdk版本切换成功 以上步骤缺点\n较为繁琐，高频切换时会比较疲劳从而影响到真正要做的事情 手动编辑容易出错，并且~/.bash_profile文件比较重要，要尽量避免手动编辑其内容，内容出错严重可能会导致无法正常的使用电脑 工具化 通过 vim ~/.zshrc 指令编辑zsh配置，如果没有安装zsh工具的同学请务必安装，这是提高开发效率以及体验的必要工具\n追加配置内容如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # JDK 多版本管理 ## JDK 8、11、17 的 export 命令 export JAVA_8_HOME=\u0026#34;/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home\u0026#34; export JAVA_11_HOME=\u0026#34;/Users/lin/dev/java/jdk-11.0.13.jdk/Contents/Home\u0026#34; export JAVA_17_HOME=\u0026#34;/Users/lin/dev/java/jdk-17.0.10.jdk/Contents/Home\u0026#34; ## alias 命令链接到 export 命令，方便随时调整 JDK 版本 alias j8=\u0026#34;export JAVA_HOME=$JAVA_8_HOME\u0026#34; alias j11=\u0026#34;export JAVA_HOME=$JAVA_11_HOME\u0026#34; alias j17=\u0026#34;export JAVA_HOME=$JAVA_17_HOME\u0026#34; ## 默认使用 java8 export JAVA_HOME=$JAVA_8_HOME source ~/.bash_profile 编辑结束后通过 source ~/.zshrc 进行配置生效\n配置好后效果如下\n注：之前已经配置了 alias jv=\u0026lsquo;java -version\u0026rsquo; ，因此可以通过jv指令快速查看jdk版本\n","date":"2024-11-27T11:48:45+08:00","image":"https://sherlock-lin.github.io/p/jdk%E5%A4%9A%E7%89%88%E6%9C%AC%E5%88%87%E6%8D%A2%E5%B7%A5%E5%85%B7/gannet-9003524_1280_hu7120391022597348638.jpg","permalink":"https://sherlock-lin.github.io/p/jdk%E5%A4%9A%E7%89%88%E6%9C%AC%E5%88%87%E6%8D%A2%E5%B7%A5%E5%85%B7/","title":"JDK多版本切换工具"},{"content":"概述 RDD是弹性分布式数据集,是Spark中最基本的数据抽象,用来表示分布式数据集合,其支持分布式操作!\n在Spark中创建RDD的创建方式大概可以分为三种\n从集合中创建RDD 从外部存储创建RDD 从其他RDD创建 而从集合中创建RDD，Spark主要提供了两中函数：parallelize和makeRDD。我们可以先看看这两个函数的声明\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 /** Distribute a local Scala collection to form an RDD. * * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call * to parallelize and before the first action on the RDD, the resultant RDD will reflect the * modified collection. Pass a copy of the argument to avoid this. * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection */ def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) } /** Distribute a local Scala collection to form an RDD. * * This method is identical to `parallelize`. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection */ def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { parallelize(seq, numSlices) } /** * Distribute a local Scala collection to form an RDD, with one or more * location preferences (hostnames of Spark nodes) for each object. * Create a new partition for each collection item. * @param seq list of tuples of data and location preferences (hostnames of Spark nodes) * @return RDD representing data partitioned according to location preferences */ def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope { assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t =\u0026gt; (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), math.max(seq.size, 1), indexToPrefs) } 可以看到makeRDD的底层也是调用的parallelize方法进行实现，两者从语义上是相同的。\n实战 RDD中的数据可以来源于2个地方：本地集合或外部数据源\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import org.apache.spark.rdd.RDD import org.apache.spark.{SparkConf, SparkContext} object RDDDemo01_Create { def main(args: Array[String]): Unit = { //TODO 0.env/创建环境 val conf: SparkConf = new SparkConf().setAppName(\u0026#34;spark\u0026#34;).setMaster(\u0026#34;local[*]\u0026#34;) val sc: SparkContext = new SparkContext(conf) sc.setLogLevel(\u0026#34;WARN\u0026#34;) //TODO 1.source/加载数据/创建RDD val rdd1: RDD[Int] = sc.parallelize(1 to 10) //8 val rdd2: RDD[Int] = sc.parallelize(1 to 10,3) //3 val rdd3: RDD[Int] = sc.makeRDD(1 to 10)//底层是parallelize //8 val rdd4: RDD[Int] = sc.makeRDD(1 to 10,4) //4 //RDD[一行行的数据] val rdd5: RDD[String] = sc.textFile(\u0026#34;data/input/words.txt\u0026#34;)//2 val rdd6: RDD[String] = sc.textFile(\u0026#34;data/input/words.txt\u0026#34;,3)//3 // rdd5.foreach(println) //RDD[一行行的数据] val rdd7: RDD[String] = sc.textFile(\u0026#34;data/input/ratings10\u0026#34;)//10 val rdd8: RDD[String] = sc.textFile(\u0026#34;data/input/ratings10\u0026#34;,3)//10 //RDD[(文件名, 一行行的数据),(文件名, 一行行的数据)....] val rdd9: RDD[(String, String)] = sc.wholeTextFiles(\u0026#34;data/input/ratings10\u0026#34;)//2 val rdd10: RDD[(String, String)] = sc.wholeTextFiles(\u0026#34;data/input/ratings10\u0026#34;,3)//3 // println(\u0026#34;==================\u0026#34;) // rdd10.foreach(println) // println(rdd1.getNumPartitions)//8 //底层partitions.length println(rdd2.partitions.length)//3 println(rdd3.getNumPartitions)//8 println(rdd4.getNumPartitions)//4 println(rdd5.getNumPartitions)//2 println(rdd6.getNumPartitions)//3 println(rdd7.getNumPartitions)//10 println(rdd8.getNumPartitions)//10 println(rdd9.getNumPartitions)//2 println(rdd10.getNumPartitions)//3 //TODO 2.transformation //TODO 3.sink/输出 } } 总结 RDD的创建有3种方式(集合、外部存储、其他RDD)，RDD的数据来源有2种(集合、外部存储) makeRDD底层是调用parallelize，两者本质上是相同的 RDD的getNumPartition方法和RDD的partitions.length返回的值是相等的 在读取大量小文件时应该用wholeTextFiles替代textFile方法 ","date":"2024-11-19T16:27:59+08:00","image":"https://sherlock-lin.github.io/p/spark%E5%AE%9E%E6%88%9801_rdd%E5%88%9B%E5%BB%BA/trees-8686902_1280%202_hu6307041370177207454.jpg","permalink":"https://sherlock-lin.github.io/p/spark%E5%AE%9E%E6%88%9801_rdd%E5%88%9B%E5%BB%BA/","title":"spark实战01_RDD创建"},{"content":"概述 当spark应用被提交到集群上应用时，应用架构包含两个角色\nDriver Program: 申请资源和调度Job执行 Executors: 运行Job中的Task任务和缓存数据 这两个角色都是JVM进程，其关系图如下\nDriver程序运行的位置可以通过–deploy-mode 来指定\nclient:表示Driver运行在提交应用的Client上(默认)\ncluster:表示Driver运行在集群中(Standalone：Worker，YARN：NodeManager的AM所在的Container上)\nClient模式 client模式spark任务提交的流程图如下\n通过以下指令启动spark任务，可以看到在执行指令的命令行输出结果\n1 2 3 4 5 6 7 8 9 10 11 12 bin/spark-submit \\ --master yarn \\ --deploy-mode client \\ --driver-memory 512m \\ --executor-memory 512m \\ --num-executors 1 \\ --total-executor-cores 2 \\ --class org.apache.spark.examples.SparkPi \\ examples/jars/spark-examples_2.12-3.5.3.jar \\ 10 执行语句的命令行输出：Pi is roughly 3.1384751384751386 在Yarn页面可以看到这个任务已经执行完成\ncluster(集群)模式 cluster模式spark任务提交的流程图如下\n以cluster模式执行计算Pi的spark任务\n1 2 3 4 5 6 7 8 9 10 bin/spark-submit \\ --master yarn \\ --deploy-mode cluster \\ --driver-memory 512m \\ --executor-memory 512m \\ --num-executors 1 \\ --total-executor-cores 2 \\ --class org.apache.spark.examples.SparkPi \\ examples/jars/spark-examples_2.12-3.5.3.jar \\ 10 通过yarn页面上可以看到任务已经成功计算完成\n查看计算结果的日志\n通过日志可以看到已经将结果打印出来\n两种模式的流程图 client模式 Driver在任务提交的本地机器上运行，Driver启动后会和ResourceManager通讯申请启动ApplicationMaster 随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster的功能相当于一个ExecutorLaucher，只负责向ResourceManager申请Executor内存； ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程； Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数； 之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行。 cluster模式 Driver在任务提交的本地机器上运行，Driver启动后会和ResourceManager通讯申请启动ApplicationMaster 随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster的功能相当于一个ExecutorLaucher，只负责向ResourceManager申请Executor内存 ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程； Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数； 之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行 两种模式比较 Client模式(了解即可)\nDriver运行在Client上，和集群通信成本高 输出结果会在客户端显示，调试、测试方便 Cluster模式(需要掌握)\nDriver运行在Yarn中，和集群通信成本低 Driver输出结果不能在客户端显示 Driver运行在ApplicationMaster这个节点上由Yarn管理，如果出现问题Yarn会重启ApplicationMaster(Driver) 总结 cluster和client模式最本质的区别是：Driver程序运行在哪里。企业实际生产环境中使用cluster，调试或者简单测试用client Driver是一个JVM Process 进程，编写的Spark应用程序就运行在Driver上，由Driver进程执行； Master(ResourceManager)：是一个JVM Process 进程，主要负责资源的调度和分配，并进行集群的监控等职责； Worker(NodeManager)：是一个JVM Process 进程，一个Worker运行在集群中的一台服务器上，主要负责两个职责，一个是用自己的内存存储RDD的某个或某些partition；另一个是启动其他进程和线程（Executor），对RDD上的partition进行并行的处理和计算。 Executor：是一个JVM Process 进程，一个Worker(NodeManager)上可以运行多个Executor，Executor通过启动多个线程（task）来执行对RDD的partition进行并行计算，也就是执行我们对RDD定义的例如map、flatMap、reduce等算子操作。 ","date":"2024-11-11T19:41:08+08:00","image":"https://sherlock-lin.github.io/p/sparkonyarn%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/image-20241108193357851_hu16324133340311923784.png","permalink":"https://sherlock-lin.github.io/p/sparkonyarn%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/","title":"SparkOnYarn两种模式详解"},{"content":"背景 在mac机器上启动Yarn集群，然后通过flink、spark提交on yarn计算任务失败\n排查 在yarn web页面上看到无存活的NM节点，同时也无分配的cpu和内存 在首页看到报错信息：The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster. 上面两个问题指向了一种可能，那就是NodeManager没有正常启动，查看其日志看到以下报错\n1 2024-09-27 11:09:13,458 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /Users/lin/dev/bigdata/hadoop-3.3.6/hdfs/tmp/nm-local-dir : used space above threshold of 90.0% ] ; 1/1 log-dirs usable space is below configured utilization percentage/no more usable space [ /Users/lin/dev/bigdata/hadoop-3.3.6/logs/userlogs : used space above threshold of 90.0% ] 报错信息很明确，就是NM存放数据所在的磁盘使用率超过90%，而在NM启动时如果判断超过90%则会启动失败\n解决方案一：清理磁盘 第一种思路就是磁盘不够就清理数据\n查询NM报错目录所对应的磁盘使用信息 1 2 3 4 5 6 7 8 df -h /Users/lin/dev/bigdata/hadoop-3.3.6/hdfs/tmp/nm-local-dir //通过输出信息可以看到这块磁盘使用率到达91%，第一列是磁盘信息，最后一列是所挂在的文件目录 Filesystem Size Used Avail Capacity iused ifree %iused Mounted on /dev/disk1s2 466Gi 397Gi 43Gi 91% 3.3M 448M 1% /System/Volumes/Data //也可以通过 df -h 查看所有磁盘和目录的映射关系 查看 /System/Volumes/Data 目录下前十个最占空间的文件夹 1 du -h -d 1 | sort -rh | head -n 10 查找当前磁盘上最占空间的不重要文件进行删除，释放磁盘空间，当这块磁盘使用率降到90%以下时，重新启动NM即可 解决方案二：yarn修改参数【推荐方案】 在yarn-site.xml文件中增加以下配置(必选)\n1 2 3 4 5 6 7 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;99.0\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt; 磁盘的最大使用率。当一块磁盘的使用率超过该值时，就会标记该磁盘处于不健康状态，不再使用该磁盘。默认为 90，即可以使用磁盘 90% 的空间。 \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; 除此之外也可以额外增加以下配置(可选)\n1 2 3 4 5 6 7 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.disk-health-checker.min-healthy-disks\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;0.0\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt; 默认值是0.25，表示25%。NodeManager上最少保证健康磁盘比例，当健康磁盘比例低于该值时，NodeManager不会再接收和启动新的Container \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; 更改完配置后重启NM即可正常使用\n","date":"2024-11-07T23:48:18+08:00","image":"https://sherlock-lin.github.io/p/%E4%B8%80%E6%AC%A1yarn%E6%8E%92%E9%9A%9C%E8%AE%B0%E5%BD%95/dolphin-8949505_1280_hu3278302536827089725.jpg","permalink":"https://sherlock-lin.github.io/p/%E4%B8%80%E6%AC%A1yarn%E6%8E%92%E9%9A%9C%E8%AE%B0%E5%BD%95/","title":"一次Yarn排障记录"},{"content":"一、概述 如今技术层出不穷，组件更是百花齐放，那么如何快速去掌握一门技术/组件呢，可以从以下四个步骤进行\n基础(20%) 实战(40%) 原理(30%) 源码实现(10%) 二、基础 官方文档是一定要去看的，通过官网我们能获得以下明确的信息\n这个组件出现的背景，也就是要用来解决什么场景的问题的 这个组件是通过哪些特性进行解决的 这个组件的概念、以及特定名词的官方定义 这个组件的角色、架构，处理服务的调用流程 以上这些问题要先搞清楚，这些不明确的话在学后面容易云里雾里。这也是为什么建议一开始要从官网入手，因为其他资料像个人博客这种质量参差不齐，容易误导一些概念影响后面的学习\n三、实战 想掌握一个东西最好的方式就是上手用它，前面学习基础也是为了更好、更快的实战 一般官方文档也会引导用户如何下载、安装以及启动，这个过程有两个地方值得注意\n配置 通过观察配置可以看到这个组件的依赖，假如要配置连接mysql的信息，那么就可以推测这个组件是用mysql来存某些数据，例如配置里面有netty相关的信息，那么也可以推测这个组件是用Netty来进行的网络通信 除此之外可以结合官网看下这个组件的核心配置，一般也能了解到这个组件哪些核心功能支持怎样的配置，不同配置之间的差异 管理指令 一般在解压后的bin/sbin目录下，可以结合help指令来看看都支持哪些操作，同是也可以直接打开管理的脚本进去看，通过这个角度能够看到这个组件日常维护的相关工作，这对学习一个新的组件非常有用 在启动服务后，可以结合官方提供的案例进行操作，其中可以从以下几点进行操作\n1. 使用角度 数据写操作，都支持哪几种操作，不同操作有什么不同，写操作有什么核心的优化参数 数据读操作，都支持哪几种操作，不同操作之间有什么不同 官网提供的特性，都玩一下 2. 维护角度 查看这个组件的文件布局方式，常见的操作会导致这些文件如何变化等 官方提供的管理指令都可以执行看看效果 四、原理 在有基础以及实战的经历后，你对这个组件已经有一个认知了。当这个认知还是一个黑盒的，就像是你会用遥控器操纵电视机了，但这背后发生什么你并不清楚，因此咱们需要知道常见操作背后的流程\n1. 使用角度 数据读写操作背后流程是怎么样的 在使用一些特性功能时背后的流程是怎么样的 2. 维护角度 通过“实战阶段”使用的高频管理指令，背后的工作原理 组件自身提供的优化策略，例如定期清理、合并文件等是如何实现的 这个过程中除了官网，还可以通过官网查看一些精品博客、经典书籍等来完善对这块的了解\n五、源码 在理解原理后，如果还想知道具体是如何实现，这个时候就可以下载源码进行阅读，这个时候一定要遵循以下口诀“先整体，再局部；先抽象，再具体”，简单来说就是先看处理请求的脉络，等梳理清后再去看具体某个点的实现，先看api接口的定义， 通过api接口的定义将整个处理流程关联起来，有需要或者有时间时候再去看具体的实现。\n以下有几个可以参考的阅读思路\n单元测试\n场景驱动\n六、总结 以上就是掌握一个组件的流程，这个过程一定要谨记两点，一个是快，另一个是坚持。这个指的是学某个知识点的时候不用追求一次性全部啃透，理解得差不多就可以下一步了，只要你不停的啃，你需要掌握的知识会不停的出现在你的面前，如果在你学习的过程中过去的某个知识点会不停的阻塞你的进度，此时再多花时间去彻底啃透它也来得及，除此之外更重要的就是坚持，只要不停的花时间钻研某一个组件，总有一天你会将它吃透的。\n","date":"2024-11-06T11:13:21+08:00","image":"https://sherlock-lin.github.io/p/%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E6%8E%8C%E6%8F%A1%E4%B8%80%E9%97%A8%E6%8A%80%E6%9C%AF%E7%BB%84%E4%BB%B6/matt-le-SJSpo9hQf7s-unsplash_hu10664154974910995856.jpg","permalink":"https://sherlock-lin.github.io/p/%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E6%8E%8C%E6%8F%A1%E4%B8%80%E9%97%A8%E6%8A%80%E6%9C%AF%E7%BB%84%E4%BB%B6/","title":"如何快速掌握一门技术组件"},{"content":"概述 当读取查询启动时，它首先被发送到查询引擎。引擎利用目录检索最新的元数据文件位置，其中包含有关表的模式和其他元数据文件的关键信息，如最终指向实际数据文件的清单。在此过程中会使用列的统计信息来限制读取文件的数量，这有助于提高查询性能。\n查询请求 通过下面的例子，我们来看看在执行查询时，Apache Iceberg 的各个组件是如何协同工作的。\n1 2 3 SELECT * FROM orders WHERE order_TS between \u0026#39;2023-01-01\u0026#39; and \u0026#39;2023-01-31\u0026#39; 发送查询到计算引擎 发送查询SQL到计算引擎\n检查catalog 计算引擎会向catalog发送请求来查询当前元数据的位置，并读取该文件。如果使用的是Hadoop catalog，则会读取/orders/metadata/version-hint.txt这个文件\n从元数据文件中获取信息 计算引擎读取元数据文件，获取表的schema来为读取数据准备数据结构，然后在读取表的分区方式来了解数据是怎么组织的，同时通过存储的一些统计信息跳过无关的数据文件。同时在该文件中查到manifest list的信息\n从manifest list(清单列表)中获取信息 从元数据文件中获取清单文件路径位置后，计算引擎会读取文件snap-651651264480400836-1-2d939142-c6d2-418e-a49b-b717f9d7c652.avro获取更多信息，最重要的是获取每个快照的manifest files(清单文件) manifest files(清单文件)中记录很多表的信息，例如清单分区列的下限和上限。当引擎决定跳过哪些清单文件以更好地剪切文件时，这些信息非常有用。其他详细信息，如添加/删除的数据文件总数和每个快照添加/删除的行数，也可以在此文件中找到，引擎需要这些信息来获取特定查询的相关数据文件。\n从manifest files(清单文件)中获取信息 Iceberg默认提供分区和基于度量的过滤（列的上下限）等数据和文件优化技术，使引擎可以避免全表扫描，从而大大提高了性能保证。最后，将计算结果返回给用户\n查询引擎与catalog交互，获取当前的元数据文件 会获取该快照和清单列表位置 然后从清单列表中检索清单文件路径 引擎根据清单文件中的分区过滤器确定数据文件路径 然后将所需数据文件中的匹配数据返回给用户 时空旅行查询(Time-Travel Query) 数据库和数据仓库中的一项重要功能是能够回溯到表的特定状态，以查询历史数据（即已更改或删除的数据）。Iceberg为数据湖架构带来了类似的时间旅行功能,这对于分析企业前几个季度的数据、恢复意外删除的行或重现分析结果等情况特别有用。Iceberg 提供了两种运行时间旅行查询的方法：使用时间戳和快照 ID。\n1 2 3 4 5 6 7 8 9 SELECT * FROM hive_prod.default.sample.history; 可以通过时间戳方式查询历史数据 SELECT * FROM orders TIMESTAMP AS OF \u0026#39;2023-03-02 23:32:32.914\u0026#39; 可以通过快照ID查询历史数据 SELECT * FROM orders VERSION AS OF 4342343424244324 发送查询到计算引擎 发送查询SQL到计算引擎 检查catalog 计算引擎会向catalog发送请求来查询当前元数据的位置，并读取该文件。如果使用的是Hadoop catalog，则会读取/orders/metadata/version-hint.txt这个文件 从元数据文件中获取信息 引擎会读取元数据文件以获取表信息。当前元数据文件记录了为冰山表生成的所有快照，除非这些快照作为元数据维护策略已经过期，从可用的快照列表中，引擎会根据时间戳值或快照 ID 确定时间旅行查询中指定的特定快照。 从manifest list(清单列表)中获取信息 从元数据文件中获取清单文件路径位置后，计算引擎会读取文件snap-651651264480400836-1-2d939142-c6d2-418e-a49b-b717f9d7c652.avro获取更多信息，最重要的是获取每个快照的manifest files(清单文件) manifest files(清单文件)中记录很多表的信息，例如清单分区列的下限和上限。当引擎决定跳过哪些清单文件以更好地剪切文件时，这些信息非常有用。其他详细信息，如添加/删除的数据文件总数和每个快照添加/删除的行数，也可以在此文件中找到，引擎需要这些信息来获取特定查询的相关数据文件。 从manifest files(清单文件)中获取信息 Iceberg默认提供分区和基于度量的过滤（列的上下限）等数据和文件优化技术，使引擎可以避免全表扫描，从而大大提高了性能保证。最后，将计算结果返回给用户 查询引擎与目录交互，获取当前的元数据文件（ 然后，它会根据时间旅行查询中提供的时间戳或版本 ID 选择快照，并获取该快照的清单位置 然后从清单列表中检索清单文件路径 引擎根据清单文件中的分区过滤器确定数据文件路径 然后将所需数据文件中的匹配数据返回给用户 ","date":"2024-11-05T21:23:13+08:00","image":"https://sherlock-lin.github.io/p/iceberg%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B/image-20241101160153405_hu18092220350635905077.png","permalink":"https://sherlock-lin.github.io/p/iceberg%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B/","title":"Iceberg数据读取流程"},{"content":"概述 Iceberg的写入流程包括一系列步骤来让计算引擎更高效的写入和更新数据。当一个写入请求被发起时，它会被发送到计算引擎进行语法解析，然后查询目录验证确保数据的一致性和完整性，并根据定义的分区策略将数据写到数据文件和元数据文件，最后更新目录文件来保证后续读取操作读的是最新的数据。\n建表 1 2 3 4 5 6 7 8 //创建分区表 CREATE TABLE hive_prod.default.orders ( order_id bigint, customer_id string, order_amount DECIMAL(10,2), order_ts TIMESTAMP) USING iceberg PARTITIONED BY (HOUR(order_ts)); 发送查询到计算引擎 当执行上述语句时，这个SQL会被发送到Spakr引擎中进行解析，然后根据规则进行表的创建\n写元数据文件 在/user/hive/warehouse 地址根据库表名创建对应的目录，并在此目录下创建元数据文件，元数据文件里面存储表列的数据格式以及表的其他信息\n元数据的格式如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 { \u0026#34;format-version\u0026#34; : 2, \u0026#34;table-uuid\u0026#34; : \u0026#34;b43642c7-a62f-488a-b2b3-b1633336062e\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;hdfs://localhost:9000/user/hive/warehouse/sample\u0026#34;, \u0026#34;last-sequence-number\u0026#34; : 0, \u0026#34;last-updated-ms\u0026#34; : 1730344502743, \u0026#34;last-column-id\u0026#34; : 2, \u0026#34;current-schema-id\u0026#34; : 0, \u0026#34;schemas\u0026#34; : [ { \u0026#34;type\u0026#34; : \u0026#34;struct\u0026#34;, \u0026#34;schema-id\u0026#34; : 0, \u0026#34;fields\u0026#34; : [ { \u0026#34;id\u0026#34; : 1, \u0026#34;name\u0026#34; : \u0026#34;id\u0026#34;, \u0026#34;required\u0026#34; : true, \u0026#34;type\u0026#34; : \u0026#34;long\u0026#34;, \u0026#34;doc\u0026#34; : \u0026#34;unique id\u0026#34; }, { \u0026#34;id\u0026#34; : 2, \u0026#34;name\u0026#34; : \u0026#34;data\u0026#34;, \u0026#34;required\u0026#34; : false, \u0026#34;type\u0026#34; : \u0026#34;string\u0026#34; } ] } ], \u0026#34;default-spec-id\u0026#34; : 0, \u0026#34;partition-specs\u0026#34; : [ { \u0026#34;spec-id\u0026#34; : 0, \u0026#34;fields\u0026#34; : [ ] } ], \u0026#34;last-partition-id\u0026#34; : 999, \u0026#34;default-sort-order-id\u0026#34; : 0, \u0026#34;sort-orders\u0026#34; : [ { \u0026#34;order-id\u0026#34; : 0, \u0026#34;fields\u0026#34; : [ ] } ], \u0026#34;properties\u0026#34; : { \u0026#34;owner\u0026#34; : \u0026#34;lin\u0026#34;, \u0026#34;write.parquet.compression-codec\u0026#34; : \u0026#34;zstd\u0026#34; }, \u0026#34;current-snapshot-id\u0026#34; : -1, \u0026#34;refs\u0026#34; : { }, \u0026#34;snapshots\u0026#34; : [ ], \u0026#34;statistics\u0026#34; : [ ], \u0026#34;partition-statistics\u0026#34; : [ ], \u0026#34;snapshot-log\u0026#34; : [ ], \u0026#34;metadata-log\u0026#34; : [ ] } 这张表目前是空表，这在Iceberg中也叫一个快照，由于目前表中还没有数据，因此此快照不会指向任何的manifest list(清单列表)，因此也没有manifest files(清单文件)\n更新catalog文件并提交变更 最后，引擎会更新当前元数据指针，使其指向version-hint.text文件(存在特殊的目录) 插入查询 现在来向表中插入数据并了解下原理\n1 INSERT INTO hive_prod.default.orders VALUES (123, 456, 36.17, \u0026#39;2023-03-07 08:10:23\u0026#39;); 发送查询给计算引擎 这个SQL会被发送到Spakr引擎中进行解析，然后根据规则进行表的创建\n检查catalog 计算引擎会向catalog发送请求来查询当前元数据的位置，并读取该文件。如果使用的是Hadoop catalog，则会读取/orders/metadata/version-hint.txt这个文件\n写数据文件和元数据文件 在计算引擎知道表的元数据信息，就会开始写一个新的数据文件以及与之对应的元数据文件。然后计算引擎会根据小时进行分区，并在每个分区下将数据按照Parquet格式进行写入(默认为Parquet，但可更改)，数据会在写入数据文件前进行排序。 写入数据文件后，计算引擎会创建一个manifest文件，这个manifest文件会包含引擎写实际数据文件的路径信息，以及统计信息，如列的极值和空值的计算，这对计算引擎在查询时可以提供更高的性能，manifest文件会以.avro的格式存在HDFS上 接下来计算引擎会创建一个manifest list(清单列表)来跟踪manifest files(清单文件)，如果现有的manifest files(清单文件)跟此快照有关联，则这些文件也会被加到新的manifest list(清单列表)中 引擎会将manifest list(清单列表)写到data lake(数据湖)，这个文件里面包含 清单文件的路径、添加或删除的数据文件、行的数量以及分区的统计信息，如分区列的下限和上限。这些信息有助于读取查询排除任何非必要的清单文件，从而加快查询速度。\n更新catalog文件并提交变更 最后，计算引擎会再次访问目录确保在运行此操作时没有其他快照被提交，通过这样操作Iceberg可以保证同时有多个写入时彼此不干扰；假设当前版本在写入者写入前没变，写入完成后计算引擎会原子性提交，将表的元数据文件指针从现有的基础版本切换到新版本v2.metadata.json\nMerge查询 已经存在于目标表中，我们就会更新它。如果不存在，我们就插入新记录。下面是操作语句\n1 2 3 4 5 MERGE INTO orders o USING (SELECT * FROM orders_staging) s ON o.order_id = s.order_id WHEN MATCHED THEN UPDATE SET order_amount = s.order_amount WHEN NOT MATCHED THEN INSERT *; 发送查询给计算引擎 这个SQL会被发送到Spakr引擎中进行解析，然后根据规则进行数据的插入\n检查catalog 计算引擎会向catalog发送请求来查询当前元数据的位置，并读取该文件。如果使用的是Hadoop catalog，则会读取manifest files(清单文件)获取到表元数据存放的地址\n写数据文件和元数据文件 首先计算引擎会同时读取表orders_staging和表orders到内存里来匹配数据，在匹配时会根据表属性的策略 写入时复制（COW）和读取时合并（MOR）来做不同的事情。 简单来说使用COW策略时，每当更新表时，任何与相关记录相关联的数据文件都将重写为新的数据文件。而使用MOR时，数据文件不会被重写，相反，会生成新的删除 文件来跟踪变化。 在本次例子中，表中只有一条记录会匹配到。不过即使所有的条件不匹配，引擎仍会复制所有这些记录，只有匹配的记录才会被更新，而不匹配的记录会被写入一个独立文件。这是写入策略造成的\n写入数据文件后，计算引擎会创建一个新的manifest files(清单文件)，其中包含对这两个表对应的数据文件的地址；除此之外manifest files(清单文件)还包含这些数据文件的各种统计信息，例如列的上下限和值计数\n更新catalog文件并提交变更 最后，计算引擎会检查确保没有写入冲突，然后在manifest list(清单列表)中变更地址指向最新的元数据文件 简单来说Iceberg有以下四个文件\n元数据文件 metadata/00001-6dc53288-8cce-4d7c-8d61-fcc78ce7b88e.metadata.json manifest files(清单文件) metadata/2d939142-c6d2-418e-a49b-b717f9d7c652-m0.avro manifest list(清单列表) metadata/snap-651651264480400836-1-2d939142-c6d2-418e-a49b-b717f9d7c652.avro 数据文件 data/00000-1-62a6fb6b-96c1-4309-9308-9da40a1cd2e1-0-00001.parquet ","date":"2024-11-05T21:17:40+08:00","image":"https://sherlock-lin.github.io/p/iceberg%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/image-20241105212013545_hu9937817988410789217.png","permalink":"https://sherlock-lin.github.io/p/iceberg%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/","title":"Iceberg数据写入流程"},{"content":"架构与框架 架构的维基百科定义如下\n软件架构是指软件系统的“基础结构”，创造这些基础结构的准则，以及对这些结构的描述。\n框架的维基百科定义如下\n软件框架（Software framework）通常指的是为了实现某个业界标准或完成特定基本任务的软件组件规范，也指为了实现某个软件组件规范时，提供规范所要求之基础功能的软件产品。\n框架是和架构比较相似的概念，且两者有较强的关联关系，所以在实际工作中，这两个概念有时我们容易分不清楚。单纯从定义的角度来看，框架和架构的区别还是比较明显的：框架关注的是“规范”，架构关注的是“结构”。\n框架是一整套开发规范，架构是某一套开发规范下的具体落地方案，包括各个模块之间的组合关系以及它们协同起来完成功能的运作规则。\n业务逻辑分解 从业务逻辑的角度分解，“学生管理系统”的架构是\n登陆注册模块 个人信息模块 个人成绩模块 物理部署分解 从物理部署的角度分解，“学生管理系统”的架构是：\nNginx Web服务器 MySQL 开发规范角度分解 从开发规范的角度分解，“学生管理系统”可以采用标准的MVC框架来开发，因此架构又变成了MVC架构：\nController Model View 系统与子系统 系统的定义如下\n系统泛指由一群有关联的个体组成，根据某种规则运作，能完成个别元件不能单独完成的工作的群体。它的意思是“总体”“整体”或“联盟”。\n其中的关键字为\n关联：系统是由一群有关联的个体组成的，没有关联的个体堆在一起不能成为一个系统。 规则：系统内的个体需要按照指定的规则运作，而不是单个个体各自为政。 能力：系统能力与个体能力有本质的差别，系统能力不是个体能力之和，而是产生了新的能力。 子系统也是由一群有关联的个体所组成的系统，多半会是更大系统中的一部分。\n子系统的定义和系统定义是一样的，只是观察的角度有差异，一个系统可能是另外一个更大系统的子系统。因此一个系统的架构，只包括顶层这一个层级的架构，而不包括下属子系统层级的架构。\n模块与组件 软件模块（Module）是一套一致而互相有紧密关连的软件组织。它分别包含了程序和数据结构两部分。现代软件开发往往利用模块作为合成的单位。模块的接口表达了由该模块提供的功能和调用它时所需的元素。模块是可能分开被编写的单位。这使它们可再用和允许人员同时协作、编写及研究不同的模块。 软件组件定义为自包含的、可编程的、可重用的、与语言无关的软件单元，软件组件可以很容易被用于组装应用程序中。\n可以看到模块和组件都是系统的组成部分，只是从不同的角度拆分系统而已。从业务逻辑的角度来拆分系统后，得到的单元就是“模块”；从物理部署的角度来拆分系统后，得到的单元就是“组件”。划分模块的主要目的是职责分离；划分组件的主要目的是单元复用。\n如果你是业务系统的架构师，首先需要思考怎么从业务逻辑的角度把系统拆分成一个个模块角色，其次需要思考怎么从物理部署的角度把系统拆分成组件角色，例如选择MySQL作为存储系统。但是对于MySQL内部的体系架构（Parser、Optimizer、Caches\u0026amp;Buffers和Storage Engines等），你其实是可以不用关注的，也不需要在你的业务系统架构中展现这些内容。\n重新定义架构：4R架构 软件架构指软件系统的顶层（Rank）结构，它定义了系统由哪些角色（Role）组成，角色之间的关系（Relation）和运作规则（Rule）。\n因为这个定义中的4个关键词，都可以用R开头的英文单词来表示，分别是Rank、Role、Relation和Rule，所以我把定义简称为“4R架构定义”，每个R的详细解释如下。\n第一个R，Rank。它是指软件架构是分层的，对应“系统”和“子系统”的分层关系。通常情况下，我们只需要关注某一层的架构，最多展示相邻两层的架构，而不需要把每一层的架构全部糅杂在一起。无论是架构设计还是画架构图，都应该采取“自顶向下，逐步细化”的方式。\n第二个R，Role。它是指软件系统包含哪些角色，每个角色都会负责系统的一部分功能。架构设计最重要的工作之一就是将系统拆分为多个角色。最常见的微服务拆分其实就是将整体复杂的业务系统按照业务领域的方式，拆分为多个微服务，每个微服务就是系统的一个角色。\n第三个R，Relation。它是指软件系统的角色之间的关系，对应到架构图中其实就是连接线，角色之间的关系不能乱连，任何关系最后都需要代码来实现，包括连接方式（HTTP、TCP、UDP和串口等）、数据协议（JSON、XML和二进制等）以及具体的接口等。\n第四个R，Rule。它是指软件系统角色之间如何协作来完成系统功能。我们在前面解读什么是“系统”的时候提到过：系统能力不是个体能力之和，而是产生了新的能力。那么这个新能力具体如何完成的呢？具体哪些角色参与了这个新能力呢？这就是Rule所要表达的内容。在架构设计的时候，核心的业务场景都需要设计Rule。\n在实际工作中，为了方便理解，Rank、Role和Relation是通过系统架构图来展示的，而Rule是通过系统序列图（System Sequence Diagram）来展示的。\n总结 架构师更关注整体，高级程序员更关注实现逻辑，因此高级程序员如果有更高的追求，或者希望所负责的软件变得更好，那么在设计程序时，应该也要站在架构师的视角俯瞰全局，因此学习并掌握架构相关的知识是很有必要的。\n","date":"2024-10-30T11:22:35+08:00","image":"https://sherlock-lin.github.io/p/%E4%BB%80%E4%B9%88%E6%98%AF%E6%9E%B6%E6%9E%84/image-20241030110801229_hu8072303843774860098.png","permalink":"https://sherlock-lin.github.io/p/%E4%BB%80%E4%B9%88%E6%98%AF%E6%9E%B6%E6%9E%84/","title":"什么是架构"},{"content":"概述 基于 PulsarClient源码解析 可知，无论是生产者还是消费者在启动时，都会跟服务端建立好TCP网络连接，现在就从服务端视角看来这之后跟消费者发生的数据消费流程，如下\n消费者客户端在启动时，会通过之间建立好的TCP连接向服务端发送订阅请求 服务端Broker接收到请求后，如果发现当前订阅的Topic没创建则创建，如果已创建则为当前消费者客户端创建对应的Consumer对象，这个Consumer对象是客户端在Broker端的“代理人”，同时会给这个Consumer对象分配唯一ID 客户端向服务端发起拉取数据请求，发起流程可看 消费者工作流程源码解析 Consumer调用Dispatcher进行数据拉取 Dispatcher尝试从Broker缓存中命中数据，命中直接返回，否则继续往下 通过BK客户端请求Bookeeper进行数据的查询 将数据通过TCP连接写到客户端 以上就是服务端处理消费者请求的流程，接下来就从源码角度看看实现\n添加订阅 在消费者启动时会通过Netty客户端向服务端Broker发送订阅类型的TCP请求，服务端解析后会调用handleSubscribe方法进行处理\n在1248行会获取当前Topic对应的Topic对象(如果之前还没创建则会进行创建)，然后会在1289行调用此Topic对象订阅这个消费者对象\n继续往下跟踪\n在962行根据消费者提供的信息，在服务端创建相对应的Consumer对象(作为客户端消费者的代理)，并将此消费者添加到订阅列表中\n将Consumer对象添加到Subscription列表中\n根据订阅类型在服务端创建相对应的Dispatcher对象，这个对象负责派发Topic的数据给其对应的消费者\n初始化Dispatcher对象\n处理消费请求流程 在消费者启动时会通过Netty客户端向服务端Broker发送拉取数据的TCP请求，服务端解析后会调用handleFlow方法进行处理\n根据Consumer ID获取对应的消费者对象，通过这个Consumer处理数据拉取的逻辑\n做些参数判断，核心在于调用Subscription处理\nSubscription内部是调用Dispatcher进行处理，这个Dispatcher对象就是消费者订阅阶段创建的那个\n进行异步处理，提升服务端处理请求的并发能力\n加锁进行数据的读取\n这个方法虽然内容较多，但是关键在于365行通过游标对象读取数据，同时这里封装了第一层回调逻辑\n继续往下跟踪\n进行数据的异步读取\n封装第二层回调逻辑，并从这里继续进入读取数据\n继续往下跟踪\n继续往下跟踪\n先尝试从缓存中读取数据，同时这里封装第三层回调\n一路跟下去直到RangeEntryCacheImpl的asyncReadEntry方法，这里是尝试从堆外跳表中来命中缓存\n总结 以上就是服务端处理消费请求的全部了，可以看到这里面用了大量的异步回调，优点是在很多步骤避免了阻塞等待，大幅提升并发处理能力，而缺点就是阅读成本也上升了，如果不梳理清楚很容易看得云里雾里。\n","date":"2024-10-23T16:35:50+08:00","image":"https://sherlock-lin.github.io/p/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%A4%84%E7%90%86%E6%B6%88%E8%B4%B9%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/image-20241023153153940_hu13590278494809784595.png","permalink":"https://sherlock-lin.github.io/p/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%A4%84%E7%90%86%E6%B6%88%E8%B4%B9%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","title":"服务端处理消费请求流程源码解析"},{"content":"简析 Pulsar客户端消费数据流程如下\n用户程序循环调用Pulsar客户端对象Consumer的receive获取数据，进行业务处理 Consumer对象从队列中读取数据 在符合条件时，调用Netty客户端对象ClientCnt向Broker发送拉取数据请求 Consumer对象将从队列中读取到的数据返回给用户 拉取请求异步到达Broker Pulsar服务端将数据通过TCP传输给Netty客户端对象ClientCnt ClientCnt调用Consumer对象处理数据 Consumer对象将数据放到队列中 可以看到Pulsar通过队列将消费者工作流程拆成了两个阶段，这两个阶段并行工作\n消费数据阶段(1~5)：循环读取队列中的数据\n拉取数据阶段(6~8)：1. 在消费数据时根据条件触发拉取数据 2. 拉取数据时根据条件再次触发下次拉取数据\n从全局视角可以看到这是一个流水线pipeline阶段，用户在消费上一批数据时，下一批数据也在源源不断的写到内部队列中，避免了消费阶段等待拉取阶段的耗时，从而达到高效消费数据的效果。\n消费者工作流程 消费数据 对于用户来说，只要启动消费者程序，在34行就会源源不断有数据过来，因此从这里作为入口来探索Pulsar背后做了哪些事情\n做些简单的检查，核心逻辑在internalReceive方法，继续往下跟\nincomingMessages是一个队列，消费者会循环从这个队列中获取数据返回给用户。这里的核心思路在messageProcessed方法\nmessageProcessed方法中最重要的就是1797行，其他的逻辑基本都是一些相关指标的变化\n继续往下跟踪\ndelta是用来做循环重试请求次数，继续往下看\nsendFlowPermitsToBroker就是核心方法了，它通过TCP向Broker请求获取更多的数据\n接收服务端数据 ClientCnx是Netty客户端，在Broker往消费者写数据时，就是通过与ClientCnx完成的\n注：消费者在启动时就已经创建好ClientCnx对象，也就是跟服务端建立好了TCP连接，因此这里只需要接收数据就好了\n这个方法很长，但是大部分都是数据解析、相关指标的变化，核心方法就是1486行\n循环接收到的数据集合，并挨个调用executeNotifyCallback进行处理\n数据被放到线程池里进行异步处理，将接收和处理进行分离，提升接收数据的性能\n可以看到将服务端写来的数据放到incomingMessages队列中，到此消费者工作流程完成闭环\n消费者创建流程 看完了重要的部分，再回过头来简单看看消费者创建的流程，继续从消费数据case出发，消费者的创建是在28行\n客户端的创建用的是构造器模式，后续几个链式赋值就不用需看了，直接看最后的subscribe方法\n这个方法最终也会调用异步订阅并阻塞等待\n构造对象的subscribeAsync方法最终还是会调用Pulsar客户端对象的订阅逻辑\n如果是指定一个Topic的话，则走554逻辑，否则走563逻辑\n继续往下跟踪\n这里的getPartitionedTopicMetadata方法会通过HTTP向Broker获取这个Topic的分区情况，并根据结果来决定创建哪个消费者对象\n注：即便是多分区的情况，使用MultiTopicsConsumerImpl来处理，MultiTopicsConsumerImpl内部其实也是维护Consumer集合，也就是将一个Topic的多个分区当作多个Topic来进行消费处理\n继续往下跟踪\n继续往下\nConsumerImpl的构造方法内容太多，里面需要关心的就是 grabCnx 这个方法，这个方法一直往下跟踪，可以看到以下方法，就是跟Broker创建TCP连接，这里如果感兴趣的话可以看另外一篇文章 PulsarClient源码解析\n","date":"2024-10-22T19:18:00+08:00","image":"https://sherlock-lin.github.io/p/%E6%B6%88%E8%B4%B9%E8%80%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/image-20241022181620949_hu13618359552460518029.png","permalink":"https://sherlock-lin.github.io/p/%E6%B6%88%E8%B4%B9%E8%80%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","title":"消费者工作流程源码解析"},{"content":"Map Map算子是获取一个元素并生成一个元素，它的输入流是DataStream，通过Map算子后返回的类型是SingleOutputStreamOperator\n下面将每一个字符串元素转为整型并乘于2\n1 2 streamSource.map((MapFunction\u0026lt;String, Integer\u0026gt;) value -\u0026gt; Integer.valueOf(value) * 2) .print(); Filter Filter算子会对每个元素就行计算，计算结果一定要是布尔值，返回true的元素保留，返回false的元素丢弃。\n下面将丢弃长度超过5的元素\n1 streamSource.filter((FilterFunction\u0026lt;String\u0026gt;) value -\u0026gt; value.length()\u0026lt;=5) FlatMap FlatMap算子是获取一个元素并生成多个元素\n下面就是将每个元素根据空格进行切分，并将切分后的多个结果构造成value为1的kv结构进行返回\n1 2 3 4 5 6 7 8 streamSource.flatMap( (String value, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) -\u0026gt; { String []words = value.split(\u0026#34; \u0026#34;); for (String word : words) { out.collect(Tuple2.of(word, 1)); } } ) KeyBy KeyBy算子是根据指定字段对流进行分区，相同的Key会被分到同一个分区。返回的结果是KeyedStream类型\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 //TODO 根据指定的字段进行分组(这里指定中间的字段) streamSource.flatMap(new FlatMapFunction\u0026lt;String, Tuple3\u0026lt;String, String, Integer\u0026gt;\u0026gt;() { @Override public void flatMap(String value, Collector\u0026lt;Tuple3\u0026lt;String, String, Integer\u0026gt;\u0026gt; out) throws Exception { String[] strings = value.split(\u0026#34; \u0026#34;); out.collect(Tuple3.of(strings[0], strings[1], Integer.valueOf(strings[2]))); } }).keyBy(new KeySelector\u0026lt;Tuple3\u0026lt;String, String, Integer\u0026gt;, String\u0026gt;() { //根据指定字段进行分组 @Override public String getKey(Tuple3\u0026lt;String, String, Integer\u0026gt; value) throws Exception { return value.f1; } }).sum(2) .print(); Reduce Reduce返回单个结果值，并且Reduce每处理一个元素总是创建一个新的值。常用的方法有average、sum、min、max、count，这些通过Reduce都可以实现，除此之外还可以通过它自定义任意的聚合逻辑。但是Reduce只能操作KeyedStream对象，一般会先通过keyby处理数据流并获得KeyedStream对象，再针对分区后的结果进行Reduce做相对应的聚合操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 streamSource.flatMap((FlatMapFunction\u0026lt;String, Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;) (value, out) -\u0026gt; { String[] strings = value.split(\u0026#34; \u0026#34;); out.collect(Tuple2.of(strings[0], Integer.valueOf(strings[1]))); }).returns(Types.TUPLE(Types.STRING, Types.INT)) .keyBy(new KeySelector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;, String\u0026gt;() { @Override public String getKey(Tuple2\u0026lt;String, Integer\u0026gt; value) throws Exception { return value.f0; } }).reduce(new ReduceFunction\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;() { @Override public Tuple2\u0026lt;String, Integer\u0026gt; reduce(Tuple2\u0026lt;String, Integer\u0026gt; value1, Tuple2\u0026lt;String, Integer\u0026gt; value2) throws Exception { value1.f1 = value1.f1+value2.f1; return value1; } }).print(); Aggregations DataStream API支持多种聚合，例如min、max、sum等，这些函数可以用于KeyedStream来获得聚合结果\n1 2 3 4 5 6 KeyedStream.sum(0) KeyedStream.min(0) KeyedStream.max(0) //max和maxBy的区别是，max返回流中的最大值，maxBy返回具有最大值的键 KeyedStream.maxBy(0) KeyedStream.minBy(0) 以 KeyedStream.sum(0) 为例子，其将流中的key的值进行累加；KeyedStream.sum(1) 是将流中的value的值进行累加。\nWindow Window函数允许按一定条件对现有KeyedStream进行分组，下面是10秒的时间窗口聚合\n1 stream.keyBy(0).window(SlidingEventTimeWindows.of(Time.seconds(60),Time.seconds(10))) WindowAll WindowAll 将元素按照某种特性聚集在一起，这个函数不支持并行操作，所以使用这个算子的话需要注意性能\n1 stream.keyBy(0).windowAll(SlidingEventTimeWindows.of(Time.seconds(60),Time.seconds(10))) Union Union算子将两个或多个数据流组合在一起，这样后面在使用的时候就只需要使用一个数据流就可以了，需要注意的是做合并的流的数据类型必须都一致\n1 DataStream\u0026lt;Integer\u0026gt; union = source1.union(source2, source3.map(Integer::valueOf)); Window Join 1 source1.join(source2).where(0).equalTo(1).window(Time.seconds(5)).apply(new JoinFunction\u0026lt;\u0026gt;() {}) getSideOutput 通过侧输出流实现分流效果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 OutputTag\u0026lt;WaterSensor\u0026gt; s1Tag = new OutputTag\u0026lt;\u0026gt;(\u0026#34;s1\u0026#34;, Types.POJO(WaterSensor.class)); SingleOutputStreamOperator\u0026lt;Object\u0026gt; process = sensorDS.process(new ProcessFunction\u0026lt;WaterSensor, Object\u0026gt;() { @Override public void processElement(WaterSensor value, ProcessFunction\u0026lt;WaterSensor, Object\u0026gt;.Context ctx, Collector\u0026lt;Object\u0026gt; out) throws Exception { String id = value.getId(); if (\u0026#34;s1\u0026#34;.equals(id)) { //TODO 如果是s1，则放到s1侧输出流即可 ctx.output(s1Tag, value); } else if (\u0026#34;s2\u0026#34;.equals(id)) { //TODO 如果是s2，则放到s2侧输出流即可 ctx.output(s2Tag, value); } else { //TODO 其余的放到主流中 out.collect(value); } } }); SideOutputDataStream\u0026lt;WaterSensor\u0026gt; s1sSdeOutput = process.getSideOutput(s1Tag); wordcount(无界流处理) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 public class WordCountStreamUnBoundDemo { public static void main(String[] args) throws Exception { //TODO 1. 获取执行环境(支持本地调试环境) StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment(); executionEnvironment.setParallelism(3); //TODO 2. 创建监听TCP端口的数据流 DataStreamSource\u0026lt;String\u0026gt; streamSource = executionEnvironment.socketTextStream(\u0026#34;localhost\u0026#34;, 7777); //TODO 3. 对数据集进行分割统计操作 streamSource.flatMap( (String value, Collector\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; out) -\u0026gt; { String []words = value.split(\u0026#34; \u0026#34;); for (String word : words) { out.collect(Tuple2.of(word, 1)); } } ) .setParallelism(2) .returns(Types.TUPLE(Types.STRING, Types.INT)) //TODO 4. 对结果进行分组 .keyBy(value -\u0026gt; value.f0) //TODO 5. 累加 .sum(1) //TODO 6. 打印结果 .print(); //TODO 7. 启动任务 executionEnvironment.execute(); } } 参考 官方文档 ","date":"2024-10-22T14:35:00+08:00","image":"https://sherlock-lin.github.io/p/%E5%9F%BA%E7%A1%80%E7%AE%97%E5%AD%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/gull-8151932_1280_hu14164449129448124008.jpg","permalink":"https://sherlock-lin.github.io/p/%E5%9F%BA%E7%A1%80%E7%AE%97%E5%AD%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"基础算子学习笔记"},{"content":"引言 KeyedProcessFunction是Flink用于处理KeyedStream的数据集合，它比ProcessFunction拥有更多特性，例如状态处理和定时器功能等。接下来就一起来了解下这个函数吧\n正文 了解一个函数怎么用最权威的地方就是 官方文档 以及注解，KeyedProcessFunction的注解如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 /** * A keyed function that processes elements of a stream. * * \u0026lt;p\u0026gt;For every element in the input stream {@link #processElement(Object, Context, Collector)} is * invoked. This can produce zero or more elements as output. Implementations can also query the * time and set timers through the provided {@link Context}. For firing timers {@link #onTimer(long, * OnTimerContext, Collector)} will be invoked. This can again produce zero or more elements as * output and register further timers. * * \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;NOTE:\u0026lt;/b\u0026gt; Access to keyed state and timers (which are also scoped to a key) is only * available if the {@code KeyedProcessFunction} is applied on a {@code KeyedStream}. * * \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;NOTE:\u0026lt;/b\u0026gt; A {@code KeyedProcessFunction} is always a {@link * org.apache.flink.api.common.functions.RichFunction}. Therefore, access to the {@link * org.apache.flink.api.common.functions.RuntimeContext} is always available and setup and teardown * methods can be implemented. See {@link * org.apache.flink.api.common.functions.RichFunction#open(org.apache.flink.configuration.Configuration)} * and {@link org.apache.flink.api.common.functions.RichFunction#close()}. */ 上面简单来说就是以下四点\nFlink中输入流中的每一条数据都会触发KeyedProcessFunction类的processElement方法调用 通过这个方法的Context参数可以设置定时器，在开启定时器后会程序会定时调用onTimer方法 由于KeyedProcessFunction实现了RichFunction接口，因此是可以通过RuntimeContext上下文对象管理状态state的开启和释放 需要注意的是，只有在KeyedStream里才能够访问state和定时器，通俗点来说就是这个函数要用在keyBy这个函数的后面 processElement方法解析\nFlink会调用processElement方法处理输入流中的每一条数据 KeyedProcessFunction.Context参数可以用来读取以及更新内部状态state 这个KeyedProcessFunction跟其他function一样通过参数中的Collector对象以回写的方式返回数据 onTimer方法解析：在启用TimerService服务时会定时触发此方法，一般会在processElement方法中开启TimerService服务\n以上就是这个函数的基本知识，接下来就通过实战来熟悉下它的使用\n实战简介 本次实战的目标是学习KeyedProcessFunction，内容如下：\n监听本机7777端口读取字符串 将每个字符串用空格分隔，转成Tuple2实例，f0是分隔后的单词，f1等于1 将Tuple2实例集合通过f0字段分区，得到KeyedStream KeyedSteam通过自定义KeyedProcessFunction处理 自定义KeyedProcessFunction的作用，是记录每个单词最新一次出现的时间，然后建一个十秒的定时器进行触发 使用代码例子 首先定义pojo类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 public class CountWithTimestampNew { private String key; private long count; private long lastQuestTimestamp; public long getAndIncrementCount() { return ++count; } public String getKey() { return key; } public void setKey(String key) { this.key = key; } public long getCount() { return count; } public void setCount(long count) { this.count = count; } public long getLastQuestTimestamp() { return lastQuestTimestamp; } public void setLastQuestTimestamp(long lastQuestTimestamp) { this.lastQuestTimestamp = lastQuestTimestamp; } } 接着实现KeyedProcessFunction类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 public class CountWithTimeoutKeyProcessFunctionNew extends KeyedProcessFunction\u0026lt;Tuple, Tuple2\u0026lt;String, Integer\u0026gt;, Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; { private ValueState\u0026lt;CountWithTimestampNew\u0026gt; state; @Override public void open(Configuration parameters) throws Exception { state = getRuntimeContext().getState(new ValueStateDescriptor\u0026lt;CountWithTimestampNew\u0026gt;(\u0026#34;sherlock-state\u0026#34;, CountWithTimestampNew.class)); } // 实现数据处理逻辑的地方 @Override public void processElement(Tuple2\u0026lt;String, Integer\u0026gt; value, Context ctx, Collector\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; out) throws Exception { Tuple currentKey = ctx.getCurrentKey(); CountWithTimestampNew countWithTimestampNew = state.value(); if (countWithTimestampNew == null) { countWithTimestampNew = new CountWithTimestampNew(); countWithTimestampNew.setKey(value.f0); } countWithTimestampNew.getAndIncrementCount(); //更新这个单词最后一次出现的时间 countWithTimestampNew.setLastQuestTimestamp(ctx.timestamp()); //单词之间不会互相覆盖吗？推测state对象是跟key绑定，针对每一个不同的key KeyedProcessFunction会创建其对应的state对象 state.update(countWithTimestampNew); //给当前单词创建定时器，十秒后触发 long timer = countWithTimestampNew.getLastQuestTimestamp()+10000; //尝试注释掉看看是否还会触发onTimer方法 ctx.timerService().registerProcessingTimeTimer(timer); //打印所有信息，用于确保数据准确性 System.out.println(String.format(\u0026#34; 触发processElement方法，当前的key是 %s, 这个单词累加次数是 %d, 上次请求的时间是：%s, timer的时间是: %s\u0026#34;, currentKey.getField(0), countWithTimestampNew.getCount(), time(countWithTimestampNew.getLastQuestTimestamp()), time(timer) )); } @Override public void onTimer(long timestamp, OnTimerContext ctx, Collector\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; out) throws Exception { Tuple currentKey = ctx.getCurrentKey(); CountWithTimestampNew countWithTimestampNew = state.value(); //标记当前元素是否已经连续10s未出现 boolean isTimeout = false; if (timestamp \u0026gt;= countWithTimestampNew.getLastQuestTimestamp()+10000 ) { //out.collect(new Tuple2\u0026lt;\u0026gt;(countWithTimestampNew.getKey(), countWithTimestampNew.getCount())); isTimeout = true; } //打印所有信息，用于确保数据准确性 System.out.println(String.format(\u0026#34; 触发onTimer方法，当前的key是 %s, 这个单词累加次数是 %d, 上次请求的时间是：%s, timer的时间是: %s, 当前单词是否已超过10秒没有再请求: %s\u0026#34;, currentKey.getField(0), countWithTimestampNew.getCount(), time(countWithTimestampNew.getLastQuestTimestamp()), time(timestamp), String.valueOf(isTimeout) )); } public static String time(long timeStamp) { return new SimpleDateFormat(\u0026#34;yyyy-MM-dd hh:mm:ss\u0026#34;).format(new Date(timeStamp)); } } 最后是启动类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 public class KeyedProcessFunctionDemo2 { public static void main(String[] args) throws Exception { final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 并行度1 env.setParallelism(1); // 处理时间 env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); // 监听本地9999端口，读取字符串 DataStream\u0026lt;String\u0026gt; socketDataStream = env.socketTextStream(\u0026#34;localhost\u0026#34;, 7777); // 所有输入的单词，如果超过10秒没有再次出现，都可以通过CountWithTimeoutFunction得到 DataStream\u0026lt;Tuple2\u0026lt;String, Long\u0026gt;\u0026gt; timeOutWord = socketDataStream // 对收到的字符串用空格做分割，得到多个单词 .flatMap(new SplitterFlatMapFunction()) // 设置时间戳分配器，用当前时间作为时间戳 .assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt;() { @Override public long extractTimestamp(Tuple2\u0026lt;String, Integer\u0026gt; element, long previousElementTimestamp) { // 使用当前系统时间作为时间戳 return System.currentTimeMillis(); } @Override public Watermark getCurrentWatermark() { // 本例不需要watermark，返回null return null; } }) // 将单词作为key分区 .keyBy(0) // 按单词分区后的数据，交给自定义KeyedProcessFunction处理 .process(new CountWithTimeoutKeyProcessFunctionNew()); // 所有输入的单词，如果超过10秒没有再次出现，就在此打印出来 timeOutWord.print(); env.execute(\u0026#34;ProcessFunction demo : KeyedProcessFunction\u0026#34;); } } 演示 在启动服务前，先通过linux指令监听端口 nc -lk 7777\n启动Flink服务后，往7777端口里面发送数据\n通过IDEA的终端可以看到有日志输出，可以看到在发送消息的时候第一条日志立马打印出来并在10秒后输出第二条日志\n那么咱们尝试连续发送两条Hello呢，可以看到累加器会持续累加，并且会触发两次onTimer方法，也就是每一条消息都会触发一次。由于连续发送两条，因此可以看得到第三行日志的末尾是false，说明收到第一条后的10秒内又有相同的消息进来。第二条是ture说明在收到第二条消息后的10秒内没有消息进来\n再输入点其他的试试\n通过输出可以看到这些单词的计数器又从0开始，说明每一个Key都对应一个状态\n参考资料 https://blog.csdn.net/boling_cavalry/article/details/106299167 https://blog.csdn.net/lujisen/article/details/105510532 https://blog.csdn.net/qq_31866793/article/details/102831731\n","date":"2024-10-21T16:44:35+08:00","image":"https://sherlock-lin.github.io/p/%E4%B8%80%E6%96%87%E5%BC%84%E6%98%8E%E7%99%BDkeyedprocessfunction%E5%87%BD%E6%95%B0/grebe-7972183_1280_hu8909851503123052803.jpg","permalink":"https://sherlock-lin.github.io/p/%E4%B8%80%E6%96%87%E5%BC%84%E6%98%8E%E7%99%BDkeyedprocessfunction%E5%87%BD%E6%95%B0/","title":"一文弄明白KeyedProcessFunction函数"},{"content":"基础 数据定义语言是SQL语言中对数据库内部的对象结构进行创建、修改、删除等操作的语句，但不会涉及到表内部数据的变更\n完整建表语法树如下\n注意事项\n蓝色字体是建表语法的关键字，用于指定某些功能。\n**[]**中括号的语法表示可选。\n**|**表示使用的时候，左右语法二选一。\n建表语句中的语法顺序要和上述语法规则保持一致。\n建表练习 普通类型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 --创建数据库并切换使用 create database if not exists sherlock; use sherlockdb; --ddl create table create table t_archer( id int comment \u0026#34;ID\u0026#34;, name string comment \u0026#34;英雄名称\u0026#34;, hp_max int comment \u0026#34;最大生命\u0026#34;, mp_max int comment \u0026#34;最大法力\u0026#34;, attack_max int comment \u0026#34;最高物攻\u0026#34;, defense_max int comment \u0026#34;最大物防\u0026#34;, attack_range string comment \u0026#34;攻击范围\u0026#34;, role_main string comment \u0026#34;主要定位\u0026#34;, role_assist string comment \u0026#34;次要定位\u0026#34; ) comment \u0026#34;王者荣耀射手信息\u0026#34; row format delimited fields terminated by \u0026#34;\\t\u0026#34;; select * from t_archer; desc formatted t_archer; 执行完建表语句，可以在HDFS上看到 /user/hive/warehouse/sherlockdb.db下创建了t_archer目录\n在本地创建archer.txt文件，内容如下\n1 2 3 4 5 6 7 8 9 10 1\t后羿\t5986\t1784\t396\t336\tremotely\tarcher 2\t马可波罗\t5584\t200\t362\t344\tremotely\tarcher 3\t鲁班七号\t5989\t1756\t400\t323\tremotely\tarcher 4\t李元芳\t5725\t1770\t396\t340\tremotely\tarcher 5\t孙尚香\t6014\t1756\t411\t346\tremotely\tarcher 6\t黄忠\t5898\t1784\t403\t319\tremotely\tarcher 7\t狄仁杰\t5710\t1770\t376\t338\tremotely\tarcher 8\t虞姬\t5669\t1770\t407\t329\tremotely\tarcher 9\t成吉思汗\t5799\t1742\t394\t329\tremotely\tarcher 10\t百里守约\t5611\t1784\t410\t329\tremotely\tarcher\tassassin 将archer.txt文件上传到HDFS /user/hive/warehouse/sherlockdb.db/t_archer这个目录下\n执行表查询，可以看到数据都已正常导入\n可以看到Hive这种数据映射导入方式比MySQL的insert效率高很多\n复杂类型 1 2 3 4 5 6 7 8 9 10 create table t_hot_hero_skin_price( id int, name string, win_rate int, skin_price map\u0026lt;string,int\u0026gt; ) row format delimited fields terminated by \u0026#39;,\u0026#39; --字段之间分隔符 collection items terminated by \u0026#39;-\u0026#39; --集合元素之间分隔符 map keys terminated by \u0026#39;:\u0026#39;; --集合元素kv之间分隔符; 跟上面步骤一样\n执行建表语句\n本地创建 hot_hero_skin_price.txt 文件\n1 2 3 4 5 1,孙悟空,53,西部大镖客:288-大圣娶亲:888-全息碎片:0-至尊宝:888-地狱火:1688 2,鲁班七号,54,木偶奇遇记:288-福禄兄弟:288-黑桃队长:60-电玩小子:2288-星空梦想:0 3,后裔,53,精灵王:288-阿尔法小队:588-辉光之辰:888-黄金射手座:1688-如梦令:1314 4,铠,52,龙域领主:288-曙光守护者:1776 5,韩信,52,飞衡:1788-逐梦之影:888-白龙吟:1188-教廷特使:0-街头霸王:888 将 hot_hero_skin_price.txt 文件上传到HDFS /user/hive/warehouse/sherlockdb.db/t_hot_hero_skin_price这个目录下\n执行表查询，可以看到数据都已正常导入\n使用默认分隔符 1 2 3 4 5 6 7 create table t_team_ace_player( id int, team_name string, ace_player_name string ); --没有指定row format语句 此时采用的是默认的\\001作为字段的分隔符 select * from t_team_ace_player; 创建数据文件，通过\\001作为分隔符，同时将文件上传到HDFS，再通过Hive查询可看到已正常查到数据\n在大数据场景下，优先将\\001作为分隔符，这样在建表时可以省去不少工作，同时也能保持一个统一的规范\n指定数据存储路径 将数据文件上传到任意HDFS路径下，例如这里上传到/test1020 下\n执行建表语句\n1 2 3 4 5 create table t_team_ace_player_location( id int, team_name string, ace_player_name string) location \u0026#39;/test1020\u0026#39;; --使用location关键字指定本张表数据在hdfs上的存储路径, 可以是文件夹也可以是文件 执行表查询，可以看到数据都已正常导入\n内/外部表 理论 内部表（Internal table）也称为被Hive拥有和管理的托管表（Managed table）。默认情况下创建的表就是内部表，Hive完全管理表（元数据和数据/结构和文件）的生命周期；当删除内部表时，它会删除数据以及表的元数据。\n外部表（External table）中的数据不是Hive拥有或管理的，只管理表元数据的生命周期。要创建一个外部表，需要使用EXTERNAL语法关键字。删除外部表只会删除元数据，而不会删除实际数据。在Hive外部仍然可以访问实际数据。 而且外部表更为方便的是可以搭配location语法指定数据的路径。\n两者差异是，无论内部表还是外部表，Hive都在Hive Metastore中管理表定义及其分区信息。删除内部表会从Metastore中删除表元数据，还会从HDFS中删除其所有数据/文件。删除外部表，只会从Metastore中删除表的元数据，并保持HDFS位置中的实际数据不变。\n当需要通过Hive完全管理控制表的整个生命周期时，请使用内部表。\n当数据来之不易，防止误删，又或者文件已经存在位于远程位置时，请使用外部表，因为即使删除表，文件也会被保留。\n演示 执行语句创建内部表\n1 2 3 4 5 6 7 8 9 --默认情况下 创建的表就是内部表 create table student( num int, name string, sex string, age int, dept string) row format delimited fields terminated by \u0026#39;,\u0026#39;; 查看建表元数据\n1 2 3 --可以使用DESCRIBE FORMATTED itheima.student -- 来获取表的描述信息，从中可以看出表的类型。 describe formatted sherlockdb.student; 通过显示能看到表的类型为MANAGED_TABLE，意味着这张表被Hive管理起来，也就是内部表\n执行语句创建外部表\n1 2 3 4 5 6 7 8 9 10 11 12 --创建外部表 需要关键字 external --外部表数据存储路径不指定 默认规则和内部表一致 --也可以使用location关键字指定HDFS任意路径 create external table student_ext( num int, name string, sex string, age int, dept string) row format delimited fields terminated by \u0026#39;,\u0026#39; location \u0026#39;/stu\u0026#39;; 查看建表元数据能看到表的类型为MANAGED_TABLE\n将数据上传到HDFS这两个表的地址上，通过查询可分别看到表内有数据\n查看HDFS地址，可以看到内部表和外部表下都有对应的数据文件\n执行drop table操作\n1 2 3 drop table student; drop table student_ext; show tables; 再去查看内部表对应的HDFS上的文件，发现表对应的目录已经被删除；在看外部表可以看到\n分区表 理论 静态分区 当Hive表对应的数据量大、文件多时，为了避免查询时全表扫描数据，Hive支持根据用户指定的字段进行分区，分区的字段可以是日期、地域、种类等具有标识意义的字段。比如把一整年的数据根据月份划分12个月（12个分区），后续就可以查询指定月份分区的数据，尽可能避免了全表扫描查询。\n不同分区对应不同的文件夹，同一分区的数据存储在同一个文件夹下 指定分区查询的方式叫做分区裁剪 多重分区下，分区之间是一种递进关系，可以理解为在前一个分区的基础上继续分区，前后分区关系为树的父子节点 分区表不是建表的必要规则，而是一种优化手段 分区字段不能是表中已有字段，不能重复 分区字段是虚拟字段，其数据并不存储在底层文件中 Hive支持多重分区，也就是在分区的基础上继续分区，划分得更细粒度 建立分区表的语法\n1 CREATE TABLE table_name (column1 data_type, column2 data_type) PARTITIONED BY (partition1 data_type, partition2 data_type,….); 动态分区 所谓静态分区指的是分区的字段值是由用户在加载数据的时候手动指定的。\n1 load data [local] inpath \u0026#39; \u0026#39; into table tablename partition(分区字段=\u0026#39;分区值\u0026#39;...); 所谓动态分区指的是分区的字段值是基于查询结果自动推断出来的。核心语法就是insert+select。在创建动态分区表时，要先把数据存放在一张表当中，这张表可以是一张临时表，也可以是其他查询表。\n启用hive动态分区，需要在hive会话中设置两个参数：\n1 2 set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; 第一个参数表示开启动态分区功能，第二个参数指定动态分区的模式。分为nonstick非严格模式和strict严格模式。strict严格模式要求至少有一个分区为静态分区。\n分区表的使用重点在于：\n一、建表时根据业务场景设置合适的分区字段。比如日期、地域、类别等；\n二、查询的时候尽量先使用where进行分区过滤，查询指定分区的数据，避免全表扫描。\n分桶表 分桶表也叫做桶表，源自建表语法中bucket单词。是一种用于优化查询而设计的表类型 分桶表对应的数据文件在底层会被分解为若干个部分，也就是被拆分成若干个独立的小文件 在分桶时，我们要指定根据哪个字段将数据分为几桶（几个部分），分桶的字段一定要是表中存在的字段 分区相当于将数据按多个文件夹划分，分桶是将原来的文件拆分成多个小文件存储 默认规则是：Bucket number = hash_function(bucketing_column) mod num_buckets。\n可以发现桶编号相同的数据会被分到同一个桶当中。hash_function取决于分桶字段bucketing_column的类型：\n如果是int类型，hash_function(int) == int;如果是其他类型，比如bigint,string或者复杂数据类型，hash_function比较棘手，将是从该类型派生的某个数字，比如hashcode值。\n1 2 3 4 5 --分桶表建表语句 CREATE [EXTERNAL] TABLE [db_name.]table_name [(col_name data_type, ...)] CLUSTERED BY (col_name) INTO N BUCKETS; 分桶的好处\n针对分桶字段进行查询时，减少全表扫描 JOIN时可以提高MR程序效率，减少笛卡尔积数量 分桶表数据可以进行高效的抽样 演示非分区场景 创建非分区表\n1 2 3 4 5 6 7 8 9 10 11 12 13 create table t_all_hero( id int, name string, hp_max int, mp_max int, attack_max int, defense_max int, attack_range string, role_main string, role_assist string ) row format delimited fields terminated by \u0026#34;\\t\u0026#34;; 将六个数据文件上传到表t_all_hero对应的HDFS目录下\n查询表\n执行查询语句\n1 2 --查询role_main主要定位是射手并且hp_max最大生命大于6000的有几个 select count(*) from t_all_hero where role_main=\u0026#34;archer\u0026#34; and hp_max \u0026gt;6000; 可以看到查询结果非常缓慢（有报错，待处理）\n演示分区表 创建分区表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --注意分区表创建语法规则 --分区表建表 create table t_all_hero_part( id int, name string, hp_max int, mp_max int, attack_max int, defense_max int, attack_range string, role_main string, role_assist string ) partitioned by (role string)--注意哦 这里是分区字段 row format delimited fields terminated by \u0026#34;\\t\u0026#34;; 将六个数据文件上传到HDFS的/tmp目录下\n通过静态加载语句分别将六个文件加载成分区表的六个分区\n1 2 3 4 5 6 7 --从HDFS静态加载分区表数据 load data inpath \u0026#39;/tmp/archer.txt\u0026#39; into table t_all_hero_part partition(role=\u0026#39;sheshou\u0026#39;); load data inpath \u0026#39;/tmp/assassin.txt\u0026#39; into table t_all_hero_part partition(role=\u0026#39;cike\u0026#39;); load data inpath \u0026#39;/tmp/mage.txt\u0026#39; into table t_all_hero_part partition(role=\u0026#39;fashi\u0026#39;); load data inpath \u0026#39;/tmp/support.txt\u0026#39; into table t_all_hero_part partition(role=\u0026#39;fuzhu\u0026#39;); load data inpath \u0026#39;/tmp/tank.txt\u0026#39; into table t_all_hero_part partition(role=\u0026#39;tanke\u0026#39;); load data inpath \u0026#39;/tmp/warrior.txt\u0026#39; into table t_all_hero_part partition(role=\u0026#39;zhanshi\u0026#39;); 查询表可得\n执行查询语句\n1 2 3 4 --非分区表 全表扫描过滤查询 select count(*) from t_all_hero where role_main=\u0026#34;archer\u0026#34; and hp_max \u0026gt;6000; --分区表 先基于分区过滤 再查询 select count(*) from t_all_hero_part where role=\u0026#34;sheshou\u0026#34; and hp_max \u0026gt;6000; 演示多重分区表 通过指令创建多重分区表\n1 2 3 4 5 6 -----多重分区表 --单分区表，按省份分区 create table t_user_province (id int, name string,age int) partitioned by (province string); --双分区表，按省份和市分区 --分区字段之间是一种递进的关系 因此要注意分区字段的顺序 谁在前在后 create table t_user_province_city (id int, name string,age int) partitioned by (province string, city string); 上传文件到HDFS\n静态加载多分区\n1 2 3 4 5 6 7 --双分区表的数据加载 静态分区加载数据 load data inpath \u0026#39;/tmp/user.txt\u0026#39; into table t_user_province_city partition(province=\u0026#39;zhejiang\u0026#39;,city=\u0026#39;hangzhou\u0026#39;); load data inpath \u0026#39;/tmp/user.txt\u0026#39; into table t_user_province_city partition(province=\u0026#39;zhejiang\u0026#39;,city=\u0026#39;ningbo\u0026#39;); load data inpath \u0026#39;/tmp/user.txt\u0026#39; into table t_user_province_city partition(province=\u0026#39;shanghai\u0026#39;,city=\u0026#39;pudong\u0026#39;); 查看HDFS目录层级，可以看到两层分区就是树层级管理\n查询分区数据\n1 2 --双分区表的使用 使用分区进行过滤 减少全表扫描 提高查询效率 select * from t_user_province_city where province= \u0026#34;zhejiang\u0026#34; and city =\u0026#34;hangzhou\u0026#34;; 演示动态分区表 开启动态分区参数\n1 2 3 --动态分区 set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; 创建分区表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --创建一张新的分区表 t_all_hero_part_dynamic create table t_all_hero_part_dynamic( id int, name string, hp_max int, mp_max int, attack_max int, defense_max int, attack_range string, role_main string, role_assist string ) partitioned by (role string) row format delimited fields terminated by \u0026#34;\\t\u0026#34;; select * from t_all_hero; 执行分区动态插入\n1 2 3 --执行动态分区插入 insert into table t_all_hero_part_dynamic partition(role) --注意这里 分区值并没有手动写死指定 select tmp.*,tmp.role_main from t_all_hero tmp; 查询表数据\n1 select * from t_all_hero_part_dynamic; ","date":"2024-10-21T15:57:06+08:00","image":"https://sherlock-lin.github.io/p/hiveddl%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20241020161557027_hu8469343391443913845.png","permalink":"https://sherlock-lin.github.io/p/hiveddl%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"HiveDDL学习笔记"},{"content":"背景 在HDFS集群运维过程中，扩缩容是常见的事情，常常发生的缩容情况有如下几种\n实际业务增长低于集群创建时的预估规模 公司业务高峰过后，例如双11等，之前临时扩容的节点需要进行下线 费用控制等原因 那问题来了，HDFS在设计时就把诸如机器故障考虑进去了，能否直接把某台运行Datanode的机器关掉然后拔走呢？理论上可行的，不过在实际的集群中，如果某份数据只有一份副本而且它就在这个机器上，那么直接关掉并拔走机器就会造成数据丢失。本文将介绍如何Decommission DataNode以及可能会遇到Decommission超时案例及其解决方案。\n前置动作(业务侧) 提前跟相关使用方同步变更事项（变更时间、变更动作、变更可能造成的影响） 非特殊情况务必在业务低峰期进行变更操作 前置动作(技术侧) 健康检查 1 2 3 4 5 # 检查对应哪些Block发生了损坏（显示具体的块信息和文件路径信息） hdfs fsck / -list-corruptfileblocks -openforwrite -files -blocks -locations # 检查HDFS文件系统健康 通过命令的方式查看或者web ui 信息进行查看 hdfs fsck / 如果发现存在HDFS坏的块，可通过以下指令进行修复\n修复方式一(不建议) 可通过以下执行删除块损坏的文件，然后对应的业务系统数据重刷\n1 2 # 若hdfs出现块问题可进入如下操作处理 hdfs fsck file_name -delete\t//删除路径下的坏块 如果仅仅删除损坏块对应的文件，那么对应的数据丢了多少我们是不知道的，如何去确保数据补回来呢? 这是需要思考的。 如果是日志类的数据，丢一点点是没有关系的，那就没事 如果对应的数据是业务数据，比如订单数据，那是不能丢的，数据重刷和维护都是需要报告的 修复方式二 手动修复Block的命令(最多重试10次)\n1 2 # 修复坏块，retries 为重试次数 hdfs debug recoverLease -path / -retries 5 当1个block有对应的三个副本，其中一个副本损坏了，但是有另外两个副本存在，是可以利用另外两个副本进行修复的，因此我们可以使用debug命令进行修复\n修复方式三 通过配置参数自动修复\n1 2 3 4 # 当数据块损坏后，DataNode节点在执行directoryscan操作之前，都不会发现损坏directoryscan操作间隔6h进行 dfs.datanode.directoryscan.interval : 21600 # 在DataNode向NameNode进行blockreport之前，都不会恢复数据块;blockreport操作是间隔6h dfs.blockreport.intervalMsec : 21600000 调整带宽 1 2 3 # balance带宽，平衡带宽值应小于磁盘和网络上的带宽， # 以最大限度地减少对群集的影响，但需花费更长时间，如下值仅供参考 dfs.datanode.balance.bandwidthPerSec = 500MB/S 下线操作 添加黑名单 在Active Namenode节点，把需要下线的DataNode的主机名加入到dfs.hosts.exclude(该配置项在hdfs-site.xml)所指定的文件中，有多个要下线的DataNode用换行符分割，建议一次Decommission节点小于hdfs备份数。\n刷新Node 1 hdfs dfsadmin -refreshNodes 检查状态 打开NameNode Web UI，跳转到Datanodes页面，检查要下线的DataNode的状态是否已更改为“Decommission In Progress”，如图\n观测要下线的节点，当它上面所有块均已复制，其状态将变为“Decommissioned”，如下图，此时就可以关闭DataNode进程\n停止DataNode 执行以下命令 $HADOOP_HOME/sbin/hadoop-daemon.sh stop datanode\n收尾动作 在Active NameNode主机上，清空dfs.hosts.exclude文件添加的要下线的节点主机名,然后再次执行以下命令： hdfs dfsadmin -refreshNodes\n案例分析(一) 问题描述 正常3个节点，一天就可以完成下线，这次执行了5天还没完成，并不是不动，而是动的非常慢\n原因分析 查看NameNode Web UI的Datanodes页面如下\nUnder replicated blocks：当前block的副本 \u0026lt; 所设置的副本(默认就是小于3) 数量 Blocks with no live replicas：没有live 的副本，存在的副本可能都在Decommission的节点上 Under Replicated Blocks In files under construction: 当前正在复制中的block个数 在decommission几个节点中，存在副本只在Decommission的节点上的情况。这意味着，如果“删除”数据节点，则带有这些块的文件将被损坏。以防这种现象出现，Decommission节点会被阻塞。\n解决方案 检查0/1/2副本的文件，强制改成三副本\n1 2 3 4 5 6 7 8 9 10 1、打印所有块信息： hdfs fsck / -files -blocks \u0026gt; ./blocks.txt 2、检查副本： sed \u0026#39;$!N;/Live_repl=0/P;D\u0026#39; ./blocks.txt | grep \u0026#34;block\u0026#34; | awk \u0026#39;{print $1}\u0026#39; \u0026gt; ./rep_0.txt # 0副本文件 sed \u0026#39;$!N;/Live_repl=1/P;D\u0026#39; ./blocks.txt | grep \u0026#34;block\u0026#34; | awk \u0026#39;{print $1}\u0026#39; \u0026gt; ./rep_1.txt # 1副本文件 sed \u0026#39;$!N;/Live_repl=2/P;D\u0026#39; ./blocks.txt | grep \u0026#34;block\u0026#34; | awk \u0026#39;{print $1}\u0026#39; \u0026gt; ./rep_2.txt # 2副本文件 3、改副本数：（改之前确认一下文件内容） for hdfsfile in `cat ./rep_0.txt`; do hdfs dfs -setrep 3 $hdfsfile; done for hdfsfile in `cat ./rep_1.txt`; do hdfs dfs -setrep 3 $hdfsfile; done for hdfsfile in `cat ./rep_2.txt`; do hdfs dfs -setrep 3 $hdfsfile; done 案例分析(二) 问题描述 文件未关闭导致Decommission超时， 当待Decommission DataNode节点中存在打开中的文件，表明此文件目前不是一个完整状态，此文件副本就无法复制到其它datanode节点上，由于存在未完全复制完的副本，则待Decommission会被阻塞超时。通过以下步骤检查Decommission 节点是否存在打开的文件。\n解决方案 通过下面语句查询处于打开中的文件，通过此文件追查相对应的业务，让其重启或者关闭相对应的任务\n1 2 3 4 5 6 检查日志获取正在退服的块 grep \u0026#34;Is current datanode\u0026#34; hadoop-xx-namenode-xxxxxxxxx.log | tail | awk \u0026#39;{print $9}\u0026#39; 根据block id 获取对应的文件，并检查是否处于打开状态 hdfs fsck -blockId block_id hadoop fsck 文件名 -files -blocks -locations -openforwrite 下线参数调优 参数名称 默认值 参数含义 dfs.namenode.decommission.interval 30 每次启动monitor线程处理退服节点的间隔 dfs.namenode.decommission.blocks.per.interval 500000 每个批次最多处理多少个文件块 dfs.namenode.decommission.max.concurrent.tracked.nodes 100 同时处理退服的节点个数 dfs.namenode.replication.work.multiplier.per.iteration 32 每次复制的块的个数为 dn的个数* 该参数 dfs.namenode.replication.max-streams 64 进行复制任务分配时，单个DN 任务的最大值 dfs.namenode.replication.max-streams-hard-limit 128 若DN 的复制任务大于改值时，不会将其选为复制的源节点 默认参数下，Decommission执行速度较慢，建议检查参数是否为建议值，可适当调大下列参数\n1 2 3 dfs.namenode.replication.work.multiplier.per.iteration 注意：过大会导致块复制操作占用过多的带宽，影响业务 dfs.namenode.replication.max-streams dfs.namenode.replication.max-streams-hard-limit 参考文献 HDFS集群缩容案例: Decommission DataNode HDFS Block 损坏解决方案 ","date":"2024-10-21T11:47:44+08:00","image":"https://sherlock-lin.github.io/p/%E4%B8%8B%E7%BA%BFhdfs%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/image-20241021114920502_hu7282857752455849115.png","permalink":"https://sherlock-lin.github.io/p/%E4%B8%8B%E7%BA%BFhdfs%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","title":"下线HDFS最佳实践"},{"content":"背景 每天elasticsearch集群在上午某个时间段CPU几乎打满，此时访问elasticsearch的服务rt会跟着抖动，通过排查发现是由于这个时间段会有数据通过hive任务写到elasticsearch，这个hive任务使用的是ES-Hadoop插件做的数据导入，整个问题的罪魁祸首已经发现了，那么应该怎么去解决呢？\n优化方案一 ES-Hadoop组件已经发展多年，按道理说已经很成熟了，一般出现这种问题大概率都是使用不当的缘故，因此通过查看 官网配置 来查看是否使用不当，通过一番查看下来大致看到两个比较相关的配置\n1 2 es.batch.size.entries: 控制每批写到Elasticsearch的消息条数 es.batch.write.retry.wait：控制重试批次之间时间间隔 通过反复针对这两个参数调优后发现，基本没太大的收益，此时elasticsearch集群CPU的突刺仿佛一把利刃插进心脏一样，不甘心，一定要将它消灭，此时又参考了网上不少优秀的文章例如 https://cloud.tencent.com/developer/article/1612108 这位大佬写的，但是并没有解决我的问题，继续摸索下一个方案\n优化方案二 ES-Hadoop配置方式行不通，那么换个思路，咱们是通过Hive来将数据写到elasticsearch集群的，那么如果咱们将Hive任务的并行度降低些行不行呢？想到这里不禁喜悦起来，感觉自己又行了\n通过下面几个配置来控制Hive写elasticsearch的并行度\n1 2 3 set hive.exec.reducers.max=20; set tez.am.vertex.max-task-concurrency=20; set hive.tez.auto.reducer.parallelism=false; 虽然已经大幅降低并行度了，但是elasticsearch的CPU还是高居不下，仿佛在说，你过来打我呀～\n优化方案三 通过上面两个失败的方式后，通过思考🤔发现最大的问题是elasticsearch集群的处理能力赶不上处理的能力，那么如果限制elasticsearch的写速率呢，通过跟elasticsearch dba沟通发现不支持这种方式。那只能从写的任务进行下手了\n通过查阅ES-Hadoop代码发现，代码写得还挺好的，封装都不错～\n通过跟踪代码发现最关键的代码在 BulkProcessor类的add方法，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 /** * Adds an entry to the bulk request, potentially flushing if the request reaches capacity. * @param payload the entire bulk entry in JSON format, including the header and payload. */ public void add(BytesRef payload) { // check space first // ba is the backing array for data if (payload.length() \u0026gt; ba.available()) { if (autoFlush) { flush(); } else { throw new EsHadoopIllegalStateException( String.format(\u0026#34;Auto-flush disabled and bulk buffer full; disable manual flush or increase \u0026#34; + \u0026#34;capacity [current size %s]; bailing out\u0026#34;, ba.capacity())); } } data.copyFrom(payload); dataEntries++; if (bufferEntriesThreshold \u0026gt; 0 \u0026amp;\u0026amp; dataEntries \u0026gt;= bufferEntriesThreshold) { if (autoFlush) { flush(); } else { // handle the corner case of manual flush that occurs only after the buffer is completely full (think size of 1) if (dataEntries \u0026gt; bufferEntriesThreshold) { throw new EsHadoopIllegalStateException( String.format( \u0026#34;Auto-flush disabled and maximum number of entries surpassed; disable manual \u0026#34; + \u0026#34;flush or increase capacity [current size %s]; bailing out\u0026#34;, bufferEntriesThreshold)); } } } } 上面的逻辑也很清晰，就是要么数据写满本地容量或者数据写够配置的条数就触发一次发送。那么如果咱们能够控制两个批次之间间隔，是否就可以让elasticsearch集群“休息”一下，降低CPU从而正常的给其他业务提供服务呢？\n纸上得来终觉浅，绝知此事要躬行。说干就干，以下是我的代码改动，供大家参考\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /** * 控制写入批次的间隔 */ private void waitSomeTime() { int batchWaitTime = settings.getBatchWaitTime(); System.out.println(\u0026#34;batchWaitTime is:\u0026#34; +batchWaitTime); if (batchWaitTime \u0026lt;= 0) return; try { System.out.println(\u0026#34;start sleep!\u0026#34;+\u0026#34;,time is :\u0026#34;+new Date()+\u0026#34;, thread name is:\u0026#34;+Thread.currentThread().getName()); Thread.sleep(batchWaitTime); System.out.println(\u0026#34;end sleep!\u0026#34;+\u0026#34;,time is :\u0026#34;+new Date()+\u0026#34;, thread name is:\u0026#34;+Thread.currentThread().getName()); } catch (Exception e) { e.printStackTrace(); } } Settings类\n1 2 3 public int getBatchWaitTime() { return Integer.valueOf(getProperty(ES_BATCH_WAIT_TIME, ES_BATCH_WAIT_TIME_DEFAULT)); } ConfigurationOptions接口\n1 2 3 /** Elasticsearch batch size given in bytes */ String ES_BATCH_WAIT_TIME = \u0026#34;es.batch.wait.time\u0026#34;; String ES_BATCH_WAIT_TIME_DEFAULT = \u0026#34;0\u0026#34;; 最后在每写完一批数据后触发休息一下\n改动完后，再结合以下配置执行任务\n1 2 3 4 5 6 7 8 9 10 ES-Hadoop配置 \u0026#39;es.batch.size.bytes\u0026#39;=\u0026#39;5mb\u0026#39; \u0026#39;es.batch.size.entries\u0026#39;=\u0026#39;2000\u0026#39;, \u0026#39;es.batch.wait.time\u0026#39;=\u0026#39;2000\u0026#39;, //咱们自己新加的配置 \u0026#39;es.batch.write.refresh\u0026#39;=\u0026#39;false\u0026#39;, Hive参数调优 set hive.exec.reducers.max=20; set tez.am.vertex.max-task-concurrency=20; set hive.tez.auto.reducer.parallelism=false; 通过测试验证将CPU从几乎100%干到了20%～30%，算是圆满完成任务了，心中的石头也落了下来，终于可以睡个好觉了～\n补充说明 除了上述的几种方式，还有一些可供参考的方案列在这里\n使用elasticsearch-rest-client-xxx.jar，不过这是通过http写入的性能会差些 扩容elasticsearch集群（经费充足的话可以考虑） 对数据做列处理，就是在业务高峰期仅写必要的几列数据到elasticsearch集群（但可能CPU还是会高） 在解决完这个问题后，本来想给ES-Hadoop社区提这个issue并进行fix的。但是发现了已经有大佬反馈过这个问题了 https://github.com/elastic/elasticsearch-hadoop/pull/1405，但是由于社区的前辈们认为有其他更好的解决方式并没有merge相关的代码\n总结 以上就是整个问题的解决过程，我也相信一定有更好，更优雅的解决方式，如果你恰好有好的想法也可以给ES-Hadoop社区提供；但无论黑猫还是白猫，能抓到老鼠的才是好猫，通过方案三快速的解决的问题并且稳定运行了一年，给公司节省下扩容集群的成本，这在我这个菜鸡看来已经够了。如果你也比较赶时间的话，可以考虑直接试下我的jar包，希望能对您有帮助。\n","date":"2024-10-14T00:15:06+08:00","image":"https://sherlock-lin.github.io/p/%E5%8E%8B%E5%88%B6es-hadoop%E8%BF%99%E5%A4%B4%E9%87%8E%E5%85%BD%E7%9A%84%E6%96%B9%E5%BC%8F%E6%9D%A5%E4%BA%86/image-20240221185820644_hu16452014449977385359.png","permalink":"https://sherlock-lin.github.io/p/%E5%8E%8B%E5%88%B6es-hadoop%E8%BF%99%E5%A4%B4%E9%87%8E%E5%85%BD%E7%9A%84%E6%96%B9%E5%BC%8F%E6%9D%A5%E4%BA%86/","title":"压制es Hadoop这头野兽的方式来了"},{"content":"引言 缓存，这是一个经久不衰的话题，它通过“空间换时间”的战术不仅能够极大提升处理查询性能还能很好的保护底层资源。最近针对系统数据缓存的优化后，由于这是一个通用的场景并且有了一点心得因此在这里分享出来。\n案例 无论是传统Web后端应用还是大数据平台服务应用，本质上都是一个Java进程并且应用相关数据一般都是存在Mysql。从抽象的角度来看，所有请求基本都要经过以下几个流程：参数校验、鉴权、请求处理、请求响应。这几个流程特别是前几个往往需要依赖存在MySQL中的系统数据，例如判断请求中携带的的服务ID在系统中是否有对应的服务、获取服务对应的认证信息进行身份校验、请求处理时常常依赖的一些数据如某个变动非常低频的小表或者一些配置在MySQL中的系统配置等，此时如果是你，会怎么去设计的这个缓存？我先举两个负责过的项目之前是如何设计的\nA项目 服务启动的时候读取MySQL中的系统数据并以HashMap的格式存在Java进程中，不会自动更新，在有配置变动时循环调用所有服务器的ip接口触发重新加载数据。具体流程如下\n这种设计的优点如下\n实现简单，不依赖外部组件 有数据变更立马更新所有缓存数据，基本不会在缓存中读到旧数据的逻辑 以上是这种设计的优点，可能也是设计者这么设计的原因。但是随着维护会出现以下问题\n所有服务器的ip是配置在apollo中的，因此每次扩容机器时都要将新的ip增加到apollo中，否则可能会出现缓存不更新现象 在服务从ECS迁移到K8s后，服务的ip是动态分配的并且每次重启后都不一样，因此当有配置变动时只能手动重启服务或者执行curl调用接口来触发各个节点进行配置更新 B项目 服务启动的时候读取MySQL中的系统数据并以HashMap的格式存在Java进程中，并记录当前更新时间，在查询缓存时会判断此时距离上次更新时间是否已经超过30秒(硬编码)，如果超过则重新触发查询MySQL更新缓存。具体流程如下\n这种设计的优点如下\n服务之间无需通信，相比A服务来说是会自己主动更新 除了上面这一个优点貌似旧没有了，值得吐槽的点很多，大致列举一下它实现的问题\n针对每一个需要更新的缓存都新建一个自己独立的类来实现CommandLineRunner接口 硬编码30s过期的逻辑以及查询时再去做更新动作(类似第一次惩罚) 更新缓存时的逻辑是，先全量查询Mysql对应的表，然后循环对比缓存中的数据，有新增的就追加到缓存，再去跟缓存比较是否有删除的，有的话再去缓存中删除 分析 这两个服务在更新系统数据缓存上都存在不少问题，现在再回过头来思考🤔下，我们理想的系统数据缓存的实现应该是什么样子的？我大概整理了一下大致如下\n服务启动时要全量读取对应的数据以Map格式存在Java进程的内存中 服务应该是定时自行去Mysql同步最新的数据，并且定时周期是允许可配置的 一些重要配置变更时触发各个服务立马去Mysql同步数据(可选，需要参考具体业务场景) 基于这个场景我对比了下当前比较热门的缓存工具Guava和Caffeine，最终发现它们都不适合咱们的这个场景。它们的设计更偏向于解决缓存热点数据的问题，简单来说就是咱们的场景是Java内存有10G，而存在Mysql中的数据有100M，我们需要将这100M的数据存在Java内存中进行请求加速，需要解决的是如何全量加载到内存的问题。而Guava和Caffeine的设计要解决的问题时，如何更有效的合理的利用内存，例如Java内存只有10G，而存在MySQL的数据有100G，因此需要将这100G中的热点数据存在Java内存中来提升性能和降低高频访问Mysql，它具体要考虑的问题大致有以下几点\n通过LRU算法或者变种来保留热点数据 通过大小限制以及时间限制来剔除掉缓存数据从而保证Java内存不会被撑爆 在有新数据写入或者数据更新时，同步更新缓存中的数据 \u0026hellip;. 通过这些你会发现其实这个场景真不适合用Guava和Caffeine，这个过程中也翻阅了一些大佬的实现，但是发现都有点跑偏了，有种为了用Caffeine而强行用的味道。例如 https://www.vincentli.top/2020/09/01/jvm-local-cache-case-caffeine/，这里本质上还是跟B项目的实现一样，只不过用Caffeine替换了Java的HashMap罢了。在这里补充说下，Caffeine是非常优秀的缓存工具，现在很多优秀的开源组件例如 Pulsar 中也会用它来加速查询，但盲目的使用是不可取的，并且这肯定也不是Caffeine设计者想看到的。\n我的实现 上面说了这么多，我分享下自己的实现供大家参考以及批判。在这里通过一个定时线程池实现即可，在启动时触发一次并在后续周期性的刷新配置即可，核心代码也就两三行。先看下流程\n代码实现如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public class CacheManager implements CommandLineRunner { private static final Long DEFAULT_INTERVAL = 36000L; private final ScheduledExecutorService apiSystemConfigExecutorService = Executors.newSingleThreadScheduledExecutor(); private static Map\u0026lt;String, String\u0026gt; apiSysConfig = new HashMap\u0026lt;\u0026gt;(); @Override public void run(String... args) throws Exception { apiSystemConfigExecutorService.scheduleAtFixedRate(this::loadApiSysConfigDataFromDB, 0, Optional.ofNullable(PropertyConfigurer.getLong(\u0026#34;SYSTEM_CONFIG_UPDATE_PERIOD\u0026#34;)).orElse(DEFAULT_INTERVAL), TimeUnit.SECONDS); } public void loadApiSysConfigDataFromDB(){ ApisDao apisDao = SpringContextHolder.getBean(ApisDao.class); List\u0026lt;ApiSysConfigEntity\u0026gt; sysConfigEntityList = apisDao.selectSystemConfig(); if (null == sysConfigEntityList || sysConfigEntityList.size() == 0){ return; } logger.info(\u0026#34;load ApiSysConfigEntity from api_sys_config in mysql ,the size of api in cache is \u0026#34; + sysConfigEntityList.size()); apiSysConfig = sysConfigEntityList.stream() .collect(Collectors.toMap(ApiSysConfigEntity::getName, ApiSysConfigEntity::getValue)); } } 总结 在工作中只要我们观察和思考，就会发现其实是存在不少值得完善的地方，此时应该考虑对它们进行完善，否则如果长期维护一个“丑陋”的系统，你的思维、以及审美也会随之跟着降低，以至于久而久之就觉得这种设计也挺好的，甚至后续再有类似的场景时你还是会选择这种设计。工作的本质也是一场修行，在对系统进行改进完善的过程也是自我完善的过程，简称“借物得道”，同时如果读者针对这个场景有更合适的设计也欢迎在下方一起讨论。\n","date":"2024-10-12T19:15:11+08:00","image":"https://sherlock-lin.github.io/p/%E5%85%B3%E4%BA%8E%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E7%BC%93%E5%AD%98%E7%9A%84%E6%80%9D%E8%80%83%E4%BB%A5%E5%8F%8A%E8%AE%BE%E8%AE%A1/image-20240412140549821_hu14806540156354507896.png","permalink":"https://sherlock-lin.github.io/p/%E5%85%B3%E4%BA%8E%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E7%BC%93%E5%AD%98%E7%9A%84%E6%80%9D%E8%80%83%E4%BB%A5%E5%8F%8A%E8%AE%BE%E8%AE%A1/","title":"关于系统数据缓存的思考以及设计"},{"content":"Maven依赖 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-mysql-cdc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 程序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public static void main(String[] args) throws Exception { //TODO 1. 获取执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); //TODO 2. 创建MySqlSource并提供连接mysql的信息 MySqlSource\u0026lt;String\u0026gt; mySqlSource = MySqlSource.\u0026lt;String\u0026gt;builder() .hostname(\u0026#34;localhost\u0026#34;) .port(3306) .username(\u0026#34;root\u0026#34;) .password(\u0026#34;12345678\u0026#34;) .databaseList(\u0026#34;test\u0026#34;) .tableList(\u0026#34;test.t_user\u0026#34;) .startupOptions(StartupOptions.initial()) .deserializer(new JsonDebeziumDeserializationSchema()) .build(); //TODO 3. 将source装载进环境变量里 DataStreamSource\u0026lt;String\u0026gt; mysqlSourceDS = env.fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), \u0026#34;MysqlSource\u0026#34;); //TODO 4. 打印数据 mysqlSourceDS.print(); //TODO 5. 启动任务 env.execute(\u0026#34;Print MySQL Snapshot + Binlog\u0026#34;); } 验证 新增 启动程序，然后在mysql test库t_user表下插入数据\n1 insert into t_user value (124,\u0026#39;test\u0026#39;, 18, 1); 通过控制台可以看到输出如下信息，可以看到已经成功监听\n1 {\u0026#34;before\u0026#34;:null,\u0026#34;after\u0026#34;:{\u0026#34;id\u0026#34;:124,\u0026#34;name\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;age\u0026#34;:18,\u0026#34;status\u0026#34;:1},\u0026#34;source\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;1.9.8.Final\u0026#34;,\u0026#34;connector\u0026#34;:\u0026#34;mysql\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;mysql_binlog_source\u0026#34;,\u0026#34;ts_ms\u0026#34;:1728610478000,\u0026#34;snapshot\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;db\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;sequence\u0026#34;:null,\u0026#34;table\u0026#34;:\u0026#34;t_user\u0026#34;,\u0026#34;server_id\u0026#34;:1,\u0026#34;gtid\u0026#34;:null,\u0026#34;file\u0026#34;:\u0026#34;binlog.000007\u0026#34;,\u0026#34;pos\u0026#34;:7510723,\u0026#34;row\u0026#34;:0,\u0026#34;thread\u0026#34;:490,\u0026#34;query\u0026#34;:null},\u0026#34;op\u0026#34;:\u0026#34;c\u0026#34;,\u0026#34;ts_ms\u0026#34;:1728610478607,\u0026#34;transaction\u0026#34;:null} 更新 通过以下指令执行更新操作\n1 update t_user SET status = 0 WHERE ID = 124; 通过控制台可以看到输出如下信息，跟新增的差异是，op那里从c变成了u，并且before内也有数据了，是变更前的内容\n1 {\u0026#34;before\u0026#34;:{\u0026#34;id\u0026#34;:124,\u0026#34;name\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;age\u0026#34;:18,\u0026#34;status\u0026#34;:1},\u0026#34;after\u0026#34;:{\u0026#34;id\u0026#34;:124,\u0026#34;name\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;age\u0026#34;:18,\u0026#34;status\u0026#34;:0},\u0026#34;source\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;1.9.8.Final\u0026#34;,\u0026#34;connector\u0026#34;:\u0026#34;mysql\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;mysql_binlog_source\u0026#34;,\u0026#34;ts_ms\u0026#34;:1728613503000,\u0026#34;snapshot\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;db\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;sequence\u0026#34;:null,\u0026#34;table\u0026#34;:\u0026#34;t_user\u0026#34;,\u0026#34;server_id\u0026#34;:1,\u0026#34;gtid\u0026#34;:null,\u0026#34;file\u0026#34;:\u0026#34;binlog.000007\u0026#34;,\u0026#34;pos\u0026#34;:7511031,\u0026#34;row\u0026#34;:0,\u0026#34;thread\u0026#34;:491,\u0026#34;query\u0026#34;:null},\u0026#34;op\u0026#34;:\u0026#34;u\u0026#34;,\u0026#34;ts_ms\u0026#34;:1728613503293,\u0026#34;transaction\u0026#34;:null} 删除 1 DELETE FROM t_user WHERE ID = 124; 相比之前的打印信息，op变成了d，并且before有删除之前的信息，after变成了null\n1 {\u0026#34;before\u0026#34;:{\u0026#34;id\u0026#34;:124,\u0026#34;name\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;age\u0026#34;:18,\u0026#34;status\u0026#34;:0},\u0026#34;after\u0026#34;:null,\u0026#34;source\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;1.9.8.Final\u0026#34;,\u0026#34;connector\u0026#34;:\u0026#34;mysql\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;mysql_binlog_source\u0026#34;,\u0026#34;ts_ms\u0026#34;:1728613633000,\u0026#34;snapshot\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;db\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;sequence\u0026#34;:null,\u0026#34;table\u0026#34;:\u0026#34;t_user\u0026#34;,\u0026#34;server_id\u0026#34;:1,\u0026#34;gtid\u0026#34;:null,\u0026#34;file\u0026#34;:\u0026#34;binlog.000007\u0026#34;,\u0026#34;pos\u0026#34;:7511348,\u0026#34;row\u0026#34;:0,\u0026#34;thread\u0026#34;:491,\u0026#34;query\u0026#34;:null},\u0026#34;op\u0026#34;:\u0026#34;d\u0026#34;,\u0026#34;ts_ms\u0026#34;:1728613633915,\u0026#34;transaction\u0026#34;:null} 参考 官方文档 ","date":"2024-10-11T10:29:13+08:00","image":"https://sherlock-lin.github.io/p/flinkcdc%E8%AF%BB%E5%8F%96binlog%E6%A0%B7%E4%BE%8B/the-creative-exchange-d2zvqp3fpro-unsplash_hu5876398126655421130.jpg","permalink":"https://sherlock-lin.github.io/p/flinkcdc%E8%AF%BB%E5%8F%96binlog%E6%A0%B7%E4%BE%8B/","title":"FlinkCDC读取binlog样例"},{"content":"版本 Pulsar: 3.3.0 Flink: 1.18.0 Maven依赖 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026lt;properties\u0026gt; \u0026lt;flink.version\u0026gt;1.18.0\u0026lt;/flink.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!--连接器管理--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-base\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-pulsar\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.1.0-1.18\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; Flink读取Pulsar 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 public class PulsarExample { public static String serverUrl = \u0026#34;http://localhost:8081\u0026#34;; public static void main(String[] args) throws Exception { /*1. 创建环境变量*/ StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //Flink相关配置(非必要) // env.disableOperatorChaining(); // env.enableCheckpointing(60 * 1000 * 5, CheckpointingMode.EXACTLY_ONCE); // env.getCheckpointConfig().setCheckpointTimeout(60000); // env.setStateBackend(new MemoryStateBackend()); // env.setRestartStrategy(RestartStrategies.fixedDelayRestart(0, 3 * 1000)); /*2. 从Pulsar中读取数据*/ PulsarSource\u0026lt;String\u0026gt; source = PulsarSource.builder() .setServiceUrl(serverUrl) .setStartCursor(StartCursor.earliest()) .setTopics(\u0026#34;public/sherlock-namespace-1/my-topic\u0026#34;) .setDeserializationSchema(new SimpleStringSchema()) .setSubscriptionName(\u0026#34;my-subscription-1\u0026#34;) .build(); DataStreamSource\u0026lt;String\u0026gt; streamSource = env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;Pulsar \u0026#34; + \u0026#34;Source\u0026#34;); /*3. 打印数据*/ streamSource.print(); /*4. 启动任务*/ env.execute(); } } 验证 启动读Pulsar程序 为了方便调试，直接在IDEA中启动即可\n先往Pulsar中写入数据 1 2 3 4 5 6 7 8 9 10 11 public static void commonProducer() throws Exception { final String topic = \u0026#34;persistent://public/sherlock-namespace-1/my-topic\u0026#34;; Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(Schema.STRING).topic(topic).create(); for (int i = 0; i \u0026lt; 10000; i++) { producer.send(\u0026#34;hello java API pulsar:\u0026#34; + i + \u0026#34;, 当前时间为：\u0026#34; + new Date()); System.out.println(\u0026#34;hello java API pulsar:\u0026#34; + i + \u0026#34;, 当前时间为：\u0026#34; + new Date()); } producer.close(); } 查看管理平台 在管理页面可以看到有数据写入，同时也有数据流出，并且能看到名为 my-subscription-1的订阅者，这也是咱们Flink程序中定义的订阅名称。\n查看程序控制台 Flink写数据到Pulsar 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 public class PulsarSinkExample { public static String serverUrl = \u0026#34;http://localhost:8081\u0026#34;; public static String adminUrl = \u0026#34;pulsar://localhost:6650\u0026#34;; public static void main(String[] args) throws Exception { /*1. 创建环境变量*/ StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //Flink相关配置(非必要) // env.disableOperatorChaining(); // env.enableCheckpointing(60*1000*5, CheckpointingMode.EXACTLY_ONCE); // env.getCheckpointConfig().setCheckpointTimeout(60000); // env.setStateBackend(new MemoryStateBackend()); // env.setRestartStrategy(RestartStrategies.fixedDelayRestart(0, 3 * 1000)); /*2. 从Pulsar中读取数据*/ PulsarSource\u0026lt;String\u0026gt; source = PulsarSource.builder() .setServiceUrl(serverUrl) .setStartCursor(StartCursor.earliest()) .setTopics(\u0026#34;public/sherlock-namespace-1/my-topic\u0026#34;) .setDeserializationSchema(new SimpleStringSchema()) .setSubscriptionName(\u0026#34;my-subscription-1\u0026#34;) .build(); DataStreamSource\u0026lt;String\u0026gt; streamSource = env.fromSource(source, WatermarkStrategy.noWatermarks(), \u0026#34;Pulsar \u0026#34; + \u0026#34;Source\u0026#34;); //定义输出的topic String topic = \u0026#34;persistent://public/sherlock-namespace-1/my-topic-output\u0026#34;; /*3. 写到Pulsar中*/ PulsarSink\u0026lt;String\u0026gt; sink = PulsarSink.builder() .setServiceUrl(serverUrl) .setAdminUrl(adminUrl) .setTopics(topic) .setSerializationSchema(new SimpleStringSchema()) .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) .build(); streamSource.sinkTo(sink); /*4. 启动任务*/ env.execute(); } } 验证 启动写到Pulsar程序 查看管理平台 启动程序，读取Topic my-topic的数据并写到my-topic-output 中，查看管理页面可以看到数据已写到my-topic-output 中\n启动消费Topic程序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public static void commonConsumer() throws Exception { final String topic = \u0026#34;persistent://public/sherlock-namespace-1/my-topic-output\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Consumer consumer = pulsarClient.newConsumer(Schema.STRING).topic(topic).subscriptionType(SubscriptionType.Shared).subscriptionName(\u0026#34;sub_07\u0026#34;).subscribe(); while (true) { Message\u0026lt;String\u0026gt; message = consumer.receive(); String msg = message.getValue(); System.out.println(\u0026#34;消息数据为：\u0026#34; + msg); consumer.acknowledge(message); } } 查看程序控制台 通过程序消费此Topic数据(由于Java程序消费报错，因此这里改用Flink来消费验证)，可以看到数据是正常的被写到Pulsar了\n问题 在启动Java客户端代码消费Flink写到Pulsar的数据时，客户端会报以下错，有趣的是通过Flink是能够正常读取的\n在官方也找到相同的 case\n1 2 3 4 5 Exception in thread \u0026#34;main\u0026#34; org.apache.pulsar.client.api.PulsarClientException$IncompatibleSchemaException: {\u0026#34;errorMsg\u0026#34;:\u0026#34;Topic does not have schema to check\u0026#34;,\u0026#34;reqId\u0026#34;:503535884505001157, \u0026#34;remote\u0026#34;:\u0026#34;localhost/127.0.0.1:6650\u0026#34;, \u0026#34;local\u0026#34;:\u0026#34;/127.0.0.1:61406\u0026#34;} at org.apache.pulsar.client.api.PulsarClientException.unwrap(PulsarClientException.java:1069) at org.apache.pulsar.client.impl.ConsumerBuilderImpl.subscribe(ConsumerBuilderImpl.java:103) at com.sherlock.admin.ManagerConsumer.commonConsumer(ManagerConsumer.java:59) at com.sherlock.admin.ManagerConsumer.main(ManagerConsumer.java:39) 参考 官方文档\n其他版本的官方文档\n","date":"2024-10-10T21:24:43+08:00","image":"https://sherlock-lin.github.io/p/flink%E9%9B%86%E6%88%90pulsar%E8%BF%9B%E8%A1%8C%E8%AF%BB%E5%86%99/image-20241010212758255_hu1641948039960571053.png","permalink":"https://sherlock-lin.github.io/p/flink%E9%9B%86%E6%88%90pulsar%E8%BF%9B%E8%A1%8C%E8%AF%BB%E5%86%99/","title":"Flink集成Pulsar进行读写"},{"content":"环境准备 组件版本 Pulsar(3.3.0)\nMysql(8.0.39 Homebrew方式安装)\nPulsar连接插件 pulsar-io-debezium-mysql-3.3.0.nar\n插件下载地址，选择自己Pulsar对应的版本下载使用即可\nMySQL开启binlog 如果是mysql5的需要开启，mysql8的话默认开启，由于本次使用的是mysql8，因此mysql无须做任何操作\n配置debezium-mysql-source-config.yaml（文件名可自定义） 在connectors目录下创建debezium-mysql-source-config.yaml文件，用于source的启动，配置详情如下(可直接使用)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 tenant: \u0026#34;sherlock-warehouse\u0026#34; # pulsar租户，可自定义 namespace: \u0026#34;sherlock-namespace-1\u0026#34; # pulsar命名空间，可自定义 name: \u0026#34;debezium-mysql-source\u0026#34; # source名，可自定义 topicName: \u0026#34;debezium-mysql-topic\u0026#34; # topic名，可自定义 archive: \u0026#34;connectors/pulsar-io-debezium-mysql-3.3.0.nar\u0026#34; # nar文件的存放位置，一般放在Pulsar安装目录下的connectors parallelism: 1 #并行度 configs: ## config for mysql, docker image: debezium/example-mysql:0.8 database.hostname: \u0026#34;localhost\u0026#34; # 数据库ip database.port: \u0026#34;3306\u0026#34;\t# 数据库port database.user: \u0026#34;root\u0026#34;\t# 数据库用户名 database.password: \u0026#34;12345678\u0026#34; # 数据库密码 database.server.id: \u0026#34;1\u0026#34;\t# mysql实例的唯一标识，对应my.ini下的server-id值，mysql8的话直接填1即可 database.server.name: \u0026#34;dbserver1\u0026#34; # 服务名，Pulsar会根据它拼接Topic database.whitelist: \u0026#34;flink\u0026#34;\t# 需要访问的数据库 ## database.exclude.list: \u0026#34;demo_dap\u0026#34; # 不需要访问的数据库，可以没有这一条 ## table.include.list:\u0026#34;demo_esb.ack_sample,demo_esb.aac_sample\u0026#34; #需要访问的表，不填默认所有表 ## table.exclude.list:\u0026#34;demo_esb.ack sample_test\u0026#34; # 不需要访问的数据表，选填 ## column.include.list:\u0026#34;demo_esb.ack_sample.s_ID,demo_esb.ack_sample.s_coDE\u0026#34; # 需要访问的列，不填默认所有列 ##column.exclude.list:\u0026#34;demo_esb.ack_sample.s TEsri # 不需要访问的列，选填 ## include.schema.changes:true\u0026#34; # 包含schema的改变，可以没有这一条 snapshot.mode: \u0026#34;schema_only_recovery\u0026#34; database.history: \u0026#34;org.apache.pulsar.io.debezium.PulsarDatabaseHistory\u0026#34; database.history.pulsar.topic: \u0026#34;history-topic\u0026#34; database.history.pulsar.service.url: \u0026#34;pulsar://127.0.0.1:6650\u0026#34;\t# pulsar集群服务地址 ## KEY_CONVERTER_CLASS_CONFIG, VALUE_CONVERTER_CLASS_CONFIG key.converter: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34; value.converter: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34; ## OFFSET_STORAGE_TOPIC_CONFIG offset.storage.topic: \u0026#34;offset-topic\u0026#34; 启动 MySQL启动 1 sudo mysql.server start pulsar启动 1 2 3 bin/pulsar-daemon start zookeeper bin/pulsar-daemon start bookie bin/pulsar-daemon start broker 也可以以单节点方式启动Broker pulsar standalone\npulsar-source(mysql)启动 1 bin/pulsar-admin source localrun --source-config-file connectors/debezium-mysql-source-config.yaml 查看pulsar对应的namespace下已创建对应的Topic，可以看到这里是三层管理如 dbserver1.flink.user\ndbserver1是用来标记这个flink source实例，flink是监听的数据库，user是监听的表。也就是每张被监听的mysql表在pulsar都有唯一对应的Topic\n验证 启动Pulsar 消费者 1 bin/pulsar-client consume -s \u0026#34;first-subscription\u0026#34; persistent://sherlock-warehouse/sherlock-namespace-1/dbserver1.flink.user -n 0 插入数据 1 insert into `user`(`name`,`age`,`status`) values (\u0026#39;hadoop123\u0026#39;,17,0); 查看消费情况 查看消费情况(批量插入) 通过faker生成多条信息插入到mysql，可以实时看到pulsar也监听到最新的数据变动\n问题分析 启动source报错 分析后发现可能是在监听mysql历史变更binlog时出现异常导致的，由于本次只是测试验证，因此新建一个mysql 数据库以及表来进行验证，通过新库表验证不会报此问题\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 2024-09-29T17:55:42,424+0800 [blc-localhost:3306] ERROR io.debezium.connector.mysql.MySqlStreamingChangeEventSource - Error during binlog processing. Last offset stored = {transaction_id=null, file=binlog.000004, pos=4905, server_id=1, event=1}, binlog reader near position = binlog.000004/4980 2024-09-29T17:55:42,424+0800 [blc-localhost:3306] ERROR io.debezium.pipeline.ErrorHandler - Producer failure io.debezium.DebeziumException: Error processing binlog event at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.handleEvent(MySqlStreamingChangeEventSource.java:374) ~[debezium-connector-mysql-1.9.7.Final.jar:1.9.7.Final] at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.lambda$execute$25(MySqlStreamingChangeEventSource.java:874) ~[debezium-connector-mysql-1.9.7.Final.jar:1.9.7.Final] at com.github.shyiko.mysql.binlog.BinaryLogClient.notifyEventListeners(BinaryLogClient.java:1246) [mysql-binlog-connector-java-0.27.2.jar:0.27.2] at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1072) [mysql-binlog-connector-java-0.27.2.jar:0.27.2] at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:631) [mysql-binlog-connector-java-0.27.2.jar:0.27.2] at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:932) [mysql-binlog-connector-java-0.27.2.jar:0.27.2] at java.base/java.lang.Thread.run(Thread.java:842) [?:?] Caused by: io.debezium.DebeziumException: Encountered change event for table test.test whose schema isn\u0026#39;t known to this connector at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.informAboutUnknownTableIfRequired(MySqlStreamingChangeEventSource.java:650) ~[debezium-connector-mysql-1.9.7.Final.jar:1.9.7.Final] at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.informAboutUnknownTableIfRequired(MySqlStreamingChangeEventSource.java:691) ~[debezium-connector-mysql-1.9.7.Final.jar:1.9.7.Final] at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.handleUpdateTableMetadata(MySqlStreamingChangeEventSource.java:628) ~[debezium-connector-mysql-1.9.7.Final.jar:1.9.7.Final] at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.lambda$execute$13(MySqlStreamingChangeEventSource.java:845) ~[debezium-connector-mysql-1.9.7.Final.jar:1.9.7.Final] at io.debezium.connector.mysql.MySqlStreamingChangeEventSource.handleEvent(MySqlStreamingChangeEventSource.java:354) ~[debezium-connector-mysql-1.9.7.Final.jar:1.9.7.Final] ... 6 more 启动消费报错 用java客户端消费报的错，\n1 2 3 4 5 Exception in thread \u0026#34;main\u0026#34; org.apache.pulsar.client.api.PulsarClientException$IncompatibleSchemaException: {\u0026#34;errorMsg\u0026#34;:\u0026#34;Key schemas or Value schemas are different schema type, from key schema type is JSON and to key schema is BYTES, from value schema is JSON and to value schema is BYTES\u0026#34;,\u0026#34;reqId\u0026#34;:2131068157376809305, \u0026#34;remote\u0026#34;:\u0026#34;localhost/127.0.0.1:6650\u0026#34;, \u0026#34;local\u0026#34;:\u0026#34;/127.0.0.1:50274\u0026#34;} at org.apache.pulsar.client.api.PulsarClientException.unwrap(PulsarClientException.java:1069) at org.apache.pulsar.client.impl.ConsumerBuilderImpl.subscribe(ConsumerBuilderImpl.java:103) at com.sherlock.admin.ManagerConsumer.binLogConsumer(ManagerConsumer.java:130) at com.sherlock.admin.ManagerConsumer.main(ManagerConsumer.java:42) 总结 可以看到集成还是比较简单的，这个工作思路本质上跟Flink CDC的是一样的，都是对开源的debezium进行二次封装，debezium是一个java开发的专门用于监听数据库变化的项目，因此如果想深入了解数据库数据变化感知细节的话可以对它进行研究。如果当前公司生态是采用的Pulsar，例如IoT行业的公司，有同步数据库的场景但是不想依赖其他大数据相关的组件如Flink等，可以优先考虑直接使用Pulsar IO进行数据集成。\n参考文献 Pulsar IO之CDC Debezium Connector Pulsar集成Debezium监听MySQL日志 官方配置介绍 ","date":"2024-10-08T17:29:07+08:00","image":"https://sherlock-lin.github.io/p/pulsario%E9%9B%86%E6%88%90mysql/image-20241008163436302_hu2242598783164892916.png","permalink":"https://sherlock-lin.github.io/p/pulsario%E9%9B%86%E6%88%90mysql/","title":"PulsarIO集成mysql"},{"content":"简析 距离初次写笔记/博客已经差不多十年了，这其中换了不少平台，每次换平台都少不了一顿折腾，导致有段时间都不怎么喜欢写博客了，更喜欢以笔记的方式记录下，现在在基于github搭建完博客后，终于有种安心的感觉了，不禁思考起，承载博客的最终形式应该是什么样子的，首先先简单回顾这些年用过的博客网站吧。\nV1(CSDN) 时间：15～18年\n15年时候开始想输出技术文章，在平时查阅资料以及周围同学交流时，觉得CSDN很火，于是就在CSDN上注册了号并且持续输出文章，当时接触了很多新技术，但是写的大部分内容实际上都是一些常识类的东西，很少有自己深入挖掘。当时选择CSDN还有一些其他因素，总结来说就是入门门槛低、引流好，对中文友好，相比之下Github、自建网站等来说对小白来说不是很友好，因此就这样用了几年的CSDN。\nV2(segmentfault/简书/github) 时间：18～19\n在查看技术时，被简书的简约、漂亮的界面吸引住了，相比CSDN天花乱坠的广告以及充斥着不少重复内容的情况，简书的吸引力简直太大了，于是开始在上面进行内容的输出，但是没有持续多久，发现这上面的引流做得不是很好，认真写了不少文章基本没啥流量；后来转战segmentfault，也是类似的理由。之后也在github写过一段时间，github好在可以永久的给你保证数据不会丢而且这个平台基本不会倒，但是因为UI以及没有什么好的反馈逐渐放弃。\nV3(博客园/知乎) 时间：19～21\n在经历过segmentfault/简书后，愈发觉得不应该过度在乎界面，更应该注重内容的质量，同时为了不再折腾，想选一个老牌稳重点网站，于是把目光瞄准了博客园，整体来说博客园应该是做得对开发者很友好的平台，虽然UI比较简陋，但是支持作者自定义UI呀，通过css和js一样能自定义出优美、酷炫的界面，同时它的后台管理功能也是做得比较好的，除此之外还在知乎并行的写文章，但不得不说，知乎的markdown支持真的不行，更适合写一些对格式要求没那么强的内容。\nV4(hexo自建网站) 时间：21～22\n虽然很喜欢博客园，但是有不少需要自己动手做的地方了，作为一个非前端要花不少时间去调css和js其实挺恼火的，后来想了想既然反正都花了这么多时间，为什么不直接自己搭建个网站呢，于是基于 hexo+github 搭建了一个，虽然也折腾了不少，但好歹不花钱弄了一个稳定的、属于自己的博客网站，但后来有几次hexo报错并且工作 忙了起来，就扔在那里了；再后来电脑重装系统导致本地文件丢失，由于当时只是将编译好的博客数据部署到github，因此github也没有保持原来的数据，基于这个原因就不想再继续弄了\nV5(CSDN/公众号) 时间：23～24\n思考之后，觉得作为已经工作多年的人并且对技术还有热情的人，首先当下重中之重是保持输出技术文章，平台是其次，只要能够保持高质量输出文章内容，自己的水平也会不断进步，平台只是个工具而已，并且随着在CSDN输出文章增多以及粉丝增多，CSDN也逐渐给我开放了不少酷炫的主题，而且用CSDN的话也能降低写博客的成本，于是这一年在CSDN贡献了不少的博客，同时觉得公众号可以打造自己的IP并且也挺酷炫的，因此有些觉得质量不错的文章也会在公众号再发一次。但是随着博客数量的增加，写博客的附加成本在逐渐上升，首先是CSDN不支持导入markdown文件的同时导入图片，因此每次要在CSDN上发布文章，还需要手动的在编辑页面上一张一张 的上传图片，这个方式现在回想都觉得挫；同理，公众号也是一样，相当于每发一次文章同样的事情需要做两遍，我陷入了沉思～\n在通过摸索后，发现其实可以通过图床的方式例如PicGo来解决，就是在typera写文章时，图片会自动上传到网上例如gitee、github上，然后本地直接引用网络连接，这样就完美解决上诉问题，这样本地、CSDN、公众号的都通过网络连接访问gitee上的图片资源，从而避免了每次发布编辑图片，但是这个时候CSDN网站变得很卡， 想翻阅历史的博客能加载几个小时，以及编辑一些历史文章再保存时也会卡很久或者直接失败～\n至于公众号，更适合发一些权威或者说改动不大的内容，如果有编辑过自己公众号文章的朋友应该也比较清楚，基于这个原因我也放弃了公众号。\nV6(hugo自建网站) 时间：现在\n回顾过去这些年的经历，再结合自己的情况，需求逐渐清晰了起来。\n背景：我是要用一辈子去参与编程的，因此输出博客是长期存在的需求(因为编程是脑力活动，需要深度思考，输出高质量技术博客可以push自己持续吸收高质量技术)\n以下需求按优先级排序：\n稳定的平台\n每次迁移平台都意味着有一堆适配的杂活要做，这些事情本质上就是没有意义的事情，对个人没有任何的成长而言；不需要过多参与到平台的维护工作中，例如自己购买服务器部署之类的，例如有些时间比较忙服务崩了都不知道，我期望的是这些文章我一旦发布后，即便我很多年不去维护它，它也能持续很好的工作～\n免费\n一方面不希望花钱，因为也没打算靠写博客赚钱，另一方面，也希望我认真写的文章不要依赖某一个个体，例如我很喜欢的左耳朵耗子，他的文章是购买海外服务器部署的，而在他逝去后，如果服务器被回收了，那真的是一大损失。虽然我的文章质量并不能相提并论，但是也是认真思考过的东西，如果可以我也希望即使我离开这个世界，它也能帮到后来有需要的学徒，就像现在的我依然在看左耳朵耗子的文章一样，即便作者已不在这个世界，但有时候仿佛在跟作者沟通、辩论一样，思想的东西就是这样，因此有一个能够免费并且长期承载这些思想的东西是非常重要的\n支持自定义\n正如教员所说的“枪杠子里出政权”，一些关键的东西一定要把控在手上。例如主题的选择、交互的控制、响应性能的优化等等，这样在未来有任何变动，我都能通过自己自定义去进行处理\n维护人工成本低/写文章成本低\n从长期的角度来看，不需要人维护，并且发布新文章的成本要很低，像手动一张张上传图片的操作绝不可取，其他适配的工作也是能少就少\n基于以上需求，我最终的方案也有雏型了，随着不断的完善，以下是我的最终形态\n通过本地电脑的typora编写Blog内容，这些Blog都是通过hugo创建并且管理的，同时这个目录也是由git管理的，这个时候写完文章也可以通过hugo启动本地博客网站进行展示以及调整 通过git命令将数据push到github服务端持久化起来，此时相当于数据已经持久化备份起来了，之后即便本地电脑坏了对博客内容都不会有丝毫的影响 通过配置workflows文件，在仓库1感应到有新的资源被push来时，会出发action，将仓库1最新的数据通过hugo编译部署在仓库2上，由仓库2对外提供web服务。 在有用户在博客评论时，评论信息会自动持久化到仓库3，以issue的形式持久化并且正常展示到博客网站上，这里我使用的是utterances工具来做的持久化操作 在博客搭建好之后，我们需要做的只有1、2两步，第3和第4步咱们可以永远不用去管它，如果有人像我一样觉得第二步操作输入几条指令太麻烦，可以将起封装成一件执行的shell脚本，这样就之需要专注Blog内容的编写，编写完后点一个按钮，数据会自动上传到仓库1，之后就可以通过访问博客网站正常使用了。\n总结 如果将这个整个博客系统看作一个软件的话，那么分层如下\n再聊回咱们的标题，你觉得理想情况下，blog的归处是什么样子的？或者说博客文章应该放在哪里是最好的？\n有人会觉得，有必要这么大费周张的思考这个吗，但我觉得这是很有必要的，任何事情你如果想把它做好，就一定要有闭环的思维，如果你认真写了几篇高质量的文章，由于像我上面一样，通过多次平台的迁移导致丢失了不少，这难道不是一种损失吗。这就像是工作上的失误导致公司的业务造成损失，这个损失也许是可以接受的(例如博客文章内容质量不高)，但万一哪一天公司业务起来了呢，也就是你文章质量逐渐上来或者是你开始把写博客也当作事业一样来对待时，如何给自己的博客文章提供稳定性保障，这就需要好好思考了也就是标题的问题。\n如果有其他更好的设计，欢迎在底下评论。补充说明下，上面提到的所有平台不排除作者打开方式不对，仅供读者参考，这个解决方案也是，仅供参考，希望能让有同样需求的同学少走一点弯路，这就是我写本篇文章的初衷\n","date":"2024-09-21T10:53:25+08:00","image":"https://sherlock-lin.github.io/p/blog%E5%BD%92%E5%A4%84/image-20240921101717625_hu1519644137074903956.png","permalink":"https://sherlock-lin.github.io/p/blog%E5%BD%92%E5%A4%84/","title":"Blog归处"},{"content":"简析 Broker的启动流程框架基本如下\n触发启动 初始化 读取配置、检测、赋值 启动 Bookie启动 Broker启动 启动Netty 启动后台监控任务 何时、如何触发启动 Broker的启动基本都是靠维护人员主动触发的，入口是Broker提供的脚本 bin/pulsar、bin/pulsar-daemon。常见的启动指令有 bin/pulsar standalone、 bin/pulsar broker、bin/pulsar-daemon start broker等，今天就从bin/pulsar broker流程进行探讨。先来看看bin/pulsar的脚本逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 pulsar_help() { cat \u0026lt;\u0026lt;EOF //在这里可以看到pulsar支持的操作，当然也可以通过bin/pulsar help 指令进行查看 Usage: pulsar \u0026lt;command\u0026gt; where command is one of: //启动Broker服务 broker Run a broker server //启动bookie服务 bookie Run a bookie server zookeeper Run a zookeeper server configuration-store Run a configuration-store server discovery Run a discovery server proxy Run a pulsar proxy websocket Run a web socket proxy server functions-worker Run a functions worker server sql-worker Run a sql worker server sql Run sql CLI standalone Run a broker server with local bookies and local zookeeper autorecovery Run an autorecovery service .... } //如果执行的是bin/pulsar broker则会走到这里，可以清楚的看到最终会调用PulsarBrokerStarter类进行启动 if [ $COMMAND == \u0026#34;broker\u0026#34; ]; then PULSAR_LOG_FILE=${PULSAR_LOG_FILE:-\u0026#34;pulsar-broker.log\u0026#34;} exec $JAVA $LOG4J2_SHUTDOWN_HOOK_DISABLED $OPTS -Dpulsar.log.file=$PULSAR_LOG_FILE org.apache.pulsar.PulsarBrokerStarter --broker-conf $PULSAR_BROKER_CONF $@ elif [ $COMMAND == \u0026#34;bookie\u0026#34; ]; then PULSAR_LOG_FILE=${PULSAR_LOG_FILE:-\u0026#34;bookkeeper.log\u0026#34;} exec $JAVA $OPTS -Dpulsar.log.file=$PULSAR_LOG_FILE org.apache.bookkeeper.server.Main --conf $PULSAR_BOOKKEEPER_CONF $@ Broker启动流程 通过脚本能够看到是通过PulsarBrokerStarter进行启动，因此现在就直接从它的main方法进行跟踪吧\n这里的初始化和启动两条链路都值得看，首先跟踪下初始化Broker的链路\n可以看到初始化的过程中分别做了 配置加载、初始化Broker、初始化Bookkeeper以及AutoRecoveryMain服务等。接下来就就看服务启动链路。\n可以看到启动入口很简洁，就是启动上面初始化好的Broker、Bookkeeper、AutoRecoveryMain，这里先看Broker的启动流程也就是276行\n可以看到这个方法非常大，里面内容非常丰富，一起来详细看看\n简单总结下这个方法，它主要创建了以下几个对象\nCoordinationServiceImpl对象，用于协调Broker选主\nBrokerService对象，这个是启动Broker对象的核心\nLoadManager对象，用于管理Broker对象的负载均衡\nSchemaStorage对象，负责处理读写schema的请求\nOffloadPoliciesImpl，负责分层存储操作\n并启动WebService服务，负责对外提供http服务\nWorkerService服务，负责处理function计算操作\n这里面的BrokerService是最核心的，在这里进去看下它创建的逻辑\n此方法主要做了下面几件事\n创建Netty服务端，用于处理生产者/消费者/代理的TCP请求 创建定期检测服务 不活跃的检测 消息过期检测 压缩检测 消费者检测 初始化五个Map容器 维护Topic对象 维护集群副本复制的客户端 维护连接当前Broker的管理流 维护Topic归属信息 维护多层级的Topic信息？ 启动DelayedDeliveryTrackerLoader跟踪延迟消息 启动Broker拦截器BrokerEntryMetadataInterceptors 启动限额管理对象BundlesQuotas 启动Netty服务端(此时Pulsar服务具备处理所有客户端请求能力) 启动定期检测服务 不活跃的检测 消息过期检测 压缩检测 消费者检测 消息积压检测 总结 PulsarService是Pulsar服务启动的核心类，其内置了七大重要的对象如下图\nBrokerService: 核心是启动Netty，处理客户端的TCP连接，同时通过多个Map容器维护例如Topic信息、Topic归属信息等等，除此之外还启动一批定时线程定期检测(消息过期、压缩、客户端活跃等) LoadManager: 负责处理Broker服务的负载均衡 WebService: 对外提供HTTP服务，例如管理流的操作(元数据)等 CoordinationService: 给Broker提供协调服务，例如Broker选主操作 SchemaStorage: 提供schema相关的一切服务，常见的就是schema的读写 OffloadPolicies: 提供分层存储，冷数据自动迁移到外部服务中 WorkerService: 管理Worker实例，用来执行Function计算任务 以上就是Pulsar启动流程所做的事情，其中Bookkeeper的启动以及其余的功能例如CoordinationService、SchemaStorage等等都值得单独新开一篇文章进行讲解，这里就不混在一起讲了。\n","date":"2024-09-18T10:05:57+08:00","image":"https://sherlock-lin.github.io/p/%E5%90%AF%E7%A8%8Bpulsar%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E9%AB%98%E9%80%9F%E5%90%AF%E5%8A%A8%E5%BC%95%E6%93%8E%E6%8F%AD%E7%A7%98%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%B7%A8%E5%85%BD%E7%9A%84%E8%AF%9E%E7%94%9F/image-20240913141630196_hu5121917752973153749.png","permalink":"https://sherlock-lin.github.io/p/%E5%90%AF%E7%A8%8Bpulsar%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E9%AB%98%E9%80%9F%E5%90%AF%E5%8A%A8%E5%BC%95%E6%93%8E%E6%8F%AD%E7%A7%98%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%B7%A8%E5%85%BD%E7%9A%84%E8%AF%9E%E7%94%9F/","title":"启程Pulsar：深入剖析高速启动引擎，揭秘消息中间件巨兽的诞生"},{"content":"引言 关于Pulsar Schema，咱们要想想以下几个问题\nPulsar 中的 Schema 是什么？ Pulsar Schema Registry的作用是什么？ 怎么使用？ 原理是什么？ Schema 是什么 Schema是定义结构化数据和二进制字节数组之间转换的逻辑，Pulsar的消息是以非结构化的二进制数组进行存储的，Schema只有在读写时才会被应用于数据上，因此生产者和消费者需要对Schema达成一致。Pulsar通过Schema Registry作为一个中央仓库存储Schema信息，它可以协调生产者和消费者保证相同的Schema，它可以存储多个版本的Schema，支持不同的兼容性配置以及根据兼容性的要求进行Schema的演进。Pulsar将Schema存储在Bookie上，Schema的写入、读取都通过Broker和Bookie交互，这个逻辑跟消息的读写操作是一只的，因此不需要额外考虑Schema的可用性和可靠性问题，因此整体看Pulsar实现Schema Registry的方式非常优雅\n类型安全在所有数据应用中都非常重要，生产者和消费者需要某种机制协调数据类型来避免各种潜在的问题，比如序列化和反序列化方式不一致。数据安全通常有两种处理方式client-side和service-side，本质上就是客户端用时决定和服务端提前保证\nclient-side：将一切交给用户，客户端自行负责消息的序列化和反序列化并且保证生产消费时消息的类型安全，这种方式的最大问题就是类型是通过约定的，一旦生产者写入非约定的数据，下游的消费者将没有办法解析数据\nserver-side：数据安全由服务端保证，生产者和消费者都需要跟服务端提前确定数据类型。这种方式真正意义上保证了数据的类型安全，避免了生产者写入非法数据的问题\n两种差异如下图\n怎么使用 client-side 生产者代码逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 //schema在第一次写入的时候就已经决定好了，后续用其他的schema消息类型会写入失败 public static void customSchemaProducer() { try { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;byte[]\u0026gt; producer = pulsarClient.newProducer() .topic(\u0026#34;persistent://sherlock-api-tenant-1/sherlock-namespace-1/topic_8\u0026#34;) .create(); User user = new User(); user.setName(\u0026#34;老六\u0026#34;); user.setAge(21); user.setAddress(\u0026#34;海南\u0026#34;); //由用户自行做序列化逻辑 producer.send(JSON.toJSONString(user).getBytes()); producer.close(); pulsarClient.close(); } catch (Exception e) { } } 消费者逻辑代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public static void customSchemaConsumer() { try { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Consumer\u0026lt;byte[]\u0026gt; consumer = pulsarClient.newConsumer() .topic(\u0026#34;persistent://sherlock-api-tenant-1/sherlock-namespace-1/topic_8\u0026#34;) .subscriptionName(\u0026#34;sub_03\u0026#34;) .subscribe(); while(true) { Message\u0026lt;byte[]\u0026gt; message = consumer.receive(); //由用户自行做序列化逻辑 byte[] user = message.getValue(); System.out.println(\u0026#34;消息数据为：\u0026#34;+JSON.parseObject(user, User.class).toString()); consumer.acknowledge(message); //consumer.negativeAcknowledge(message); } } catch (Exception e) { } } 执行效果如下\n1 消息数据为：User{name=\u0026#39;老六\u0026#39;, age=21, address=\u0026#39;海南\u0026#39;} server-side 生产者代码逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 //schema在第一次写入的时候就已经决定好了，后续用其他的schema消息类型会写入失败 public static void customSchemaProducer() { try { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); //由Pulsar做序列化逻辑 Producer\u0026lt;User\u0026gt; producer = pulsarClient.newProducer(AvroSchema.of(User.class)) .topic(\u0026#34;persistent://sherlock-api-tenant-1/sherlock-namespace-1/topic_7\u0026#34;) .create(); User user = new User(); user.setName(\u0026#34;王武\u0026#34;); user.setAge(36); user.setAddress(\u0026#34;海南\u0026#34;); producer.send(user); producer.close(); pulsarClient.close(); } catch (Exception e) { } } 消费者逻辑代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public static void customSchemaConsumer() { try { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); //由Pulsar做序列化逻辑 Consumer\u0026lt;User\u0026gt; consumer = pulsarClient.newConsumer(AvroSchema.of(User.class)) .topic(\u0026#34;persistent://sherlock-api-tenant-1/sherlock-namespace-1/topic_7\u0026#34;) .subscriptionName(\u0026#34;sub_03\u0026#34;) .subscribe(); while(true) { Message\u0026lt;User\u0026gt; message = consumer.receive(); User user = message.getValue(); System.out.println(\u0026#34;消息数据为：\u0026#34;+user); consumer.acknowledge(message); //consumer.negativeAcknowledge(message); } } catch (Exception e) { } } 执行效果如下\n1 消息数据为：User{name=\u0026#39;王武\u0026#39;, age=36, address=\u0026#39;海南\u0026#39;} 小结 分别查看两个Topic的Schema信息如下图\n通过查询client-side的Schema信息，会发现Pulsar服务端其实并没有进行存储，相当于不指定Schema的话Pulsar默认都用byte数组\n再来看看server-side的Schema信息，可以看到打印如下，namespace是pojo类的包路径，name是pojo类名，然后fields就是pojo类的各个字段的属性(像不像mysql里面的表结构，不少场景Topic就是当作表来用的)，然后type是AVRO是由于咱们是用的avro进行序列化的。\n除了在读写数据时指定Schema，Pulsar还支持通过admin管理流提前指定好，具体指令在这里。如果是用Pulsar来作为实时数仓场景，强烈建议提前通过admin管理流进行指定好，配置isSchemaValidationEnforced可以考虑开启。如果条件允许可以考虑做成服务化，例如通过Web页面提供新建Schema、修改Schema操作并接入公司内部的审批流等\n1 2 3 pulsar-admin schemas upload --filename POST /admin/v2/schemas/:tenant/:namespace/:topic/schema pulsar-admin schemas get sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_3 原理解析 Schema相关的流程咱们需要关注以下几个\n注册Schema流程 生产者端侧 消费者端侧 指定服务器 Schema生效流程 更新Schema流程 注册Schema流程 生产者端侧\n生产者实例会在内部构造schema实例，生产者会通过它对数据进行转换 生产者会请求连接Broker，并传递schema信息 SchemaInfo Broker会在schema registry中查找这个schema是否被注册，如果已经注册了就将注册的schema版本返回给生产者 Broker检查是否支持自动更新schema，如果配置不允许自动更新，则这个schema不能被注册并且拒绝生产者 Broker进行schema兼容性检查，如果通过检查则将此schema存储在schema registry并返回版本给生产者，生产者所有消息以这个schema格式进行发送；若是检查没通过则拒绝生产者 消费者端\n消费者实例会在内部构造schema实例 消费者请求连接Broker，并传递schema信息 SchemaInfo Broker检查这个Topic是否已经在使用，有的话跳到第五步，否则跳到第四步 Broker检查是否支持自动更新schema，如果支持则注册这个schema，否则拒绝客户端 Broker进行schema兼容性检查，通过则连接否则拒绝客户端 源码解析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public static void customSchemaProducer() { try { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;User\u0026gt; producer = pulsarClient.newProducer(AvroSchema.of(User.class)) .topic(\u0026#34;persistent://sherlock-api-tenant-1/sherlock-namespace-1/topic_7\u0026#34;) .create(); User user = new User(); user.setName(\u0026#34;王武\u0026#34;); user.setAge(36); user.setAddress(\u0026#34;海南\u0026#34;); producer.send(user); producer.close(); pulsarClient.close(); } catch (Exception e) { } } 文献 Pulsar：Schema Registry介绍 官方文档 深度解读 Pulsar Schema ","date":"2024-09-13T10:34:56+08:00","image":"https://sherlock-lin.github.io/p/schema%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/image-20240308104136110_hu13493609005795593452.png","permalink":"https://sherlock-lin.github.io/p/schema%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/","title":"Schema深度解析"},{"content":"简析 schema是pulsar重要的功能之一，现在就一起从源码的视角看下管理流创建schema时客户端和服务端的表现\n客户端 客户端主要经历以下四个步骤\n创建Schema实例\n根据数据类型创建相对应的实例，例如Avro创建AvroSchema、JSON创建JSONSchema等\n获取处理Schema的对象\n管理流PulsarAdmin对象获取SchemasImpl对象，这个对象是专门处理所有schema相关的操作。除此之外PulsarAdmin对象还维护着Clusters、Brokers、Tenants等等管理维护集群的重要对象，通过这些对象可以很好的管理维护Pulsar集群\n构造SchemaInfo\n通过Schema实例创建其对应的SchemaInfo信息，里面就包括这个schema的名字、schema的结构化信息、schema类型等等，最后SchemaInfo这个对象会转成字符串发到服务端\n发送HTTP请求\n通过post请求将数据发到服务端，这里是通过Java Rest库javax.ws.rs-api进行处理的\n服务端 服务端主要经历以下四个步骤\n参数格式校验\n校验租户、命名空间的是否有效(判空、是否有特殊字符) 从缓存中根据Topic获取其对应的TopicName对象 权限校验\n判断是否是Topic的owner 判断当前用户是否有操作当前Topic的权限 SchemaRegistry注册schema\nSchemaRegistryService是服务端处理所有schema相关的对象，而schema相关的读写操作是依赖它的成员SchemaStorage进行处理的，SchemaStorage的最终是通过Bookkeeper客户端对象发送写请求\n写Bookkeeper\n通过LedgerHandle对象向Bookkeeper服务端发送写请求\n小结 schame新建流程概括起来就是，客户端构造schema信息，服务端负责schema校验，bookkeeper负责schema的存储\n客户端源码解析 源码跟踪 下面是通过管理流创建schema的样例代码，核心就是通过PulsarAdmin.schemas获取schema对象，这个schema对象负责所有客户端跟schema相关的操作，包括schema的增删改查等。通过方法的第二个参数可以看到是通过Schema接口提供的静态方法AVRO来构造Avro格式的schema对象，除此之外Schema接口还提供了诸如JSON、KeyValue、PROTOBUF等静态方法提供对应数据格式的schema对象，这里如果将这块构造schema对象逻辑抽成简单工厂模式可能会更合适些\n接下来就进入createSchema方法，顾名思义可以知道这个方法就是用于创建schema的，第一个参数是topic，第二个参数是SchemaInfo对象，这个对象包含了所有要新建的schema信息，这里会将它转换为PostSchemaPayload对象传递给下一个方法。PostSchemaPayload是用来请求到服务端的参数\n这个方法并不会有返回值，sync方法是处理异步结果对象，它在正常写成功情况下不会做任何操作，但如果有什么错误会往外抛出异常。这里核心逻辑是在createSchemaAsync方法\n可以看到这个方法的返回值是个异步对象，146行这里会获取当前topic对应的TopicName对象，并通过schemaPath方法构造WebTarget对象，这个对象中就包含着要请求的HTTP地址，主要是根据当前Topic的版本来决定请求服务端哪个版本的处理方法。除此之外还可以看到有通过Entity.json方法将PostSchemaPayload对象转换为HTTP请求的参数对象，转换逻辑是javax.ws.rs-api这个网络库封装的，就不进行跟踪了\n这里就是客户端最后发送的地方，request方法中还会发送前的安全相关检查，async方法基本上就说明本次HTTP请求是异步的，而post方法也能看得出，这是一个POST类型的HTTP请求，再往后就是将请求发送出去了\n不知是否有人好奇参数WebTarget长什么样子，通过通过调试可以看到值为\n/admin/v2/schemas/public/test-namespace-jytixthzgatgirem/test-multi-version-schema-one/schema\n此值仅供学习参考，具体这个值的构造逻辑如下\n小结 简单归纳如下\n通过Schema接口构造对应数据格式的schema对象，由此对象可得到schema相关的元信息SchemaInfo 构造请求目标的HTTP地址 通过javax.ws.rs-api提供的库发送异步HTTP请求到服务端 服务端源码解析 源码跟踪 服务端的接收逻辑在SchemasResource类，这个类在org.apache.pulsar.broker.admin包下，这个包下全是处理管理流相关的操作，如果有做pulsar平台化需求的，这个包下的相关逻辑值得一读。\n再来看看postSchema方法，首先是validateTopicName方法，这个方法就是对入参进行判空、是否有特殊字符做检查；接下来就是核心方法postSchemaAsync，通过方法名可以推断出这是个异步处理schema写请求的方法\npostSchemaAsync方法看似复杂，实际上核心的就是133行，其余的方法大概说一下，validateOwnershipAndOperationAsync方法主要检查当前用户是否有新建schema的操作权限，getSchemaCompatibilityStrategyAsyncWithoutAuth方法相对复杂一些，放到后面详细讲解。那么再看回133行，其中getSchemaRegistryService方法获取的是SchemaRegistryServiceImpl对象，顾名思义可以知道Pulsar的SchemaResistry相关的功能都是由它进行处理，现在先看它的putSchemaIfAbsent方法\nSchemaStorage对象是SchemaRegistryServiceImpl的核心成员，负责schema存储相关的操作。在新建schema时会调用它的put方法进行创建；这里有个trimDeletedSchemaAndGetList方法，如果put方法在创建schema时有任何异常，则此方法会去删除该新建的schema，避免写\u0026quot;一半\u0026quot;的情况发生，某种意义上这也是一种回滚的设计。\n这里的getAll方法很重要，会根据schema的id来查询是否已经存在当前schema，有的话则将版本号加1。处理完之后就调用put方法\n这里没什么逻辑，继续往下跟踪\ngetSchemaLocator方法会构造LocatorEntry对象，调用putSchema\n由于是初次创建schema，因此直接走到337行；如果这个topic已经创建过schema则会读取之前的schema信息再新增，同时把版本号自增\n在这里可以看得到构造IndexEntry对象，这是消息的索引对象，后续用来加速查询schema\n这个方法的内容就很眼熟了(bookkeeper相关内容)，createLedger方法会先创建这个Ledger\n在576行可以看到最终调用bookkeeper创建这个Ledger\n再来看看addEntry方法，这里核心也是调用bookkeeper的ledgerHandle进行数据写入\n这个方法是属于Bookkeeper客户端的逻辑了，通过方法注释可以看到，这个方法负责将数据异步写入到一个打开的Ledger。Bookkeeper相关的逻辑后续在单独写post进行讲解\n小结 简单归纳如下\n参数格式校验、操作权限校验 查询当前Topic是否已经创建过schema，有则以插入时版本号自增 如果是初次创建Schema，则调用bookkeeper创建Ledger 往这个Schema对应的Ledger内插入schema元数据信息 其他 序列化对象创建流程 现在再专门来看看序列化对象的创建过程，回到开头管理流创建schema的地方，Schema.AVRO方法是咱们本次要看的\n通过注释可以看到，此静态方法是创建一个Avro类型的schema对象，getDefaultImplementation方法是获取实现类(饿汉单例设计模式)，而newAvroSchema方法才是本次要看的\n继续往下跟踪\n获取对应处理的类加载器，并通过对应的类加载器创建AvroSchema实例\n54行是核心，其他的都是赋值操作\nsuper调用父类构造函数做赋值操作，还是继续看\n继续跟踪parse逻辑\nFACTORY.createParser方法是jackson的方法，用于创建JsonParser对象的；因此继续跟踪parse方法\n1471行可以看到返回了我们想要的Schema对象，那么Schema.parse方法就是重中之重\n这个方法是核心，本身会递归的进行解析赋值给schema对象\n相信读者读到这里也好奇schema长什么样，因此提供下图让读者感受下，能大概推测得出来这里已经涵盖了schema的结构信息了\ngetSchemaCompatibilityStrategyAsyncWithoutAuth方法 AdminResource#getSchemaCompatibilityStrategyAsyncWithoutAuth方法是在服务端处理schema创建请求阶段会调用的方法，现在就一起跟踪看看\n731行和739行分别是获取Topic级别和Namespace级别的schema兼容策略，如果没有定义则默认自动更新。例如Topic A之前已经创建过schema1，那么如果此时再发起schema2创建请求，则服务端会继续保存并且生效schema2，只不过它的版本号会进行累加，当然，也可以配置为不支持schema策略不支持更新，一旦确定了后就不允许再变更\n总结 相信大家对schema创建的流程已经很清楚了，再次简单归纳下\n客户端根据用户定义的结构信息创建对应的Schema对象，并将结构信息以HTTP请求发给服务端 服务端检测并根据Schema兼容策略做相对应的处理，一般情况下会调用Bookkeeper创建Ledger以及Entry Bookkeeper将此Schema数据持久化到磁盘，相当于Schema信息会被Bookkeeper当作一条消息进行存储 这基本上就是全部内容，当然细节感兴趣的小伙伴可以自行跟踪代码，相信你会有更多收获～\n","date":"2024-08-01T10:02:55+08:00","image":"https://sherlock-lin.github.io/p/%E7%AE%A1%E7%90%86%E6%B5%81%E5%88%9B%E5%BB%BAschema%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/image-20240801111546500_hu11517481045871944850.png","permalink":"https://sherlock-lin.github.io/p/%E7%AE%A1%E7%90%86%E6%B5%81%E5%88%9B%E5%BB%BAschema%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","title":"管理流创建schema流程源码解析"},{"content":"Pulsar客户端简析 pulsar服务是经典的C/S架构，由客户端和服务端构成。服务端提供处理读写请求服务，客户端负责发起读写请求。pulsar将客户端按照读写分成了生产者和消费者，但是无论怎么分，它们本质上都是Pulsar客户端并有很多相同的地方，本篇就针对客户端进行分析。\n下图是客户端的组成部分\nConnectionPool连接池(值得深挖)\n连接池是客户端的核心，维护着跟服务端的TCP连接，客户端的读写(生产者/消费者)都强依赖网络连接，因为最后数据还是要通过底层的网络进行通信\nLookupService\n具有路有属性的客户端角色，负责查找Topic归属的Broker节点\n线程池\n事件线程池\n外部线程池\n内部线程池\n创建客户端对象 创建流程 先看看下面一段代码\n这是通过生产者写数据到Pulsar服务端的case，相信大家不会感到陌生，其中34行就是创建客户端的逻辑，因此就从这里进行跟踪\n通过跟踪PulsarClient.builder方法可以看到这是一个静态方法，可以看到返回值是ClientBuilder对象，具体实现逻辑咱们先不关心，咱们只需要知道这个方法会创建一个ClientBuilder就够了，结合之前的创建的逻辑可以知道最终会走到ClientBuilder#build方法\nbuild方法中核心就是50行的通过PulsarClientImpl构造函数创建客户端对象，其他的都是一些配置校验工作。下面就直接进入PulsarClientImpl的构造函数进行分析\n这个构造方法虽然逻辑不少，但实际上重要的就是201行的初始化\n创建网络连接池 初始化 这里的ClientCnx对象非常重要，后面再细讲。现在先继续跟踪构造函数的逻辑\n这里初始化了Netty客户端，在跟外部创建网络连接时直接通过Netty客户端进行即可。客户端初始化阶段，网络连接池基本上就做了这些事，那么它是怎么工作的呢，让我们来往下看\n工作流程 在创建生产者对象时，会走到PulsarClientImpl#getConnection这个方法，从这里开始进行分析\n此方法第一行是通过Lookup服务查找Topic归属的Broker节点信息，Lookup相关的感兴趣的可以看 Apache Pulsar源码解析之Lookup机制 这篇文章，在969行会将Lookup查到的Broker地址进行创建网络连接\n可以看到获取网络连接方法最终会调用ConnectionPool类的方法进行获取\n这里会根据参数randomKey(Broker IP地址)进行网络连接的复用，如果还没创建的话则进行创建，同时检查清理不可用的网络连接进行释放\n调用createConnection方法进行连接创建，同时也在通道创建通道成功后绑定监听器，在通道被关闭时一起关闭当前的网络连接。\n将unresolvedPhysicalAddress解析出正确的地址，现在还不太清楚已经有了logicalAddress(目标Broker地址)，还解析这个的用途是什么\n调用connectToAddress方法创建网络连接，如果出现异常时，如果服务端在Lookup阶段返回不止一个地址，那么就尝试跟下一个地址创建网络连接\n这里应该就很熟悉了，就是通过Netty客户端创建跟目标Broker节点的TCP连接。\n小结 连接池是一个ConcurrentHashMap，key是IP+端口，value是ClientCnx来缓存这些连接。ClientCnx除了管理连接还管理所有的业务命令，例如发送消息是Send命令，服务端会对应一个handleSend方法来处理这个命令。\n连接池是客户端最重要的内容，无论是生产者还是消费者在创建的时候，都会去连接池获取/创建跟Broker的网络连接，后续的数据读写本质上都是通过Netty的channel进行的，因此可见它是相当重要的。连接池的设计将网络连接跟其他读写功能剥离开来达到职责分离的效果，同时通过池化概念，在多个生产者/消费者情况下不仅节约了网络连接的创建，同时还提升网络连接创建的性能(复用思路)。\n其他功能 LookupService Lookup机制工作逻辑在 Apache Pulsar源码解析之Lookup机制 这篇文章已经说明了，这里主要一起看看它的创建逻辑。在PulsarClientImpl构造函数中，我们能看到以下代码\nPulsar支持两种Lookup实现，一种是通过http协议去跟服务端通信，另一种是通过二进制方式查询(性能更好)。本次就跟着比较常见的http方式进行分析\n通过构造函数可以看到是通过创建HttpClient对象进行的，这里我一开始以为是用的开源的HttpClient就没继续跟，后来在调试的时候跟进去才发现，这是Pulsar 自定义对象\n跟踪进去看，其他都是赋值、安全检查的工作，核心在161行的httpClient创建。 这里可以看到是创建的AsyncHttpClient对象，这是一个封装Netty的async-http-client-2.12.1.jar的外部包，这是支持异步处理的高性能HTTP工具包\nMemoryLimitController 由于MemoryLimitController的内容不多，就看看它的创建以及工作流程\n在PulsarClientImpl的构造函数中可以看到会调用MemoryLimitController构造函数进行创造，从这里进去分析\n构造逻辑很简单，就只有赋值操作，到这里就成功创建MemoryLimitController对象了，那就再看看它是怎么工作的吧。MemoryLimitController的核心工作逻辑是checkTrigger方法\n这里会调用trigger的run方法，这个trigger的逻辑其实就是创建MemoryLimitController的构造方法第三个参数，也就是上面的reduceConsumerReceiverQueueSize方法，那么继续跟踪\n循环调用所有的消费者的reduceCurrentReceiverQueueSize方法，也就是说MemoryLimitController目前只能限制消费者的内存。\n这里的限制逻辑相当于将所有消费者的接收队列的容量减半，避免队列中的数据占用过多内存。由此可见MemoryLimitController目前做的还是比较小范围的内存控制，并且整体逻辑并不复杂。\n线程池 客户端对象初始化时候，会创建三个线程池，现在就来分析下它们的用途\n首先是externalExecutorProvider线程池通过查看调用的方法，可以看到是被消费者使用，里面的工作线程主要是用于消费服务端的消息\n再来看看internalExecutorProvider线程池，可以看到是用来是一些跟偏向Pulsar系统层面的工作\n至于scheduledExecutorProvider相信就更简单了，顾名思义都是处理一些周期性的任务，例如下面的周期性的同步Topic分区信息到客户端\n总结 PulsarClient中的EventLoopGroup负责创建TCP连接，ConnectionPool对象负责管理连接，在创建ConnectionPool对象时会通过EventLoopGroup创建连接。 PulsarClient使用Netty来创建TCP连接，并管理一个连接池和两个线程池，所有Producer和Consumer都会复用PulsarClient的连接池和线程池，这样可以避免客户端创建过多的连接和线程。因此通常一个进程中只创建一个PulsarClient，每个Topic可以自己单独创建Producer和Consumer。 由于Broker使用了Reactor模型，单线程只负责转发事件，而数据的读取、解码、处理等都是在工作线程中完成的，也就是服务端所有请求都是异步完成的；客户端Producer/Consumer都是异步的，因此不存在单连接的性能问题。 PulsarClient默认只会与每个Broker建立一个连接，如果觉得不够可以通过配置来调大。 ClientCnx非常重要，这是netty的入队处理逻辑类。后续会专门针对这个类进行解析 ","date":"2024-07-17T10:34:21+08:00","image":"https://sherlock-lin.github.io/p/pulsarclient%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/image-20240715212010651_hu12175125588812162081.png","permalink":"https://sherlock-lin.github.io/p/pulsarclient%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","title":"PulsarClient源码解析"},{"content":"引言 知乎上有一个热门的问题，什么是人生的顶级享受？ 看到这个问题我回想到了多年前，那个炎热的午后，我在学校的图书馆里，一边参照书籍，一边用着一部破旧的thinkpad敲着一行行Java代码，写一会执行调试一下，或皱眉，或喜笑颜开，这是在外人的视角；而内心世界里，我仿佛就是一个无所不能的魔法师🧙‍♀️，一挥动魔法棒🪄，高楼大厦拔地而起，再念一段咒语，山川河流、各个形态的动物都随我所想的模样出现，最后大喝一声，“启动～”，这个“世界”就仿佛被赋予了生命力一样活了过来，河流唱着歌从高山流下来，经过嬉闹的丛林，丛林里小鹿在跟蝴蝶🦋嬉闹，此时池边蛰伏已久的鳄鱼🐊正准备扑向小鹿，只听我大喝一声，“停～”，整个世界就像按下了暂停键，所有东西都停止在那一刻，于是乎我敲着键盘重新编码鳄鱼的行为，鳄鱼不需要捕食其他动物也能存活，只需要通过呼吸和喝水也能获得足够能量。有人说，你这不胡扯吗，这怎么可能？但这就是可以的，因为这是属于我的世界，这就是编程的世界。\n编程的本质 编程的本质是什么，过去我经验尚浅不敢写这种内容，现如今工作久了脸皮厚了，尝试写一下吧。编程跟其他所有脑力密集型行业一样，本质工作都是设计。政治家的设计，是解决如何让一个国家变得更加富强；建筑师的设计，是解决如何让大楼可以同时兼顾稳定、优雅美观；飞机建造者的设计，是解决如何低成本建造出性能优越的飞机等等，而编程也是一样，程序员也就是coder解决的是，如何用软件解决真实世界的各个业务场景的问题，同时在性能和成本方面都要有一定的要求。但有别于其他行业，编程可以让设计者快速、低成本的验证自己的想法，设计飞机的人，要是下一秒就能看到自己亲手设计的飞机翱翔在天空中的姿态，设计社会框架例如理想国的人，要是下一秒就能看到自己设计的国度等等，那会是多么幸福的一件事情，而编程恰恰就可以！在大部分场景，只需要在编译器中执行一下，自己的想法立马就能得到验证。因此，虽然编程发展了几十年，相关的编程语言数不胜数，涉及的领域从最开始的破译军事机密、到原子弹研究逐步到现在的互联网商业，再到元宇宙、GPT等等，但万物不离其宗，本质都是通过设计来解决特定场景的问题。因此，程序员的核心竞争力应该是思维或者是设计能力，而远远不是具体的某一门编程语言，某一项技术，更不应该是某个职称。因此笔者认为，如果热爱编程，应该更看重如何提升自己设计能力以及解决问题能力，切勿因为一些非重要的事情乱了道心。\n工作与编程 大概是从12年左右开始，国内互联网行业迎来了大爆发，由于行业红利，大批人转到互联网行业来，相关的培训机构也如雨后春笋增长了起来，大幅降低了程序员的门槛。因此互联网行业鱼龙混杂，有特别特别牛的大佬，同时也有很多不具备所谓编程思维的程序员，而目前互联网行业的公司，大部分的工作内容基本都是CURD，更有甚者基本都不编程，每天开不完的会以及拉通对齐。在这里并不是要批判什么，而是想跟热爱编程的伙伴说，即便工作很忙以及工作上很少用到编程，但也请别放弃对编程的热爱。过去我认为对于程序员来说，工作等同于编程，工作需要什么就去学什么以及使用什么，而因为种种因素，最终逐渐丧失对编程的热爱。而现在，我跟喜欢将工作与编程分开，就像两个进程一样互相资源隔离，但又偶尔保持联系。一方面在将工作做好的同时也提升自己的沟通表达、拆解抽象问题的能力，这个过程中学到的东西是有助于咱们去学编程；另一方面，在学习编程时，提升对技术的深广度理解，又能更好的作用于日常的工作，而脱离了工作的限制，咱们可以大幅发挥自己的热爱，内心真心想学什么就大胆去学，不用考虑具体的技术是否能应用到当前的工作，尽可能的深挖，因为技术都是自相似的，任何东西学到一定的深度要考虑的问题以及解决方式都是相似的，但区别就在于需要你静下心来去思考沉淀。千万不要因为社会上的一些焦虑言论止步探索的那颗心，编程跟其他技能爱好一样，需要大量的时间沉淀磨练，相对应的，它也会给你很丰厚的反馈；因此，好好享受编程吧～\n编程的未来 现如今，随着GPT、人工智能的爆火，也逐步出现自动写代码的程序，也许未来真的可以取代人类进行编程。针对这个情况，我也分享下我的看法\n思维 如果编程水平一直停留在入门级别，那确实很容易被取代，即便不是人工智能，也同样会有很多其他比你厉害的人取代你，因此我们需要不断提升自己的技术能力以及业务能力，如果你能对业务很了解并且能用对应的技术实现业务并解决业务的痛点，那么恭喜你，基本上不用担心被人工智能取代，而是应该抱着积极的心态，因为人工智能说白了也是工具，需要人去驾驭，而你通过驾驭它可以比别人更好的完成业务，这是一件值得兴奋的事情 自我修行 退一步来说，即使在未来人工智能已彻底取代人类编程的工作，我们真的就要放弃编程吗？同理放在其他场景，现在人工智能击败了柯洁，那么人类就不用围棋了吗？如果有一天人工智能也能拉出很好听的小提琴曲子，人类就彻底放弃小提琴了吗？ 因此这个问题其实是有点荒谬的，说到底，其实像这些技能本质上是 道-术-器中的器，是要通过器来习得术和道，这个过程只能自己来，没有人能替我们走，人工智能更不行。因此现在能看到，即便机器臂已经能发挥比人类很大的力量，依然有很多人在锻炼自己的身体肌肉，即便人工智能不停的在各个领域打破一些人的记录，但一样有无数的在在这个领域继续自我突破。说白了，这不是人跟人工智能的较量，而是一场人跟自己的较量，因此无论人工智能发展到何种程度，人类依然不会停止修行突破，那么多行业都如此，编程怎会例外 最后，我非常庆幸能成为一名coder，我周围所看到的程序员伙伴都是比较单纯、朴素、和对生活充满热爱的一群人，跟大家伙一样，我们也在用自己的方式给这个世界添砖加瓦，发光发热～\n","date":"2024-04-28T15:09:13+08:00","image":"https://sherlock-lin.github.io/p/%E7%BC%96%E7%A8%8B%E6%9C%AC%E6%BA%90/image-20240920151110972_hu12095366861263762317.png","permalink":"https://sherlock-lin.github.io/p/%E7%BC%96%E7%A8%8B%E6%9C%AC%E6%BA%90/","title":"编程本源"},{"content":"引言 你是否遇到以下问题\n时间过得很快，不知道过去在忙什么 事情很多很杂，感觉一直都很忙但是好像也没啥收获 生活、工作中不顺心的事情很多，心里比较烦躁压抑 那么可以尝试参考《奇特的一生》中的男主一样，尝试将每天重要的事情、思考记录下来，通过每天输出记录来促进自己反思，同时文字还有疗伤的功效，很多气愤恼怒的问题，在你用文字输出后情绪能能得到不少的缓解。有人可能会觉得每天都很忙了哪有时间来进行记录，其实不然，每天只需要花不到十分钟的时间记录核心的一件事情、分析原因以及改进措施就够了，所带来的好处我总结了下大概有以下三点\n节省生命 提升感知细节的能力 正视痛苦 节省生命 我们平时会是不是冒出一些想法💡、灵感，如果不记录下来很快就会遗忘从而导致浪费生命，例如本人之前在处理一个问题时最后所探索出来的解决方式之前就有思考🤔过，但是由于没记录下来又白白浪费这次的时间。因为灵感虽然是一瞬间冒出来的，但是背后却是一段生活经历的积累。如果不记录下来后面可能还是会不断的出现导致生命变得低效。 另一方面，我们每天要面对的事情非常的多，大致可以分为重要紧急、重要不紧急、不重要紧急，不重要不紧急。有时候我们可能会陷入里面不那么重要的事情，导致影响了重要紧急的事情，这是由于当局者迷，而记录的方式可以帮助我们以第三人称的视角的审视，进而调整自己的工作安排，更加有的放矢。\n感知细节的能力 在不反思的情况下，我们的生活会变得很“粗糙”从而导致虽然做了很多事但是也没有感觉，而通过记录就可以从很多微小的时间从捕捉感触合馆来呢，一个动作、一句话、一个场景、一个选择、一种情绪等都会让人产生感悟，甚至每天心中都会去反思这件事，这回让我们对生活的觉知有很大的提升。通过每天定期记录，我们自己也会下意识的去思考，现在在做的这件事是什么？为什么要去做？怎么样可以做得更好等等，从而偏向于做更重要的事情并且放更多的精力在这件事情上，由于思考投入比较多，我们对这件事的细节流程也会格外的印象深刻。\n正视痛苦 人生在世，难免会遇到很多耿耿于怀、气愤、悲伤的事情，此时大多数人会选择逃避，沉浸在负面情绪中。而反思天然就有正视痛苦的力量，通过反思，通过记录，你就会以第三人称的视角，像是看别人的故事一般客观的去分析思考这件事情，通过客观的思考分析就能将情绪和事件抽离开来，针对事件中自己需要改进的点记录下来避免下次再犯，进而得到提升以及成长。这才是痛苦促进人进步的核心，因此即便遇到再难过的事情都可以尝试记录下来分析，久而久之再遇到痛苦悲伤的事情 我们也不会那么害怕，因为换个视角这又是一次提升自我的机会。\n总结 最后，我祝愿大家身体健康、万事顺心，如果不行的话那就祝愿大家都有面对、解决痛苦的能力，以及过上高效、快意的人生～\n","date":"2024-04-19T15:09:14+08:00","image":"https://sherlock-lin.github.io/p/%E8%AE%B0%E5%BD%95%E6%96%B9%E5%BC%8F%E9%87%8D%E6%96%B0%E6%89%93%E5%BC%80%E4%BA%BA%E7%94%9F/f6bca0216ea5e504d6fd85e06d61631d_hu17895883417847532878.png","permalink":"https://sherlock-lin.github.io/p/%E8%AE%B0%E5%BD%95%E6%96%B9%E5%BC%8F%E9%87%8D%E6%96%B0%E6%89%93%E5%BC%80%E4%BA%BA%E7%94%9F/","title":"记录方式重新打开人生"},{"content":"介绍 在软件设计中，为了方便能够应对不同的场景，一般在一些容易有差异的环节会考虑允许用户自定义逻辑，拦截器就是其中的一种实现方式，像Spring、Kafka、Pulsar等都支持这种方式。流程简化起来就如下图，客户端跟服务端的写消息请求和接收请求都要先通过一遍拦截器，因此用户都过自定义拦截器逻辑就能以一种无侵入、规范化的方式来改动消息发送以及处理响应的行为。\n使用 ProducerInterceptor接口 ProducerInterceptor是Pulsar提供的接口，通过实现该接口用户可以在消息发送和发送成功阶段注入自定义的逻辑来扩展Pulsar客户端的能力，进而优雅的解决某些场景的问题。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @InterfaceAudience.Public @InterfaceStability.Stable public interface ProducerInterceptor extends AutoCloseable { void close(); boolean eligible(Message message); Message beforeSend(Producer producer, Message message); void onSendAcknowledgement( Producer producer, Message message, MessageId msgId, Throwable exception); default void onPartitionsChange(String topicName, int partitions) { } } 这里针对这五个方法大概介绍下\nclose：由于该接口实现了AutoCloseable，因此也要定义生产者关闭时要释放的资源，如果没有就空着 eligible：判断拦截器针对那些消息生效，默认false不生效。这个相当于Java8 Stream里的filter，属于职责分离的设计 beforeSend：在每条消息要发送时会调用此方法，因此如果在发送前想做点什么可以考虑在这里实现 onSendAcknowledgement：在每条消息消息发送服务端响应后(无论成功失败)会调用此方法 onPartitionsChange：在分区数有变动的时候会调用这里的逻辑。这是3.2版本新加的逻辑，2.8以及之前的版本没有此接口 实现之统计 这里对生产者累计发送的消息条数进行统计，实现逻辑如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 public class SherlockCountProducerInterceptor implements ProducerInterceptor { private AtomicLong count = new AtomicLong(1); @Override public void close() { } @Override public boolean eligible(Message message) { return true; } @Override public Message beforeSend(Producer producer, Message message) { System.out.println(\u0026#34;累计发送消息条数：\u0026#34;+count.getAndIncrement()); return message; } @Override public void onSendAcknowledgement(Producer producer, Message message, MessageId msgId, Throwable exception) { } } 逻辑比较简单，其实就是通过一个计数器在每次发送时进行加1即可，并且在eligible中返回true也就是对所有发送的消息都生效，每条消息在发送前都会调用一次beforeSend方法进行自增操作并打印出来。\n现在拦截器的逻辑已经定义好了，接下来怎么使用呢，请继续往下看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public static void customInterceptorProducer() throws Exception { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(Schema.STRING) .topic(\u0026#34;sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_3\u0026#34;) .intercept(new SherlockCountProducerInterceptor())\t//拦截器生效逻辑 .create(); for (int i = 0; i \u0026lt; 200; i++) { producer.send(\u0026#34;hello java API pulsar:\u0026#34;+i+\u0026#34;, 当前时间为：\u0026#34;+new Date()); } producer.close(); pulsarClient.close(); } 上述就是使用拦截器的case，通过这种方式就能轻松的定义所需要注入的逻辑。上述代码执行后输出如下\n可以看到我们通过拦截器完成了消息发送的统计功能，可以发散设想一想，像根据不同key进行分组统计、统计某个时间段消息发送失败的条数等功能也同样可以通过拦截器实现。\n实现之二次处理 实现统计感觉还不得劲，再折腾一个。假设咱们的生产者中会发送很多地区的消息，这些消息有些是中国的，有些是新加坡的，有些是巴西的，这个时候它们的时间就有歧义了，因为不同时区的时间是有差异的，那咱们尝试用拦截器来实现一下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 public class SherlockAdapterTimeProducerInterceptor implements ProducerInterceptor { @Override public void close() { } @Override public boolean eligible(Message message) { // if (\u0026#34;V3\u0026#34;.equals(String.valueOf(message.getSchemaVersion()))) { // return true; // } if (\u0026#34;Singapore\u0026#34;.equals(message.getKey())) { System.out.println(\u0026#34;这条消息是新加坡地区的，进行处理！\u0026#34;); return true; } System.out.println(\u0026#34;这条消息是中国地区的，不进行处理！\u0026#34;); return false; } @Override public Message beforeSend(Producer producer, Message message) { System.out.println(\u0026#34;拦截到一条新加坡地区的消息，现在进行处理，消息内容为：\u0026#34;+message.getValue()); return message; } @Override public void onSendAcknowledgement(Producer producer, Message message, MessageId msgId, Throwable exception) { } } 上面就是demo，继续将这个拦截器应用于生产者\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public static void customInterceptorProducer() throws Exception { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(Schema.STRING) .topic(\u0026#34;sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_3\u0026#34;) .intercept(new SherlockAdapterTimeProducerInterceptor()) .create(); producer.newMessage().key(\u0026#34;China\u0026#34;).value(\u0026#34;下单动作\u0026#34;).send(); producer.newMessage().key(\u0026#34;Singapore\u0026#34;).value(\u0026#34;收藏动作\u0026#34;).send(); producer.newMessage().key(\u0026#34;China\u0026#34;).value(\u0026#34;取消动作\u0026#34;).send(); producer.newMessage().key(\u0026#34;Singapore\u0026#34;).value(\u0026#34;订阅动作\u0026#34;).send(); producer.close(); pulsarClient.close(); } 执行可以看到下面的输出\n通过输出的结果可以分析出来eligible的逻辑是生效的，针对新加坡地区的消息会进行处理，而中国的消息保持不变，所有地区的时间通过此拦截器来统一成东八区的时间。\n小结 通过上述例子可以看到我们可以通过拦截器实现任意的逻辑，但是这里需要注意的是，拦截器里面尽量不要放过多的逻辑，因为这可能会影响生产者发送消息的速度，并且也容易造成处理逻辑的分散。拦截器最好是做一些校验、适配、状态记录等一些需要前置完成并且轻量级的操作。\n实现原理 初始化流程 通过使用我们可以看到在创建生产者对象是只要通过.intercept方法传入拦截器对象即可生效，那么我们就先通过这个方法来看看实现逻辑\n1 2 3 4 5 6 7 8 9 10 ProducerBuilder\u0026lt;T\u0026gt; intercept(org.apache.pulsar.client.api.interceptor.ProducerInterceptor... interceptors); public ProducerBuilder\u0026lt;T\u0026gt; intercept(ProducerInterceptor... interceptors) { if (this.interceptorList == null) { this.interceptorList = new ArrayList(); } this.interceptorList.addAll(Arrays.asList(interceptors)); return this; } 通过代码跟踪可以看到ProducerBuilderImpl方法中会先将拦截器对象集合赋值给自己的成员变量，也就是它先保存一份在后面使用。在最终调用create方法来创建Producer时，最终会走到该类的createAsync方法，核心逻辑如下\n1 2 3 4 5 public CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; createAsync() { .... return this.interceptorList != null \u0026amp;\u0026amp; this.interceptorList.size() != 0 ? this.client.createProducerAsync(this.conf, this.schema, new ProducerInterceptors(this.interceptorList)) : this.client.createProducerAsync(this.conf, this.schema, (ProducerInterceptors)null); } } 如果用户通过.intercept方法传入了自定义的拦截器，则会调用PulsarClientImpl带有拦截器对象的构造方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 public \u0026lt;T\u0026gt; CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; createProducerAsync(ProducerConfigurationData conf, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { .... //这个方法的核心逻辑就这一行，继续往下跟踪 return this.createProducerAsync(topic, conf, schema, interceptors); } private \u0026lt;T\u0026gt; CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; createProducerAsync(String topic, ProducerConfigurationData conf, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { .... //同理，核心逻辑就这一行 producer = this.newProducerImpl(topic, -1, conf, schema, interceptors, producerCreatedFuture); } protected \u0026lt;T\u0026gt; ProducerImpl\u0026lt;T\u0026gt; newProducerImpl(....) { return new ProducerImpl(this, topic, conf, producerCreatedFuture, partitionIndex, schema, interceptors); } public ProducerImpl(PulsarClientImpl client, String topic, ProducerConfigurationData conf, CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture, int partitionIndex, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { //只有这里有用到，继续跟踪 super(client, topic, conf, producerCreatedFuture, schema, interceptors); .... } protected ProducerBase(PulsarClientImpl client, String topic, ProducerConfigurationData conf, CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { .... //对父类的成员变量进行赋值 this.interceptors = interceptors; } 通过上面的代码跟踪我们可以知道，当我们通过拦截器创建的Producer对象，它是有在内部维护一个ProducerInterceptors对象来存储我们所指定的拦截器集合的逻辑\n那么我们来看看ProducerInterceptors的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 public class ProducerInterceptors implements Closeable { .... private final List\u0026lt;ProducerInterceptor\u0026gt; interceptors;\t//存储拦截器集合逻辑 .... //在消息发送前进行触发 public Message beforeSend(Producer producer, Message message) { Message interceptorMessage = message; for (ProducerInterceptor interceptor : interceptors) { //调用拦截器的eligible方法来判断是否要对当前这条消息进行拦截处理，这个就是咱们上面实现的eligible接口 if (!interceptor.eligible(message)) { continue; } try { //循环调用拦截器集合里的每个拦截器对这条消息进行处理 interceptorMessage = interceptor.beforeSend(producer, interceptorMessage); } catch (Throwable e) { .... } } return interceptorMessage; } //逻辑跟beforeSend基本一致 public void onSendAcknowledgement(Producer producer, Message message, MessageId msgId, Throwable exception) { for (ProducerInterceptor interceptor : interceptors) { if (!interceptor.eligible(message)) { continue; } try { interceptor.onSendAcknowledgement(producer, message, msgId, exception); } catch (Throwable e) { log.warn(\u0026#34;Error executing interceptor onSendAcknowledgement callback \u0026#34;, e); } } } public void onPartitionsChange(String topicName, int partitions) { for (ProducerInterceptor interceptor : interceptors) { try { interceptor.onPartitionsChange(topicName, partitions); } catch (Throwable e) { log.warn(\u0026#34;Error executing interceptor onPartitionsChange callback \u0026#34;, e); } } } @Override public void close() throws IOException { for (ProducerInterceptor interceptor : interceptors) { try { interceptor.close(); } catch (Throwable e) { log.error(\u0026#34;Fail to close producer interceptor \u0026#34;, e); } } } } 通过上述逻辑可以看到ProducerInterceptors本质上就是个批量管理对象，符合高内聚低耦合的设计，解耦了业务逻辑循环处理的逻辑，将这些循环处理的逻辑都封装在ProducerInterceptors类里面，然后ProducerInterceptors仅对外提供触发某几个动作的api，业务只需要在哪个阶段调用这些api即可。\n生效流程 在生产者消息发送阶段，最终都会走到ProducerImpl类的internalSendAsync方法，可以看到这里会调用拦截器进行处理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CompletableFuture\u0026lt;MessageId\u0026gt; internalSendAsync(Message\u0026lt;?\u0026gt; message) { //核心方法，跟踪进去 MessageImpl\u0026lt;?\u0026gt; interceptorMessage = (MessageImpl) beforeSend(message); .... } protected Message\u0026lt;?\u0026gt; beforeSend(Message\u0026lt;?\u0026gt; message) { if (interceptors != null) { //如果配置了拦截器则调用ProducerInterceptors类的beforeSend方法 return interceptors.beforeSend(this, message); } else { //如果没有配置拦截器则直接返回原消息 return message; } } 这是消息发送的处理逻辑，那如果是再消息发送结束后触发呢？一起来跟踪看下吧，首先还是从ProducerImp类的internalSendAsync方法开始看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @Override CompletableFuture\u0026lt;MessageId\u0026gt; internalSendAsync(Message\u0026lt;?\u0026gt; message) { sendAsync(interceptorMessage, new SendCallback() { .... @Override public void sendComplete(Exception e) { try { if (e != null) { stats.incrementSendFailed(); //从这里跟踪进去看看 onSendAcknowledgement(interceptorMessage, null, e); future.completeExceptionally(e); } else { onSendAcknowledgement(interceptorMessage, interceptorMessage.getMessageId(), null); future.complete(interceptorMessage.getMessageId()); stats.incrementNumAcksReceived(System.nanoTime() - createdAt); } } finally { interceptorMessage.getDataBuffer().release(); } } protected void onSendAcknowledgement(Message\u0026lt;?\u0026gt; message, MessageId msgId, Throwable exception) { if (interceptors != null) { //可以看到最终也是调用的ProducerInterceptors类的onSendAcknowledgement方法 interceptors.onSendAcknowledgement(this, message, msgId, exception); } } 这里的设计是异步回调的方式，将调用拦截器处理逻辑封装成参数传给下一层，在消息发送完成后再调用参数里指定的回调逻辑。那么什么时候触发呢，由于Pulsar客户端跟服务端是通过Netty的TCP通信的，因此直接看看PulsarDecoder的channelRead方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { .... switch (cmd.getType()) { .... case PRODUCER_SUCCESS: //写入消息被Broker处理后会忘生产者客户端通过TCP发送一条PRODUCER_SUCCESS类型的消息也就是这里，跟踪进去看看处理逻辑 checkArgument(cmd.hasProducerSuccess()); handleProducerSuccess(cmd.getProducerSuccess()); break; } } protected void handleProducerSuccess(CommandProducerSuccess success) { .... //生产者会在队列维护每条未被ack的写入请求消息，在Broker ack时会从这个队列中移除并获取回调处理逻辑 CompletableFuture\u0026lt;ProducerResponse\u0026gt; requestFuture = (CompletableFuture\u0026lt;ProducerResponse\u0026gt;) pendingRequests.remove(requestId); if (requestFuture != null) { ProducerResponse pr = new ProducerResponse(success.getProducerName(), success.getLastSequenceId(), success.getSchemaVersion(), success.hasTopicEpoch() ? Optional.of(success.getTopicEpoch()) : Optional.empty()); //调用回调逻辑 requestFuture.complete(pr); } else { .... } } 总结 通过使用和跟踪原理，我们对Pulsar生产者拦截器有了进一步的认识，除了生产者拦截器，Pulsar还支持Broker侧以及Bookkeeper侧的拦截器，这些放到后面再跟大家一起学习。\n","date":"2024-04-19T11:54:03+08:00","image":"https://sherlock-lin.github.io/p/%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2%E7%94%9F%E4%BA%A7%E8%80%85%E6%8B%A6%E6%88%AA%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BB%A5%E5%8F%8A%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1/image-20240419110502648_hu11315331606035570821.png","permalink":"https://sherlock-lin.github.io/p/%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2%E7%94%9F%E4%BA%A7%E8%80%85%E6%8B%A6%E6%88%AA%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BB%A5%E5%8F%8A%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1/","title":"深入探索生产者拦截器的使用以及源码设计"},{"content":"正文 Namespace下的Topic是分Bundle进行管理的，每个Namespace都是一个哈希环，而Bundle负责管理环上某个范围上的Topic。通过这种方式可以更好的进行Topic的管理。当某个Bundle上负责的Topic越来越多时，会导致负责该Bundle的Broker节点压力变大。因此Pulsar还提供了Bundle分裂的机制，这个机制支持自动触发以及手动触发，今天这篇文章就从源码的角度分析手动触发Bundle分裂时服务端会发生什么\n主动触发Bundle分裂操作方式\n1 2 3 4 5 6 7 8 9 10 bin/pulsar-admin namespaces bundles public/default //输出如下 { \u0026#34;boundaries\u0026#34; : [ \u0026#34;0x00000000\u0026#34;, \u0026#34;0x08000000\u0026#34;, \u0026#34;0x10000000\u0026#34;, \u0026#34;0x20000000\u0026#34;, \u0026#34;0x30000000\u0026#34;, \u0026#34;0x40000000\u0026#34;, \u0026#34;0x50000000\u0026#34;, \u0026#34;0x60000000\u0026#34;, \u0026#34;0x70000000\u0026#34;, \u0026#34;0x80000000\u0026#34;, \u0026#34;0x90000000\u0026#34;, \u0026#34;0xa0000000\u0026#34;, \u0026#34;0xb0000000\u0026#34;, \u0026#34;0xc0000000\u0026#34;, \u0026#34;0xd0000000\u0026#34;, \u0026#34;0xe0000000\u0026#34;, \u0026#34;0xf0000000\u0026#34;, \u0026#34;0xffffffff\u0026#34; ], \u0026#34;numBundles\u0026#34; : 17 } //指定某个bundle进行分裂 bin/pulsar-admin namespaces split-bundle --bundle 0x00000000_0x10000000 public/default 源码解析 Pulsar管理流相关的操作都是通过HTTP的方式，因为需要支持多种客户端类型(http、client、cli)。服务端处理这些操作都在admin模块下，如下图，本次要聊的bundle分裂就在Namespaces方法中\n首先从Namespaces的splitNamespaceBundle进行跟踪\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public void splitNamespaceBundle( .... @QueryParam(\u0026#34;splitAlgorithmName\u0026#34;) String splitAlgorithmName, //指定bundle分裂算法 @ApiParam(\u0026#34;splitBoundaries\u0026#34;) List\u0026lt;Long\u0026gt; splitBoundaries) { //校验参数格式以及是否存在对应的namespace validateNamespaceName(tenant, namespace); //异步进行分裂操作 internalSplitNamespaceBundleAsync(bundleRange, authoritative, unload, splitAlgorithmName, splitBoundaries) .thenAccept(__ -\u0026gt; { .... }) .exceptionally(ex -\u0026gt; { .... }); } 可以看到最外层只是做些参数校验，那么就继续跟踪internalSplitNamespaceBundleAsync方法，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 protected CompletableFuture\u0026lt;Void\u0026gt; internalSplitNamespaceBundleAsync(String bundleName, boolean authoritative, boolean unload, String splitAlgorithmName, List\u0026lt;Long\u0026gt; splitBoundaries) { return validateSuperUserAccessAsync() //权限校验 .thenAccept(__ -\u0026gt; { checkNotNull(bundleName, \u0026#34;BundleRange should not be null\u0026#34;); log.info(\u0026#34;[{}] Split namespace bundle {}/{}\u0026#34;, clientAppId(), namespaceName, bundleName); //获取当前集群所支持的bundle分裂算法，这里是硬编码的固定四种 List\u0026lt;String\u0026gt; supportedNamespaceBundleSplitAlgorithms = pulsar().getConfig().getSupportedNamespaceBundleSplitAlgorithms(); //此处省去参数检查逻辑 .... } }) .thenCompose(__ -\u0026gt; { //此处省去参数检查逻辑 .... }) .thenCompose(__ -\u0026gt; validatePoliciesReadOnlyAccessAsync()) //权限校验 .thenCompose(__ -\u0026gt; getBundleRangeAsync(bundleName)) //获取要分裂的bundle的范围 .thenCompose(bundleRange -\u0026gt; { return getNamespacePoliciesAsync(namespaceName) .thenCompose(policies -\u0026gt; //1. 校验Bundle的范围是否有效 //2.判断当前Broker节点是否负责这个bundle的管理，如果不是则重定向 validateNamespaceBundleOwnershipAsync(namespaceName, policies.bundles, bundleRange,authoritative, false)) //核心方法就是这里的 NamespaceService.splitAndOwnBundle .thenCompose(nsBundle -\u0026gt; pulsar().getNamespaceService().splitAndOwnBundle(nsBundle, unload, pulsar().getNamespaceService() .getNamespaceBundleSplitAlgorithmByName(splitAlgorithmName), splitBoundaries)); }); } 接下来就是进入NamespaceService类的splitAndOwnBundle方法，NamespaceService也是Pulsar比较重要的一个类，这里先继续跟踪分割bundle的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 public CompletableFuture\u0026lt;Void\u0026gt; splitAndOwnBundle(NamespaceBundle bundle, boolean unload, NamespaceBundleSplitAlgorithm splitAlgorithm, List\u0026lt;Long\u0026gt; boundaries) { //如果实现了自定义的分割逻辑则使用自定义的 if (ExtensibleLoadManagerImpl.isLoadManagerExtensionEnabled(config)) { return ExtensibleLoadManagerImpl.get(loadManager.get()) .splitNamespaceBundleAsync(bundle, splitAlgorithm, boundaries); } final CompletableFuture\u0026lt;Void\u0026gt; unloadFuture = new CompletableFuture\u0026lt;\u0026gt;(); final AtomicInteger counter = new AtomicInteger(BUNDLE_SPLIT_RETRY_LIMIT); //核心流程流程 splitAndOwnBundleOnceAndRetry(bundle, unload, counter, unloadFuture, splitAlgorithm, boundaries); return unloadFuture; } void splitAndOwnBundleOnceAndRetry(NamespaceBundle bundle, boolean unload, AtomicInteger counter, CompletableFuture\u0026lt;Void\u0026gt; completionFuture, NamespaceBundleSplitAlgorithm splitAlgorithm, List\u0026lt;Long\u0026gt; boundaries) { //获取bundle分割配置 BundleSplitOption bundleSplitOption = getBundleSplitOption(bundle, boundaries, config); //根据配置来选择对应的分割算法进行分割 splitAlgorithm.getSplitBoundary(bundleSplitOption).whenComplete((splitBoundaries, ex) -\u0026gt; { CompletableFuture\u0026lt;List\u0026lt;NamespaceBundle\u0026gt;\u0026gt; updateFuture = new CompletableFuture\u0026lt;\u0026gt;(); if (ex == null) { .... try { //进行bundle分割操作 bundleFactory.splitBundles(bundle, splitBoundaries.size() + 1, splitBoundaries) .thenAccept(splitBundles -\u0026gt; { .... Objects.requireNonNull(splitBundles.getLeft()); Objects.requireNonNull(splitBundles.getRight()); checkArgument(splitBundles.getRight().size() == splitBoundaries.size() + 1, \u0026#34;bundle has to be split in \u0026#34; + (splitBoundaries.size() + 1) + \u0026#34; bundles\u0026#34;); NamespaceName nsname = bundle.getNamespaceObject(); .... try { // 检查确保每个Bundle都有对应的Broker负责 for (NamespaceBundle sBundle : splitBundles.getRight()) { Objects.requireNonNull(ownershipCache.tryAcquiringOwnership(sBundle)); } //更新Bundle信息，毕竟Bundle已经分裂好了，相关的一些元数据要同步更新 updateNamespaceBundles(nsname, splitBundles.getLeft()).thenCompose(__ -\u0026gt; updateNamespaceBundlesForPolicies(nsname, splitBundles.getLeft())) .thenRun(() -\u0026gt; { bundleFactory.invalidateBundleCache(bundle.getNamespaceObject()); updateFuture.complete(splitBundles.getRight()); }).exceptionally(ex1 -\u0026gt; { .... }); } catch (Exception e) { .... } }); } catch (Exception e) { .... } } else { updateFuture.completeExceptionally(ex); } updateFuture.whenCompleteAsync((r, t)-\u0026gt; { if (t != null) { // 失败则重试几次 if ((t.getCause() instanceof MetadataStoreException.BadVersionException) \u0026amp;\u0026amp; (counter.decrementAndGet() \u0026gt;= 0)) { pulsar.getExecutor().schedule(() -\u0026gt; pulsar.getOrderedExecutor() .execute(() -\u0026gt; splitAndOwnBundleOnceAndRetry( bundle, unload, counter, completionFuture, splitAlgorithm, boundaries)), 100, MILLISECONDS); } else if (t instanceof IllegalArgumentException) { completionFuture.completeExceptionally(t); } else { // Retry enough, or meet other exception String msg2 = format(\u0026#34; %s not success update nsBundles, counter %d, reason %s\u0026#34;, bundle.toString(), counter.get(), t.getMessage()); LOG.warn(msg2); completionFuture.completeExceptionally(new ServiceUnitNotReadyException(msg2)); } return; } //更新bundle的状态 getOwnershipCache().updateBundleState(bundle, false) .thenRun(() -\u0026gt; { // update bundled_topic cache for load-report-generation pulsar.getBrokerService().refreshTopicToStatsMaps(bundle); loadManager.get().setLoadReportForceUpdateFlag(); // release old bundle from ownership cache pulsar.getNamespaceService().getOwnershipCache().removeOwnership(bundle); completionFuture.complete(null); if (unload) { // Unload new split bundles, in background. This will not // affect the split operation which is already safely completed r.forEach(this::unloadNamespaceBundle); } onNamespaceBundleSplit(bundle); }) .exceptionally(e -\u0026gt; { .... }); }, pulsar.getOrderedExecutor()); }); } 上面这个方法的逻辑比较丰富，但核心的分割流程实际上是调用的NamespaceBundleFactory的splitBundles进行的，这里就继续跟踪NamespaceBundleFactory的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 public CompletableFuture\u0026lt;Pair\u0026lt;NamespaceBundles, List\u0026lt;NamespaceBundle\u0026gt;\u0026gt;\u0026gt; splitBundles( NamespaceBundle targetBundle, int argNumBundles, List\u0026lt;Long\u0026gt; splitBoundaries) { //判断当前bundle是否支持分裂 checkArgument(canSplitBundle(targetBundle), \u0026#34;%s bundle can\u0026#39;t be split further since range not larger than 1\u0026#34;, targetBundle); .... NamespaceName nsname = targetBundle.getNamespaceObject(); final int numBundles = argNumBundles; return bundlesCache.get(nsname).thenApply(sourceBundle -\u0026gt; { final int lastIndex = sourceBundle.partitions.length - 1; //重新创建数组保存哈希环的节点，因为bundle分裂后环上的节点会增多 final long[] partitions = new long[sourceBundle.partitions.length + (numBundles - 1)]; int pos = 0; int splitPartition = -1; final Range\u0026lt;Long\u0026gt; range = targetBundle.getKeyRange(); for (int i = 0; i \u0026lt; lastIndex; i++) { //遍历当前Namespace的所有Bundle来找到需要进行分裂的目标Bundle if (sourceBundle.partitions[i] == range.lowerEndpoint() \u0026amp;\u0026amp; (range.upperEndpoint() == sourceBundle.partitions[i + 1])) { splitPartition = i; long minVal = sourceBundle.partitions[i]; partitions[pos++] = minVal; if (splitBoundaries == null || splitBoundaries.size() == 0) { long maxVal = sourceBundle.partitions[i + 1]; //numBundles就是要分割成的份数，这里相当于将原先Bundle负责的范围平均分给多个新的Bundle long segSize = (maxVal - minVal) / numBundles; long curPartition = minVal + segSize; for (int j = 0; j \u0026lt; numBundles - 1; j++) { partitions[pos++] = curPartition; curPartition += segSize; } } else { for (long splitBoundary : splitBoundaries) { partitions[pos++] = splitBoundary; } } } else { partitions[pos++] = sourceBundle.partitions[i]; } } partitions[pos] = sourceBundle.partitions[lastIndex]; if (splitPartition != -1) { // keep version of sourceBundle //根据上面旧的Bundle范围划分来分裂出多个新的bundle NamespaceBundles splitNsBundles = new NamespaceBundles(nsname, this, sourceBundle.getLocalPolicies(), partitions); List\u0026lt;NamespaceBundle\u0026gt; splitBundles = splitNsBundles.getBundles().subList(splitPartition, (splitPartition + numBundles)); return new ImmutablePair\u0026lt;\u0026gt;(splitNsBundles, splitBundles); } return null; }); } 到这里基本就结束，这条链路主要是Broker将原先的哈希环中的某一个范围拆分成多个范围的逻辑，这里保留几个问题给读者思考\nBundle分裂后是否会涉及到数据的迁移？ Bundle分裂算法有四种，区别是什么？ 参考文献 官方文档 ","date":"2024-04-16T10:34:57+08:00","image":"https://sherlock-lin.github.io/p/%E4%B8%BB%E5%8A%A8%E8%A7%A6%E5%8F%91bundle%E5%88%86%E8%A3%82%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/image-20240416181912380_hu8808141309202124509.png","permalink":"https://sherlock-lin.github.io/p/%E4%B8%BB%E5%8A%A8%E8%A7%A6%E5%8F%91bundle%E5%88%86%E8%A3%82%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","title":"主动触发Bundle分裂服务端流程源码解析"},{"content":"引言 承接 手写分布式存储系统v0.2版本 ，今天开始新的迭代开发。主要实现 服务发现 功能\n什么是服务发现 由于咱们的服务是分布式的，那从服务管理的角度来看肯定是要有一个机制来知道具体都有哪些实例可以提供服务。举个例子就是，张三家里在全国各地有不少火锅加盟店，那张三肯定要有一个方式知道这些火锅店加盟店的情况。例如上海又新开了一家加盟店，那么这家加盟店肯定要先通过某种方式联系张三，这样张三才能将配方以及食材供应给这家新的加盟店等等。\n疑问\n为什么不能通过域名映射的方式来做映射，客户端通过域名调用服务就好了为啥要专门做服务发现\n答：域名映射是对外提供服务时使用的，而我们的系统还有很多场景要做内部的服务管理，例如某个节点故障了，为了服务能够继续保证高可用，咱们的分布式存储系统就要将这个节点上所管理的数据分给其余的节点进行管理等，这个时候系统内部就需要明确知道各个分布式节点的信息。\n服务发现设计 目前服务发现设计主要有以下几种\n配置化：将所有节点的信息写在服务配置里，像ES等 使用能保证一致性的外部服务：如kafka、bookkeeper等，外部服务有zookeeper、etcd、consul等 主从架构里，所有从节点启动时自动向主服务注册自己的节点信息：如hdfs、yarn等 为了方便扩展，同时咱们的存储服务能够设计成无主架构，因此采用第二种采用外部服务zookeeper来进行实现。实现的大致流程如下图\n所有节点实例在启动时，都去zookeeper上创建属于自己的目录，在节点下线时就将自己对应的目录进行删除。这样只需要监听“服务发现目录”就能知道是否有节点上下线。同时为了避免服务故障时没能正确删除自己的目录，因此咱们采用zookeeper临时目录的功能，例如节点1启动并在zookeeper创建对应临时目录后，会每隔一小段时间向zookeeper发送请求也就是心跳，证明自己的服务还正常；如果zookeeper在等待一段时间后，没收到某个节点的心跳，就会默认这个服务已经挂了并将其对应的临时目录进行删除。\n代码实现 由于把全部代码贴上来不太现实且不易于阅读，就将开发时测试样例贴上来供大家伙参考\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 package com.sherlock; import org.apache.zookeeper.*; import org.apache.zookeeper.data.ACL; import org.apache.zookeeper.data.Stat; import java.io.File; import java.util.List; import java.util.concurrent.CountDownLatch; /** * author: shalock.lin * date: 2024/2/4 * describe: */ public class BaseZookeeper implements Watcher { private static ZooKeeper zookeeper; public static void main(String[] args) throws Exception { BaseZookeeper baseZookeeper = new BaseZookeeper(); baseZookeeper.connectZookeeper(\u0026#34;127.0.0.1:2181\u0026#34;); List\u0026lt;String\u0026gt; children = baseZookeeper.getChildren(\u0026#34;/\u0026#34;); System.out.println(children); AsyncCallback.StringCallback scb = new AsyncCallback.StringCallback() { @Override public void processResult(int rc, String path, Object ctx, String name) { System.out.println(rc); } }; asyncCreateFullPathOptimistic(zookeeper, \u0026#34;/distributed-storage-system/available/shalocklindeMacBook-Pro.local\u0026#34;, \u0026#34;testData\u0026#34;.getBytes() ,ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT,scb, null); Thread.sleep(5000); List\u0026lt;String\u0026gt; afterChildren = baseZookeeper.getChildren(\u0026#34;/\u0026#34;); System.out.println(afterChildren); } /** * 超时时间 */ private static final int SESSION_TIME_OUT = 2000; private CountDownLatch countDownLatch = new CountDownLatch(1); @Override public void process(WatchedEvent event) { if (event.getState() == Event.KeeperState.SyncConnected) { System.out.println(\u0026#34;Watch received event\u0026#34;); countDownLatch.countDown(); } } /**连接zookeeper * @param host * @throws Exception */ public void connectZookeeper(String host) throws Exception{ zookeeper = new ZooKeeper(host, SESSION_TIME_OUT, this); countDownLatch.await(); System.out.println(\u0026#34;zookeeper connection success\u0026#34;); } /** * 获取路径下所有子节点 * @param path * @return * @throws KeeperException * @throws InterruptedException */ public List\u0026lt;String\u0026gt; getChildren(String path) throws KeeperException, InterruptedException{ List\u0026lt;String\u0026gt; children = zookeeper.getChildren(path, false); return children; } /** * 获取节点上面的数据 * @param path 路径 * @return * @throws KeeperException * @throws InterruptedException */ public String getData(String path) throws KeeperException, InterruptedException{ byte[] data = zookeeper.getData(path, false, null); if (data == null) { return \u0026#34;\u0026#34;; } return new String(data); } /** * 设置节点信息 * @param path 路径 * @param data 数据 * @return * @throws KeeperException * @throws InterruptedException */ public Stat setData(String path, String data) throws KeeperException, InterruptedException{ Stat stat = zookeeper.setData(path, data.getBytes(), -1); return stat; } /** * 删除节点 * @param path * @throws InterruptedException * @throws KeeperException */ public void deleteNode(String path) throws InterruptedException, KeeperException{ zookeeper.delete(path, -1); } /** * 获取某个路径下孩子的数量 * @param path * @return * @throws KeeperException * @throws InterruptedException */ public Integer getChildrenNum(String path) throws KeeperException, InterruptedException{ int childenNum = zookeeper.getChildren(path, false).size(); return childenNum; } /** * 关闭连接 * @throws InterruptedException */ public void closeConnection() throws InterruptedException{ if (zookeeper != null) { zookeeper.close(); } } public static void asyncCreateFullPathOptimistic( final ZooKeeper zk, final String originalPath, final byte[] data, final List\u0026lt;ACL\u0026gt; acl, final CreateMode createMode, final AsyncCallback.StringCallback callback, final Object ctx) { zk.create(originalPath, data, acl, createMode, new AsyncCallback.StringCallback() { @Override public void processResult(int rc, String path, Object ctx, String name) { if (rc != KeeperException.Code.NONODE.intValue()) { callback.processResult(rc, path, ctx, name); return; } // Since I got a nonode, it means that my parents don\u0026#39;t exist // create mode is persistent since ephemeral nodes can\u0026#39;t be // parents String parent = new File(originalPath).getParent().replace(\u0026#34;\\\\\u0026#34;, \u0026#34;/\u0026#34;); asyncCreateFullPathOptimistic(zk, parent, new byte[0], acl, CreateMode.PERSISTENT, new StringCallback() { @Override public void processResult(int rc, String path, Object ctx, String name) { if (rc == KeeperException.Code.OK.intValue() || rc == KeeperException.Code.NODEEXISTS.intValue()) { // succeeded in creating the parent, now // create the original path asyncCreateFullPathOptimistic(zk, originalPath, data, acl, createMode, callback, ctx); } else { callback.processResult(rc, path, ctx, name); } } }, ctx); } }, ctx); } } 功能演示 整个功能验证逻辑如下\n服务启动前观测zookeeper对应目录下不存在数据\n启动服务，从控制台能看到服务正常启动\n再观测zookeeper对应目录下注册了服务的主机名\n通过打印输出，能看到存在该目录下的服务信息(当前存的是测试样例数据)\n停止服务，并持续观测一段时间，可以看到目录已被zookeeper删除\n总结 终于开发一点跟“分布式”相关的内容了，在使用zookeeper时踩了一点坑， 启动服务时报下述异常org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for，通过调试发现zookeeper服务端返回的信息非常有限无法得出有用的信息。结果网上的答案，排除了是防火墙、超时配置等问题后，最终发现是自己在调用zookeeper创建路径是直接传了完整的路径也就是多级目录 /distributed-storage-system/available/shalocklindeMacBook-Pro.local导致的报错，原因是zookeeper不支持递归创建多级目录，只能参考bookkeeper开发工具类从代码层面递归去zookeeper创建路径。惭愧的是，已经接触zookeeper多年，并且也翻过它的代码，却连这个基本的点都不知晓。因此进一步验证上一篇的想法，就是很多东西真的要自己去实现一遍，否则只沉浸理论容易陷入一种“什么都懂”、“什么都是理所当然”的幻觉并自我感觉良好，但这很有可能会令我们的技术止步不前\n","date":"2024-04-07T20:51:00+08:00","image":"https://sherlock-lin.github.io/p/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9Fv0.3%E7%89%88%E6%9C%AC/image-20240923205148052_hu7109961498001971171.png","permalink":"https://sherlock-lin.github.io/p/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9Fv0.3%E7%89%88%E6%9C%AC/","title":"手写分布式存储系统v0.3版本"},{"content":"引言 上回说到 手写分布式存储系统v0.1版本 ，已经实现了通过监听TCP端口并将数据写到本地磁盘的功能，今天咱们就继续往上面添砖加瓦\nv0.2版本大致做以下功能\n实现滚动写文件\n代码优化\n滚动写文件实现 由于咱们写文件是用的mmap进行文件写入，而mmap自身原因最多只能映射到不大于2G的文件。因此在一个磁盘文件写满后，咱们需要滚动写到一个新的文件中，基本上所有分布式存储系统都是这么实现的，如kakfa、pulsar、rocketmq等等。那咱们也自己尝试实现下，大致逻辑如下\n这个过程中有几个点需要考虑\n如何判断文件写满了 滚动前后文件名的变化规则 第一点可以考虑在内存中维护一个整型记录当前文件的大小，否则每次写数据时判断是否写满都要去查下linux会影响性能\n第二点文件名变化规则的设计方式有较多中，例如每次写新的文件名都用最新的等。参考几个系统的实现后决定采用写指定名字的文件例如 “file”，当这个文件写满1个G时，将“file”改名为“file”加当前时间如“file-20240202”，然后再新建一个名为“file”的文件进行写入。这样就能保证“file”这个文件永远都是当前正在写入的文件，核心代码如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 private boolean rollingFile() throws IOException { preFilepath = fileName+\u0026#34;-\u0026#34;+LocalDateTime.now().toString().replace(\u0026#34;:\u0026#34;,\u0026#34;-\u0026#34;).substring(0,19); File preFile = new File(preFilepath); boolean preFileExists = preFile.exists(); if (!preFileExists) { this.fileChannel.force(false); boolean rename = file.renameTo(preFile); if (rename) { this.fileChannel = new RandomAccessFile(new File(fileName), \u0026#34;rw\u0026#34;) .getChannel(); this.mappedByteBuffer = this.fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, fileSize); WROTE_POSITION_UPDATER.set(this, 0); return true; } else { LOG.error(\u0026#34;TieredIndexFile#rollingFile: rename current file failed\u0026#34;); return false; } } return false; } 代码优化 由于v0.1版本中实现的比较莽，因此现在需要进行一个简单的重构。重构后大致逻辑可以参考下面这张不规范的UML图，首先是抽象出一个LifecycleComponent接口，由于除了网络、持久化服务之外，未来咱们可能还会有其他的服务例如监控、插件服务等等，因此咱们需要对这些服务做一层统一的抽象，所有这些服务都要提供服务启动和服务停止的接口，这样设计之后再服务启动/停止时只需要对LifecycleComponent集合列表进行统一的启动/停止操作即可，代码维护起来也很舒服。\n网络方面是通过NetServiceImpl方法初始化并启动Netty引导类ServerBootstrap，ServerBootstrap启动后会监听Linux机器的网络端口，在监听到有请求时会交给ServerHandler 进行处理，在ServerHandler这里可以调用LocalDataStorageImpl方法进行数据持久化，LocalDataStorageImpl是数据持久化的统一入口，咱们针对mmap写入方式抽象并实现了DefaultMappedFile，提供了真正的mmap写磁盘操作。基本大致逻辑就是如此，尽量不做过度的设计，好的系统是演变过来的，等未来发展到一定阶段后再根据情形进行分析优化\n功能演示 开发完后，咱们就可以开始进行演示了，启动服务后当在控制台看到以下信息就知道服务已经正常启动，此时就可以发数据给服务端了 通过指令能看到已经在目录下创建好对应的文件，由于是通过mmap方式写的数据，因此虽然咱们还没写数据到文件内，但是可以看到文件大小已经是100Byte了，这也是mmap的特点 通过以下指令往8888端口发送数据 1 (echo \u0026#39;are you ok?\u0026#39;; sleep 2) | telnet 127.0.0.1 8888 通过控制台能够看到数据有写到磁盘，并且内存中维护的文件里存放数据的大小也在增加 重复多次第4步，可以看到日志显示已到达文件大小触发文件滚动动作 再看看linux文件系统可以看到，已经创建对应的文件 testWrite-2024-02-02T19-35-20 打印一下可以清晰的看到咱们刚刚请求的内容都被正确的持久化到磁盘中了 总结 上面基本上就是 v0.2 版本的内容了，不难但是你会发现使用一个分布式存储系统、看它的源码的体验，跟你自己实现一遍是完全不同的，一个现成的组件就像是一架飞机，你看得到它的机翼、发动机等等，你知道它是这样设计的；但，它为什么是这样设计的呢？那样不可以吗，这类问题恐怕会想的比较少或者虽然想了一下但是转头就忘了。但是当你自己设计去实现的过程中，你会遇到种种问题需要你去反复思考以及做取舍等等，这些都是你真正意义上成长的过程，甚至有时还会顿悟为什么那个东西人家要这样设计，这些都是无比令人振奋的事情，这不就是生命的意义吗\n","date":"2024-04-07T20:49:15+08:00","image":"https://sherlock-lin.github.io/p/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9Fv0.2%E7%89%88%E6%9C%AC/image-20240202190044471-6871646_hu533876039371260404.png","permalink":"https://sherlock-lin.github.io/p/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9Fv0.2%E7%89%88%E6%9C%AC/","title":"手写分布式存储系统v0.2版本"},{"content":"引言 这是手写分布式存储系统v0.1版本，只有一个目标就是支持通过tcp接收数据并落地到磁盘文件(单机模式)，那接下来就开始吧\n设计 实现一个系统，设计是最过瘾的过程没有之一，类似你搭积木前在脑海设计构建一副大致的“雏形”，只有有了这个东西之后才能够指导最终实现的方向以及确保不会偏离的太差。这里我对v0.1的预期是如下的，只要客户端能够通过tcp将数据请求到Linux机器的端口，咱们的v0.1版本就能够监听到并且将数据落地到磁盘，只需要实现这个功能就可以了。\n代码实现 这个功能中会跟网络和写磁盘打交道，那直接用Netty现成的包就好了，至于写磁盘的话用JDK原生自带的就够了。大致抽象出两个对应的接口以及实现，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 public interface NetService { void start(); void stop(); } public class NetServiceImpl implements NetService{ private static final Logger LOG = LoggerFactory.getLogger(NetServiceImpl.class); private EventLoopGroup bossGroup = null; private EventLoopGroup workerGroup = null; public void start() { //bossGroup就是parentGroup，是负责处理TCP/IP连接的 EventLoopGroup bossGroup = new NioEventLoopGroup(1); //workerGroup就是childGroup,是负责处理Channel(通道) EventLoopGroup workerGroup = new NioEventLoopGroup(30); try { ServerBootstrap bootstrap = new ServerBootstrap() .group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) //初始化服务端可连接队列,指定了队列的大小128 .option(ChannelOption.SO_BACKLOG, 128) //通过NoDelay禁用Nagle,使消息立即发出去，不用等待到一定的数据量才发出去 .option(ChannelOption.TCP_NODELAY, true) //保持长连接 .childOption(ChannelOption.SO_KEEPALIVE, true) .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ServerInitializer()); ChannelFuture future = bootstrap.bind(8888).sync(); future.channel().closeFuture().sync(); } catch (Exception e){ }finally { bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public void stop() { bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } 再写下数据存储相关的接口和类如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public interface DataStorage\u0026lt;T\u0026gt; { void save(T t) throws Exception; } public class LocalDataStorageImpl implements DataStorage\u0026lt;String\u0026gt;{ private static MappedByteBuffer mappedByteBuffer; private static Integer _1Gb = 1024*1024*1024; private static Integer _1MB = 1024*1024; public LocalDataStorageImpl() { try { FileChannel fileChannel = new RandomAccessFile(\u0026#34;./testWrite\u0026#34;, \u0026#34;rw\u0026#34;).getChannel(); mappedByteBuffer = fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, _1MB); } catch (Exception e) { e.printStackTrace(); } } @Override public void save(String data) throws Exception{ System.out.println(\u0026#34;start writeDataToFile data is :\u0026#34;+data); mappedByteBuffer.put(data.getBytes()); System.out.println(\u0026#34;writeDataToFile end!\u0026#34;); } } 再实现Netty相关的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @ChannelHandler.Sharable public class ServerHandler extends SimpleChannelInboundHandler\u0026lt;String\u0026gt; { private static final Logger LOG = LoggerFactory.getLogger(ServerHandler.class); private DataStorage dataStorage = new LocalDataStorageImpl(); @Override public void channelActive(ChannelHandlerContext channelHandlerContext) throws Exception { channelHandlerContext.write(\u0026#34;Welcome to sherlock home!\u0026#34;); channelHandlerContext.write(\u0026#34;It is \u0026#34;+ new Date()+\u0026#34;\\n\u0026#34;); channelHandlerContext.flush(); } @Override public void channelRead0(ChannelHandlerContext ctx, String request) throws Exception { LOG.info(\u0026#34;========readdata, request is {}=========\u0026#34;, request); //异步通过专门的EventLoop线程池进行处理 dataStorage.save(request); String response; boolean close = false; if (request.isEmpty()) { response = \u0026#34;Please type something.\\r\\n\u0026#34;; } else if (\u0026#34;bye\u0026#34;.equals(request.toLowerCase())) { response = \u0026#34;Have a good day!\\r\\n\u0026#34;; close = true; } else { response = \u0026#34;Did you say \u0026#39;\u0026#34; + request + \u0026#34;\u0026#39;?\\r\\n\u0026#34;; } ChannelFuture future = ctx.write(response); if (close) { future.addListener(ChannelFutureListener.CLOSE); } } @Override public void channelReadComplete(ChannelHandlerContext ctx) { ctx.flush(); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) { cause.printStackTrace(); ctx.close(); } } public class ServerInitializer extends ChannelInitializer\u0026lt;SocketChannel\u0026gt; { private static final Logger LOG = LoggerFactory.getLogger(ServerInitializer.class); private static final StringDecoder DECODER = new StringDecoder(); private static final StringEncoder ENCODER = new StringEncoder(); private static final ServerHandler SERVER_HANDLER = new ServerHandler(); @Override public void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast(new DelimiterBasedFrameDecoder(8192, Delimiters.lineDelimiter())); pipeline.addLast(DECODER); pipeline.addLast(ENCODER); pipeline.addLast(SERVER_HANDLER); } } 最后，咱们再来实现主函数逻辑\n1 2 3 4 5 6 public class Main { public static void main(String[] args) { NetService netService = new NetServiceImpl(); netService.start(); } } 基本上就差不多了，代码优化往后放放，现在嘛，能跑就行☺️\n运行调试 启动服务后，咱们通过下列指令往接口插入数据\n1 2 3 4 (echo \u0026#39;hello\u0026#39;; sleep 2) | telnet 127.0.0.1 8888 (echo \u0026#39;sherlock\u0026#39;; sleep 2) | telnet 127.0.0.1 8888 (echo \u0026#39;thanks\u0026#39;; sleep 2) | telnet 127.0.0.1 8888 (echo \u0026#39;are you ok?\u0026#39;; sleep 2) | telnet 127.0.0.1 8888 通过下面控制台的信息能够看到接收到完整的数据了，说明v0.1版本通过socket端口读取数据的链路是正常的\n再看看本地磁盘文件，通过打印出来能够看到数据是已经落到磁盘的\n小结 以上就是实现的整个过程，代码不可谓不粗糙，不过咱们讲究的就是一个莽，快速闭环看到效果才是最重要的，至于优化嘛，放到后面的版本慢慢优化~\n","date":"2024-04-07T20:46:11+08:00","image":"https://sherlock-lin.github.io/p/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9Fv0.1%E7%89%88%E6%9C%AC/image-20240131185601251-6698564_hu7506803332195288085.png","permalink":"https://sherlock-lin.github.io/p/%E6%89%8B%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9Fv0.1%E7%89%88%E6%9C%AC/","title":"手写分布式存储系统v0.1版本"},{"content":"引言 一直想写一篇服务化相关的文章，那就别犹豫了现在就开始吧\n正文 作为大数据基础架构工程师，业界也笑称“运维Boy”，日常工作就是在各个机器上部署以及维护服务，例如部署Hadoop、Kafka、Pulsar这些等等，用于给公司业务提供数据导入、存储、分析服务。这些事情在经历了十多年已经演变出以下几个阶段，今天就以唠嗑的形式进行展开说说\n单一指令阶段 这是最原始的阶段，在机器/操作系统上的所有事情都要由大数据SRE通过一条条执行进行操作，举个简单的例子。当公司需要你新搭建数仓时，也就相当于要搭建以下三层\n存储层：搭建Hadoop、Hive、HBase集群等，这是用于将所有公司数据包括用户数据的存储 导入层：搭建FlinkCDC、Flume、Kafka/Pulsar等数据导入服务，这是用于将数据导入到存储层 分析层：搭建Doris、ClickHouse、Presto、SpringBoot等服务，这是用于将存储层的数据按照预期的想法进行计算出最终可直接用于分析的结果，例如庞大的公司昨天赚了多少钱？分别是各个城市赚了多少？相比上个月多了多少等等 换算到真正要做的事情，那大致流程就是，先申请机器(物理机或者云服务)、初始化环境例如搭建SSH等等，然后下载各个组件的安装包上传到对应的机器，针对对应的组件进行配置修改以及各个组件启动的前置动作，最后再根据具体的启动指令来挨个启动机器等等。如果机器只有几台，你不会觉得什么，但是如果有三四十台的时候，你会觉得手软以及抱怨。大致抱怨以下几点\n工作量大：要靠人工登陆每台机器重复执行那么多的步骤 容易出错：这类动作重复多次容易出现人工操作失误导致影响 体验差：这类事情做多了对于SRE来说是煎熬，并不会有太多技术上的成长，最后沦为只会执行这几个指令的“工具人” 脚本化阶段 人类历史发展本质上就是对资源的利用，为了更合理的利用资源因此衍生出了各种革命，例如第一次工业革命通过蒸汽替代人力，第二次工业革命通过各种能源燃料更大幅代替人力，第三次也就是最近几十年的互联网革命，本质上也是省资源避免大量的重复劳动。\n上面这段话想表达的是， 互联网行业可以说90%以上的重复劳动都是没啥意义的，就相当于在一辆豪华的汽车🚗内是有人在里面蹬三轮，这不是很滑稽吗。因此如果你发现自己的工作中还存在大量 单一指令阶段的事情，那么务必要想办法进行脚本化。脚本本质上就是一本操作指南，给“操作系统”看的，举个例子如果你是一个果园园主，你雇了30个人进行水果采摘，你肯定不会去给每一个人讲解如何识别水果、水果具体的采摘的流程是怎么样子的，要用手托住果子在用剪刀轻轻减哪个部位之类的话。因为这样不仅耽误大量你的时间，并且每年水果成熟时你都要重复一遍，因此更高效的方式是花上一天时间写一本“水果采摘指南”，后续的每一个采摘的人直接看下指南即可知道该怎么做，这个指南就是脚本。\n那么工作中也是一样，可以将下载组件包、解压包、更改配置、服务启动/重启等操作直接封装成脚本，然后将可能会变的东西作为参数传进来，这样的话无论是针对多少台机器进行操作，你只需要在这些机器上执行一下脚本即可。在这个基础上还能做二次优化，就是在所有机器配置SSH后，你只需要在执行脚本时传入要做变更机器的标识例如IP，执行的机器就会自动将这些“逻辑”分发到各个机器上进行执行，这样的操作方式是不是更加舒服？或者说这是不是才是一个相对成熟的流程？\n那么此时大家觉得这个流程是否还存在问题？可以思考🤔一会再继续往下看。其实也很简单，就是对开发人员的专业能力是比较高的，换成是上面的例子来解释就是，果园园主不想花时间去学习写“水果采摘指南”，或者说所有果园园主单独写指南从上帝视角看本质上就是资源的浪费，要怎么解决这个问题呢。也不复杂，直接让上帝提供几份“水果采摘指南”，各个果园园主只需要选择适合自己的直接用岂不美哉？那么就引出了用户界面操作阶段。\n用户界面操作阶段 在读这篇文章的你相信对网站操作也不陌生了，例如咱们不需要知道网络底层是怎么操作，代码是怎么编写的，就能完成多人跨网络、跨时空的沟通，这些本质上要归功于用户界面操作阶段, 因为这些东西已经包装成了几个按钮。大数据SRE的工作内容其实也是可以作成几个按钮来大幅提升效率的，例如要在某些服务上搭建数仓，那么只需要在Web页面上勾选要部署服务的机器标识例如IP，然后选择想要安装的服务，然后点击确定即可完成安装，然后安装完后在Web页面就有这个服务的专门管理页面，例如服务启动、配置更改、使用情况监控等等，是不是一下子觉得高级了起来？更重要的是，你发现甚至你都不用掌握过多的SRE的知识也能完成这份工作？并且即便后续在更大的场景例如要在几百台、几千台机器进行部署维护你也不怕了？这就是互联网的魅力，所以说互联网革命也是人类历史上对资源利用的一大进步，如果咱们深处互联网时代，甚至从事互联网工作，而不具备互联网思维，那岂不是一种倒退吗或者形象点就是一个远古人生活在21世纪还在钻木取火，这就挺奇怪的。\n在这个阶段是否还有能改进的地方，接下来是我的设想或者是YY时间也就是 大模型+AIOPS阶段\n大模型+AIOPS阶段 最近几年随着大模型的爆发，有不少企业以及个人已经用它来提升和改进自己的工作效率。那么以后是否还有这样的一种模式，就是我们只要跟机器人，或者说是一位“虚拟同事”发送 给我搭建一套数仓指令是否就可以了？它会自动接续这条自然语言的语意，咱们进行各个流程的操作，在一些关键流程我们人类只需要进行审批确认没问题即可，剩下的事情交给机器去做就够了。\n在这个基础上，运维人员也不用天天盯着监控告警了，我们可以将历史发生过的事故数据以及专业知识喂人工智能，并训练它针对具体事情该做出怎样的决策，举个简单的例子比如某台机器的CPU过高，那么自动排查问题并进行修复，最后再将排查的结果以及修复的流程发给人类即可，这岂不美哉？而人类过多的做这些事情本质上还是一种资源的浪费，因为存在过多过多重复劳动、过多没有太多价值的事情，如果一个人的一辈子都在做这种事情，那本质上我们还是几万年前那个吭哧吭哧钻木头🪵的原始人，一切的一切都从来没有变过。\n总结 以上是我对大数据服务化粗糙的认知，输出出来是希望能引发一些思考🤔，当然写的过程中也引发了我自己的不少思考。这个过程中虽然存在一点批判，但绝不是针对具体的个人，而是针对目前常见的一些流程设计，单纯觉得有些设计可以变得更“美”一些。如果对服务化感兴趣的伙伴可以去针对性的学习专业的知识来改善工作内容，本篇文章仅仅是唠嗑，存在很多瑕疵，但我始终相信，多人沟通讨论可以构建设计一个更加完美的设计，因此如果能引发其他人的共鸣或者不同想法💡其实都是好事。\n","date":"2024-04-04T20:41:34+08:00","image":"https://sherlock-lin.github.io/p/%E8%AE%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1%E5%8C%96%E5%8F%91%E5%B1%95%E5%8F%B2/image-20240404185440981_hu17668148412322371707.png","permalink":"https://sherlock-lin.github.io/p/%E8%AE%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1%E5%8C%96%E5%8F%91%E5%B1%95%E5%8F%B2/","title":"论大数据服务化发展史"},{"content":"引言 距离 莽，就完事啦！已经过去了两个月，这两个月也交出了自己的答卷，这个过程中也有些自己的心得体会，这篇就作为call back吧\n高效执行力 这两个月累计输出22篇文章，算下来差不多是三天一篇，这在过去是完全不敢想象的，在舍弃“完美”后，很多自己的想法、对技术的见解都可以发出来，虽然也有写得好的，但是也有不少自己觉得不满意的，这在以前我肯定希望能够在未来打造到完美再发出来，但如果真的是那样可能我的产量会严重下滑甚至可能会停止输出博客，并且如果不发出来我自己的一些问题也无法暴露出来并无法进行改进。因此请读者带着包容的心态来进行阅读，同时我也相信搜索引擎会自动过滤掉我的那些写得糟糕的文章，同时会将我写的还行的文章送到您的面前，如果能恰好帮助您解决某些问题或者解开某个疑惑，那是我的荣幸。\n从我这两个月的输出也能从某种程度说明完美和执行力是互斥的，或者说有时候不要想着一步到位，而是想做出来，然后再不断进行完善。就像Pulsar系列的文章，有些写得比较草率，但是即便如此在输出到现如今我对Pulsar的整体流程有个更加深入的理解，在脑海里逐渐有了一个“智慧模型”，这是非常重要的，在有个这个模型或者说整体框架后，我再去针对某个点进行啃透就相对轻松很多，并且也有的放矢，自己自己当前的进度在那里\n质量 如何将事情做得又快又好应该是大部分人都在思考的问题，如今的我觉得两者就像是道家的阴阳鱼此消彼长，我们要尽量做好平衡，有时候要侧重于执行，但是当发现质量下滑得厉害时，我们又要适当放慢速度来提升质量。这会是一个不断持续的过程，而这个过程磨练的也是个人的心智或者能力。除此之外，在不断执行的过程中，也要不断进行提炼，例如一篇好的文章框架应该是如何的，提炼出一套通用的框架后就按照这套框架进行文章输出，并且在输出的过程中继续完善框架，比如很经典的5W2H模型，这个东西是什么？为什么会需要这个东西？这个东西怎么用？怎么实现？等等。基本把这些问题回答了，那么你要讲的东西也讲清楚了\n待完善 在不断输出技术的过程中，也发现自己目前还存在的一些问题，在此一起记录下来方便自我完善也供大家参考\n表达也是有框架的，我在讲解某些东西的时候逻辑不够清晰，反复讲来讲去其实就是想表达一个意思，但是由于没有很好的提炼导致有点嘴碎的感觉 绘图能力待提升，应该看更多优秀的作品，同时将某些技术的关键点通过图给关联起来，并且目前绘制的图挺不规范，例如类和功能混合在一起等等，这些都待完善 表达应该也要有层次感，就像金字塔一样，下一层的知识是为了更好的支撑上一层，彼此是有联系并且层层递进的，而目前这块做得不够好，更像是顺着某条路一条路走完但是读者会很懵 总结 这是我最近两个月关于“莽”比较大的感悟，虽然文章质量有下滑，但我觉得有时候该抓一放一，对于此时的我来说坚持输出远胜于用很多时间打磨一两片文章。因此我会在“莽”的过程中尽量提升质量，读者如果有其他的建议也希望在评论中一起进行讨论\n","date":"2024-04-04T15:09:14+08:00","image":"https://sherlock-lin.github.io/p/%E8%8E%BD%E5%B0%B1%E5%AE%8C%E4%BA%8B%E4%BA%86%E7%AC%AC%E4%BA%8C%E5%BC%B9/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu6307248181568134095.jpg","permalink":"https://sherlock-lin.github.io/p/%E8%8E%BD%E5%B0%B1%E5%AE%8C%E4%BA%8B%E4%BA%86%E7%AC%AC%E4%BA%8C%E5%BC%B9/","title":"莽就完事了第二弹"},{"content":"引言 《认知觉醒》这本书太经典了，反复读了多次还是爱不释手，因此决定针对它写写读书笔记。今天主要针对这本书的三重大脑理论进行讲解\n三重大脑 作者认为人类的大脑分为三重分别是本能脑、情绪脑以及理智脑，三者的区别如下图(从书中拷贝的，侵权请联系删除)\n很多相关书籍都会将大脑分为这三种，但本人觉得对于读者来说其实可以简化，将情绪脑和本能脑进行合并简称“非理智脑”。理智脑和非理智脑各有优缺点，理智脑相比之下更加高级，但是弱小，而非理智脑虽然低级但是非常非常强大。因此从人类远古时期到如今已跨入互联网第三次工业革命中，大部分人的决策还是通过非理智脑。这里分别例举两者的特点做下比较\n理智脑特点\n富有远见、善于权衡 能立足未来延迟满足 对大脑控制力弱 非理智脑优点\n执行力强 避难趋易和急于求成 对大脑控制力强 通过这个比较相信大家能够明显的感觉到两者的差异，这里比较两者并不是想说理智脑更好，而是希望我们更加了解自己大脑这个“工具”。大脑是我们日常生活中最常用的“工具”，在我们的灵魂向大脑发送指令时，大脑会进行思考并做出对应决策，最终将决策的结果发送到身体的各个部位进行最终的“执行动作”。打个比方，如果每个人都是一个王国，那么灵魂就是国王，而理智脑是军师，非理智脑是将军，身体的各个部位分别是这个王国的子民。因此国王要做的是将国家发展战略相关的事情交给军师，将这个战略所依赖的各个事情交给将军，让将军带领部队、子民们去攻城拔寨从而壮大这个王国。\n通过上面这个例子大家应该就清楚，理智脑和非理智脑从来不是对立的，而是协作，如何协调好两者是我们每个人需要做好的事情。用书中的原话来说就是 理智脑不是直接干活的，干活是本能脑和情绪脑的事情，因为它们的“力气”大；上天赋予理智脑智慧，是让它驱动本能和情绪，而不是直接取代它们。\n生活例子 接下来结合以下几个例子来讲讲\n营销手法\n现如今很多营销手法本质上就是攻击每个人的“非理智脑”，例如不少主播带货，通过不停的喊倒计时来制造稀缺以及紧张的氛围，此时“非理智脑”就会本能的紧张以及在损失厌恶的心理下，“非理智脑”会以压倒性的形式远远战胜“理智脑”，即便“理智脑”能察觉到买这玩意可能没啥作用，但是没办法它对大脑的控制远远不如“理智脑”。这是聪明的商家屡试不爽的招数，除此之外的场景还有很多很多，例如当房价不停的疯狂上涨时，大部分人也会丧失理智跟风上车；当股价飕飕上涨时，也同样如此，这些趋利避害以及急于求成都是“非理智脑”的缺点，至少应该意识到这一点以及尽量避免因此遭到损失。\n学习技能\n在学习新技术/技能时，我们往往坚持不了太久，这是因为从学习曲线来说大部分前期都是比较容易，但是一旦过了前期之后，再进行努力都很难看到成效，此时很多人都会建议一定要自律之类的话，但是盲目的自律其实就是在让弱小的“理智脑”去对抗强大的“非理智脑”，结果失败都是非常正常的事。因此应该用“理智脑”去引导非理智脑，就拿学小提琴为例子，我们要做的不是强迫自己每天拉几个小时的小提琴，这只会变得痛苦以及更早的放弃。“理智脑”要做的应该是，要求自己去听几场音乐会，看小提琴家们在舞台上拉出悠扬的声音，这个画面会在未来的很长时间引导着“非理智脑”去充满热情练琴，其次就是选一些自己喜欢并且容易入门的曲子，在每天练完基本功后“奖励”自己拉一会自己的曲子，在曲子拉熟练后可以适当的在朋友们面前进行表演等。这只是以小提琴为例子，其他技能都是想通的，因此应该合理的应用好我们的“非理智脑”\n总结 这类文章可能读起来会有点觉得像是傲慢的说教之类的，如果有类似的感觉我感到很抱歉，这并不是我的本意，我从过去到现在即使是将来在这方面都有要不断完善的地方，写出来一方面是给大家分享我的见解，另一方面自己也会时常回头看看、反思这块相关的内容并指导我自己更好的前行。最后跟大家分享文章中我很喜欢的一句话 习惯之所以难以改变，就是因为它是自我巩固的——越用越强，越强越用。要想从既有的习惯中跳出来，最好的方法不是依靠自制力，而是依靠知识。\n","date":"2024-04-02T15:09:59+08:00","image":"https://sherlock-lin.github.io/p/%E8%AE%A4%E7%9F%A5%E8%A7%89%E9%86%92%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%A4%A7%E8%84%91%E4%B9%8B%E4%B8%80%E5%88%87%E9%97%AE%E9%A2%98%E7%9A%84%E8%B5%B7%E6%BA%90/image-20240401215532850_hu17024335535230597268.png","permalink":"https://sherlock-lin.github.io/p/%E8%AE%A4%E7%9F%A5%E8%A7%89%E9%86%92%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%A4%A7%E8%84%91%E4%B9%8B%E4%B8%80%E5%88%87%E9%97%AE%E9%A2%98%E7%9A%84%E8%B5%B7%E6%BA%90/","title":"认知觉醒读书笔记之大脑之一切问题的起源"},{"content":"引言 在看了 你真的了解Pulsar的消息保留、积压、TTL策略吗 后，相信有不少对技术充满热情的小伙伴会疑惑，Pulsar的TTL又是怎么去失信的呢？今天就让我们一起来看看吧\n在看下面的文章时我们要带着以下几个问题\nTTL是什么时候触发的 TTL机制被触发后会发生什么 过期的消息会立马被删除吗 整体流程 在跟踪源码前先看看这张图，每个Broker内部都会有一个周期定时线程任务，每隔一段时间都会触发TTL任务。TTL任务会轮询当前Broker所管理的所有Topic中的所有订阅者，因为每个订阅者都会在Broker维护一个消费游标，因此Broker会根据用户配置的过期时间到轮询检查游标，看看有哪些消息是没被消费但是已经过了TTL的并会将游标移动到其的左侧(从实现的层面相当于表示它已经被消费了)，从而达到这些过了TTL的消息允许被删除的效果，最终再将游标的信息持久化到Bookkeeper中进行保存。\n触发TTL流程 那么直接来跟踪下触发TTL的流程吧，先从Broker的启动方法start开始看。可以看到启动的时候还会开启一系列定时任务，这里只看TTL相关的，跟踪进去startMessageExpiryMonitor方法可以看到messageExpiryMonitor其实是JDK提供的线程池ScheduledExecutorService类，在这里通过scheduleAtFixedRate方法周期性的执行过期任务检测，间隔从Broker配置中进行读取的，默认是每隔5分钟一次\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public void start() throws Exception { .... this.startInactivityMonitor(); //启动定时消息过期检测任务(TTL) this.startMessageExpiryMonitor(); this.startCompactionMonitor(); this.startConsumedLedgersMonitor(); this.startBacklogQuotaChecker(); this.updateBrokerPublisherThrottlingMaxRate(); this.updateBrokerDispatchThrottlingMaxRate(); this.startCheckReplicationPolicies(); this.startDeduplicationSnapshotMonitor(); } protected void startMessageExpiryMonitor() { int interval = pulsar().getConfiguration().getMessageExpiryCheckIntervalInMinutes(); messageExpiryMonitor.scheduleAtFixedRate(this::checkMessageExpiry, interval, interval, TimeUnit.MINUTES); } TTL启动流程 通过上一节可以看到Pulsar的TTL触发就是通过JDK的定时线程池来实现的，Broker会周期性的调用checkMessageExpiry方法进行处理，因此现在就从这个方法进行跟踪，可以看到最终会调用到Topic的checkMessageExpiry方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 public void checkMessageExpiry() { //进去看看forEachTopic的实现 forEachTopic(Topic::checkMessageExpiry); } public void forEachTopic(Consumer\u0026lt;Topic\u0026gt; consumer) { //topics是Broker维护的一个Map结构，用于记录当前Broker所维护的Topic信息 //这里使用了Java8的Consumer，相当于闭包的设计，让内部的所有Topic都执行checkMessageExpiry方法 topics.forEach((n, t) -\u0026gt; { Optional\u0026lt;Topic\u0026gt; topic = extractTopic(t); topic.ifPresent(consumer::accept); }); } 由于Topic只是接口，因此我们看它最常用的实现类也就是我们熟悉的PersistentTopic类的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 public void checkMessageExpiry() { //从topic配置中获取消息的TTL配置，这个跟上面的配置是有区别的，这里代表的是消息的有效期 int messageTtlInSeconds = topicPolicies.getMessageTTLInSeconds().get(); //通过这里的逻辑可以知道如果配置成0就不会过期 if (messageTtlInSeconds != 0) { //循环调用当前Topic订阅者检测过期 subscriptions.forEach((__, sub) -\u0026gt; { if (!isCompactionSubscription(sub.getName())) { //进行具体的检测动作，从这里继续跟踪 sub.expireMessages(messageTtlInSeconds); } }); } } public boolean expireMessages(int messageTTLInSeconds) { //获取当前积压消息的条数(指的就是未被消费的消息条数) long backlog = getNumberOfEntriesInBacklog(false); //如果没有消息积压就代表所有消息都被成功消费并且游标此时已经处于最左端，因此没必要再做TTL的检测了 if (backlog == 0 || (dispatcher != null \u0026amp;\u0026amp; dispatcher.isConsumerConnected() \u0026amp;\u0026amp; backlog \u0026lt; MINIMUM_BACKLOG_FOR_EXPIRY_CHECK \u0026amp;\u0026amp; !topic.isOldestMessageExpired(cursor, messageTTLInSeconds))) { // don\u0026#39;t do anything for almost caught-up connected subscriptions return false; } this.lastExpireTimestamp = System.currentTimeMillis(); //更新过期消息的下标，从这里继续跟踪 return expiryMonitor.expireMessages(messageTTLInSeconds); } public boolean expireMessages(int messageTTLInSeconds) { .... // 这里进去进行过期检查 checkExpiryByLedgerClosureTime(cursor, messageTTLInSeconds); .... } private void checkExpiryByLedgerClosureTime(ManagedCursor cursor, int messageTTLInSeconds) { //参数校验，可是这里为什么又做了小于0的判断，而外层只判断了等于0，可否用统一的参数校验工具进行校验呢？ if (messageTTLInSeconds \u0026lt;= 0) { return; } if (cursor instanceof ManagedCursorImpl managedCursor) { ManagedLedgerImpl managedLedger = (ManagedLedgerImpl) managedCursor.getManagedLedger(); //获得游标当前标记的可删除位置 Position deletedPosition = managedCursor.getMarkDeletedPosition(); //获取当前未被成功消费的积压日志信息，按Ledger进行排序 SortedMap\u0026lt;Long, MLDataFormats.ManagedLedgerInfo.LedgerInfo\u0026gt; ledgerInfoSortedMap = managedLedger.getLedgersInfo().subMap(deletedPosition.getLedgerId(), true, managedLedger.getLedgersInfo().lastKey(), true); MLDataFormats.ManagedLedgerInfo.LedgerInfo info = null; // 查询最接近现在的第一个未过期Ledger，那么其上一个Ledger一定是过期的并且其之前的都是过期的 for (MLDataFormats.ManagedLedgerInfo.LedgerInfo ledgerInfo : ledgerInfoSortedMap.values()) { if (!ledgerInfo.hasTimestamp() || !MessageImpl.isEntryExpired(messageTTLInSeconds, ledgerInfo.getTimestamp())) { break; } info = ledgerInfo; } //如果info不为空说明一定存在已经处于过期的Ledger也就是过期的消息集合体 if (info != null \u0026amp;\u0026amp; info.getLedgerId() \u0026gt; -1) { //获取具体过期的位置 PositionImpl position = PositionImpl.get(info.getLedgerId(), info.getEntries() - 1); if (((PositionImpl) managedLedger.getLastConfirmedEntry()).compareTo(position) \u0026lt; 0) { findEntryComplete(managedLedger.getLastConfirmedEntry(), null); } else { //这里进去检查过期位置 findEntryComplete(position, null); } } } } 通过上面的代码跟踪可以看到在获取到过期的消息位置后，最终调用了PersistentMessageExpiryMonitor类的findEntryComplete方法，那么咱们接下来跟着进去看看都发生了哪些有意思的事情吧\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 public void findEntryComplete(Position position, Object ctx) { ....\t//通过方法命名可以推测是通过标记游标的删除位置到达到TTL的效果，继续跟踪进去看看 cursor.asyncMarkDelete(position, cursor.getProperties(), markDeleteCallback, cursor.getNumberOfEntriesInBacklog(false)); .... } public void asyncMarkDelete(final Position position, Map\u0026lt;String, Long\u0026gt; properties, final MarkDeleteCallback callback, final Object ctx) { .... //异步标记删除位置 internalAsyncMarkDelete(newPosition, properties, callback, ctx); } protected void internalAsyncMarkDelete(final PositionImpl newPosition, Map\u0026lt;String, Long\u0026gt; properties, final MarkDeleteCallback callback, final Object ctx) { .... synchronized (pendingMarkDeleteOps) { switch (STATE_UPDATER.get(this)) { .... case Open: if (PENDING_READ_OPS_UPDATER.get(this) \u0026gt; 0) { pendingMarkDeleteOps.add(mdEntry); } else { //进行标记删除 internalMarkDelete(mdEntry); } break; .... } } } void internalMarkDelete(final MarkDeleteEntry mdEntry) { .... //持久化游标信息到Bookkeeper persistPositionToLedger(cursorLedger, mdEntry, cb); } void persistPositionToLedger(final LedgerHandle lh, MarkDeleteEntry mdEntry, final VoidCallback callback) { PositionImpl position = mdEntry.newPosition; PositionInfo pi = PositionInfo.newBuilder().setLedgerId(position.getLedgerId()) .setEntryId(position.getEntryId()) .addAllIndividualDeletedMessages(buildIndividualDeletedMessageRanges()) .addAllBatchedEntryDeletionIndexInfo(buildBatchEntryDeletionIndexInfoList()) .addAllProperties(buildPropertiesMap(mdEntry.properties)).build(); .... requireNonNull(lh); byte[] data = pi.toByteArray(); //调用Bookkeeper客户端将游标信息写入 lh.asyncAddEntry(data, (rc, lh1, entryId, ctx) -\u0026gt; { .... }, null); } 通过上面的跟踪可以看得到最终是通过Bookkeeper的客户端对象LedgerHandle的asyncAddEntry方法将游标信息持久化到了Bookkeeper，这里就不继续跟踪下去了，因为已经到了TTL左右范围的尽头了。\n总结 以上就是Pulsar中TTL的实现源码流程，我在这个过程中尽量省略了一些非必要的逻辑，主要是跟踪了主线，像一些地方也是值得跟踪的，感兴趣的伙伴可以自行跟踪。同时在这里我们可以看到，TTL过程只移动了游标位置并不涉及到数据的删除，说明在Pulsar的设计中将两者进行了分离，这是一种很好的职责设计思路，各自负责自己的一那部分。一方面在程序维护时可以避免改动引起一些相关性不那么大的影响，另一方面对于程序执行的整体性能来说也是比较高效的。\n","date":"2024-04-02T10:34:56+08:00","image":"https://sherlock-lin.github.io/p/pulsar%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8Bttl/image-20240331111412370_hu135517140155982200.png","permalink":"https://sherlock-lin.github.io/p/pulsar%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8Bttl/","title":"Pulsar源码解析之TTL"},{"content":"引言 任何东西都有生命周期，就像沙丁鱼罐头🥫也会过期一样，咱们的消息本身也是有生命周期的，因此像Pulsar这样的流平台/消息队列也提供了Retention、Backlog和TTL机制。\n默认清理机制 任何机制的出现都是有背景的，因此我们要先了解这三个机制出现之前的情况，才能分析出它们具体分别解决的事什么问题。首先看下图，生产者往Broker不断往Broker中写入消息，这些消息在Broker中会按照顺序从左到右进行存储的，新写入的消息是不断的在左边进行新增。消费者也是从右往左进行消费的，在Broker中会维持一个游标记录消费的情况，通过此游标Broker才可以对消息进去区分，哪些消息是可清理回收的，哪些消息是目前还不能清理回收的。这个默认清理机制符合我们使用消息队列的一部分业务场景\nRetention机制 在咱们理解了默认清理机制时，我们会有个疑问🤔️，如果消息一被成功消费就被删除，那么如果Broker的下游处理有问题需要从头消费或者从指定的某个位置消费的话，目前的清理机制是完成没办法支持的。而Pulsar设计的目标是一个大一统的流平台/消息队列，肯定是不允许这种情况的出现，因此引入了Retention机制。\n通过下图可以看到消息发布订阅过程，在默认的清理机制中游标右侧的消息是允许被删除的，但是配置Retention的情况则会按照机制进行保存一段时间。而Cursor游标左边的这些消息表示还没被消费者消费，因此即便没有配置 Retention机制这些消息也不会被删除。Retention 支持数据大小维度以及时间维度两种方式进行配置，不过它目前仅支持针对Namespace级别进行生效。\n在设置Retentio机制时可以通过defaultRetentionSizeInMB和defaultRetentionTimeInMinutes同时进行配置，此时会有以下几种情况\n时间维度 文件大小为度 消息保留效果 -1 -1 永久保存 -1 \u0026gt;0 基于数据大小进行保留 \u0026gt;0 -1 基于时间进行保留 0 0 默认配置，当消息被消费后不会被保留 0 \u0026gt;0 无效配置 \u0026gt;0 0 无效配置 \u0026gt;0 \u0026gt;0 当消息已经被消费或者没有消费者订阅时，满足其中一个条件的则不被保留 Backlog 限额机制 Retention机制解决了消息“提前”删除的问题，那么可以高枕无忧了吗？让我们来想想下面这种场景，就是未被成功消费的消息是会一直保留在Pulsar中的，那么如果写入速度一直远大于消费速度，是不是就相当于一个蓄水池入水速度远大于出水速度，最终这个池子会满一样，Pulsar的磁盘也会被打满从而影响服务的稳定性。\nBacklog的原理其实不复杂，相当于在蓄水池中标记一个水位线，当蓄水池的高度到达这条水位线时则触发报警，工作人员根据这个报警来做出相对应的处理。Pulsar其实也是一样，假如我们配置Backlog限额的大小是两条消息的大小，那么如下图，此时如果已经有两条消息未被消费，再有一条新的消息进行尝试写入，就会触发Pulsar的报警策略进行处理。\n积压警报策略有以下三种\nproducer_request_hold：生产者会暂时等待一段时间，并在之后重新进行消息发送 producer_exception：向生产者抛出异常，让生产者进行处理如停止或暂停往Pulsar进行消息的写入 consumer_backlog_eviction：Broker会清理一些积压的数据 Backlog 机制是针对Namespace级别限额，同样是支持通过数据大小以及时间两种维度配置。\nTime to live (TTL) 看起来Retention和Backlog机制已经基本满足我们的使用了，那么为啥还要加一个TTL呢？是社区闲着没事干整那么多东西吗，答案显然不是的。我们再来想想生活中的某些场景，如果在我们的业务场景中消息是有时效性的，例如股票最新的价格，如果这个价格信息是通过Pulsar进行传递的，那么如果这条消息及时没有被消费，在10分钟后它的价值理论上就没有那么高了，因为还会有源源不断的最新价格信息写入Pulsar，而用户更加关心的是最新的价格。因此根据业务场景给消息配置上TTL，可以更有利于Pulsar进行消息的回收以及资源的释放。通过下图我们可以看到，每一条消息都有一个“蜡烛”标记它的生命周期\n当这条消息上面的蜡烛燃尽时，即便这条消息还没有被消费，游标依然会移动到它左边将其标记为允许被删除，因为此时对于业务来说这条消息已经属于没有价值，没有在Pulsar继续保留的必要。可以看到这种方式相比Backlog的方式更加稳妥，因为这不依赖于消费者的消费情况\n综合 对于Pulsar存储而言，Backlog和TTL机制可以防止磁盘被耗尽；而Retention机制会占用磁盘来保留未来可能还会用到的消息。 对于影响范围而言，Retention机制仅针对已经成功消费的消息，Backlog和TTL仅针对未被成功消费的消息。 整体流程就是，在过了Retention配置的时间，已被成功消费的消息就会被删除；如果Pulsar的消息超过了Backlog限额则Pulsar会停止接收来自生产者的消息直到有更多可用的空间，因此针对消息数据配置TTL是可以非常好的保护Backlog限额的。 Pulsar物理存储的大小应该满足Retention和Backlog的总和，在设计集群时应该把消息需要保留多久、允许多少积压等考虑进去。 除此之外Pulsar支持分层存储，会将冷数据迁移到更加廉价的外部系统中存储，此时配置的策略会依然有效，因此应该考虑全面。 以上就是Pulsar Retention、Backlog和TTL机制的核心，可以满足流平台/消息队列的使用场景\n参考资料 官方文档\nUnderstanding Pulsar Message TTL, Backlog, and Retention\nApache Pulsar 之 TTL 与 Retention 策略\n","date":"2024-04-01T10:34:57+08:00","image":"https://sherlock-lin.github.io/p/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3pulsar%E7%9A%84%E6%B6%88%E6%81%AF%E4%BF%9D%E7%95%99%E7%A7%AF%E5%8E%8Bttl%E7%AD%96%E7%95%A5%E5%90%97/image-20240331085922332_hu13280254660255972346.png","permalink":"https://sherlock-lin.github.io/p/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3pulsar%E7%9A%84%E6%B6%88%E6%81%AF%E4%BF%9D%E7%95%99%E7%A7%AF%E5%8E%8Bttl%E7%AD%96%E7%95%A5%E5%90%97/","title":"你真的了解Pulsar的消息保留、积压、TTL策略吗"},{"content":"引言 今天一起来看看Pulsar服务端消息相关最重要的一个类PersistentTopic，看看它都负责哪些事情以及是如何设计的\n正文 在Topic被创建时，Pulsar集群中会有一台Broker维护这个Topic，在实现上就是维护一个PersistentTopic对象，这个PersistentTopic对象处理针对该Topic相关的一切操作，具体负责的相关操作如下\n处理订阅以及下线订阅 新增生产者对象以及下线生产者对象 处理消息写入 进行跨集群复制 记录Topic度量状态以及做Topic限流 检查消息的TTL、积压、压缩、配置策略更新 绘成表格的形式如下图\n本篇文章不会深入讲解每一项，主要是大概过下这个类都做了哪些事情以及大致逻辑，因为服务端的代码会经常跟这个类打交道，因此专门弄清楚这个类的相关知识还是很有必要的。接下来就让我们带着以下三个疑问去看下面的内容\nPersistentTopic在什么时候会被创建？都有哪些重要的成员变量？ PersistentTopic的创建流程都会发生什么？ 上面那些相关操作大概是什么实现的？ 何时创建 以下列出创建的时机，基本上流程都是发起创建Topic的时候会通过一致性哈希计算出这个Topic所归属的Bundle，然后去zookeeper获取这个Bundle所归属的Broker机器，最后请求这台Broker节点创建对应的PersistentTopic对象\n管理流创建 cli管理命令行 多语言Client Http方式 写入流创建 生产者写入流 消费者写入流 接下来看看这个类的主要成员变量，重要的一些已经加上注释解释了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 public class PersistentTopic extends AbstractTopic implements Topic, AddEntryCallback { // 管理Bookkeeper的Ledger，在做消息读取或者写入时会通过该对象 protected final ManagedLedger ledger; // 存储订阅当前Topic的所有订阅对象，key是订阅名，value是订阅对象 private final ConcurrentOpenHashMap\u0026lt;String, PersistentSubscription\u0026gt; subscriptions; // 管理对端集群，负责做跨集群数据复制 private final ConcurrentOpenHashMap\u0026lt;String/*RemoteCluster*/, Replicator\u0026gt; replicators; //跟replicators类似 private final ConcurrentOpenHashMap\u0026lt;String/*ShadowTopic*/, Replicator\u0026gt; shadowReplicators; @Getter private volatile List\u0026lt;String\u0026gt; shadowTopics; private final TopicName shadowSourceTopic; //调度限流器 private Optional\u0026lt;DispatchRateLimiter\u0026gt; dispatchRateLimiter = Optional.empty(); //调度限流锁 private final Object dispatchRateLimiterLock = new Object(); //订阅限流器 private Optional\u0026lt;SubscribeRateLimiter\u0026gt; subscribeRateLimiter = Optional.empty(); //积压游标阈值条数 private final long backloggedCursorThresholdEntries; public static final int MESSAGE_RATE_BACKOFF_MS = 1000; //处理消息重复情况 protected final MessageDeduplication messageDeduplication; //处理消息压缩服务 private TopicCompactionService topicCompactionService; // 在对外开放压缩策略配置时，根据用户配置创建对应的压缩策略 private static Map\u0026lt;String, TopicCompactionStrategy\u0026gt; strategicCompactionMap = Map.of( ServiceUnitStateChannelImpl.TOPIC, new ServiceUnitStateCompactionStrategy()); //未知 private CompletableFuture\u0026lt;MessageIdImpl\u0026gt; currentOffload = CompletableFuture.completedFuture( (MessageIdImpl) MessageId.earliest); //负责跨集群复制时订阅相关事项 private volatile Optional\u0026lt;ReplicatedSubscriptionsController\u0026gt; replicatedSubscriptionsController = Optional.empty(); //记录Topic度量相关信息，如这个Topic的写入速率、消费速率等 private static final FastThreadLocal\u0026lt;TopicStatsHelper\u0026gt; threadLocalTopicStats = new FastThreadLocal\u0026lt;TopicStatsHelper\u0026gt;() { @Override protected TopicStatsHelper initialValue() { return new TopicStatsHelper(); } }; } 创建流程 创建流程主要分为构建和初始化两个阶段，一般是先通过构造函数创建PersistentTopic，创建好后在调用其initialize方法进行初始化\n构造函数 构造函数主要做三件事：1. 更新Broker级别策略 2. 创建发布限流 3. 初始化容器。具体代码实现如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 public AbstractTopic(String topic, BrokerService brokerService) { //基本都是赋值操作 this.topic = topic; this.brokerService = brokerService; this.producers = new ConcurrentHashMap\u0026lt;\u0026gt;(); this.isFenced = false; ServiceConfiguration config = brokerService.pulsar().getConfiguration(); this.replicatorPrefix = config.getReplicatorPrefix(); topicPolicies = new HierarchyTopicPolicies(); updateTopicPolicyByBrokerConfig(); this.lastActive = System.nanoTime(); this.preciseTopicPublishRateLimitingEnable = config.isPreciseTopicPublishRateLimiterEnable(); //创建发布限流 topicPublishRateLimiter = new PublishRateLimiterImpl(brokerService.getPulsar().getMonotonicSnapshotClock()); //更新限流策略 updateActiveRateLimiters(); } public PersistentTopic(String topic, ManagedLedger ledger, BrokerService brokerService) { super(topic, brokerService); //赋值操作 this.orderedExecutor = brokerService.getTopicOrderedExecutor() != null ? brokerService.getTopicOrderedExecutor().chooseThread(topic) : null; this.ledger = ledger; //初始化容器，分别是维护当前Topic的订阅情况、跨集群副本情况 this.subscriptions = ConcurrentOpenHashMap.\u0026lt;String, PersistentSubscription\u0026gt;newBuilder() .expectedItems(16) .concurrencyLevel(1) .build(); this.replicators = ConcurrentOpenHashMap.\u0026lt;String, Replicator\u0026gt;newBuilder() .expectedItems(16) .concurrencyLevel(1) .build(); this.shadowReplicators = ConcurrentOpenHashMap.\u0026lt;String, Replicator\u0026gt;newBuilder() .expectedItems(16) .concurrencyLevel(1) .build(); this.backloggedCursorThresholdEntries = brokerService.pulsar().getConfiguration().getManagedLedgerCursorBackloggedThreshold(); .... } initialize方法 初始化方法的逻辑相比下要丰富写，归纳起来有以下四点\n获取压缩服务\n通过双重判断获取单例TwoPhaseCompactor进行压缩，有专门的线程池进行处理压缩动作\n遍历游标恢复订阅的状态\n当Topic重新分配后，在新的Broker中要根据游标恢复订阅、跨集群复制的状态，这样才能让客户端“无感”\n跨集群复制\n遍历这个Topic的游标，如果游标中存在跨集群复制游标则创建对应的GeoPersistentReplicator对象进行消息的复制\n更新配置\n更新Topic命名空间的配置策略 初始化调度限流DispatchRateLimiter 更新订阅/发布/资源组限流 更新Topic级别的配置 具体代码实现如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 public CompletableFuture\u0026lt;Void\u0026gt; initialize() { List\u0026lt;CompletableFuture\u0026lt;Void\u0026gt;\u0026gt; futures = new ArrayList\u0026lt;\u0026gt;(); //获取Broker压缩对象 futures.add(brokerService.getPulsar().newTopicCompactionService(topic) .thenAccept(service -\u0026gt; { PersistentTopic.this.topicCompactionService = service; //遍历这个Topic的游标恢复订阅中的对象 this.createPersistentSubscriptions(); })); //遍历这个Topic的游标，如果游标中存在跨集群复制游标则创建对应的GeoPersistentReplicator对象进行消息的复制 for (ManagedCursor cursor : ledger.getCursors()) { if (cursor.getName().startsWith(replicatorPrefix)) { String localCluster = brokerService.pulsar().getConfiguration().getClusterName(); String remoteCluster = PersistentReplicator.getRemoteCluster(cursor.getName()); futures.add(addReplicationCluster(remoteCluster, cursor, localCluster)); } } return FutureUtil.waitForAll(futures).thenCompose(__ -\u0026gt; brokerService.pulsar().getPulsarResources().getNamespaceResources() .getPoliciesAsync(TopicName.get(topic).getNamespaceObject()) .thenAcceptAsync(optPolicies -\u0026gt; { if (!optPolicies.isPresent()) { isEncryptionRequired = false; updatePublishDispatcher(); //进行资源组级别的隔离 updateResourceGroupLimiter(new Policies()); initializeDispatchRateLimiterIfNeeded(); updateSubscribeRateLimiter(); return; } Policies policies = optPolicies.get(); //更新命名空间级别的策略 this.updateTopicPolicyByNamespacePolicy(policies); //初始化调度限流 initializeDispatchRateLimiterIfNeeded(); //更新订阅限流 updateSubscribeRateLimiter(); //更新发布限流 updatePublishDispatcher(); //更新资源组限流 updateResourceGroupLimiter(policies); this.isEncryptionRequired = policies.encryption_required; isAllowAutoUpdateSchema = policies.is_allow_auto_update_schema; }, getOrderedExecutor()) .thenCompose(ignore -\u0026gt; initTopicPolicy()) .exceptionally(ex -\u0026gt; { .... })); } 负责内容 生产者相关 新增 生产者客户端启动时会跟Topic归属的Broker建立TCP连接并请求Broker端ServerCnx的handleProducer方法进行处理，这个方法最终会调用PersistentTopic的addProducer方法进行创建，咱们来看看实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public CompletableFuture\u0026lt;Optional\u0026lt;Long\u0026gt;\u0026gt; addProducer(Producer producer, CompletableFuture\u0026lt;Void\u0026gt; producerQueuedFuture) { //进去看看父类的实现 return super.addProducer(producer, producerQueuedFuture).thenCompose(topicEpoch -\u0026gt; { messageDeduplication.producerAdded(producer.getProducerName()); // Start replication producers if not already return startReplProducers().thenApply(__ -\u0026gt; topicEpoch); }); } public CompletableFuture\u0026lt;Optional\u0026lt;Long\u0026gt;\u0026gt; addProducer(Producer producer, CompletableFuture\u0026lt;Void\u0026gt; producerQueuedFuture) { .... //继续跟踪 return internalAddProducer(producer); .... } protected CompletableFuture\u0026lt;Void\u0026gt; internalAddProducer(Producer producer) { .... //producers是ConcurrentHashMap结构，相当于会在PersistentTopic中维护服务端的生产者对象 Producer existProducer = producers.putIfAbsent(producer.getProducerName(), producer); .... return CompletableFuture.completedFuture(null); } 删除 删除的逻辑很简单，也是通过TCP接收到客户端断开连接的请求后，会调用Producer的close方法进行资源释放，同时从维护的producers中移除此对象\n订阅相关 消费者客户端启动时会跟Topic归属的Broker建立TCP连接并请求Broker端，请求由ServerCnx的handleSubscribe方法进行处理，这个方法最终会调用PersistentTopic的internalSubscribe方法进行创建，咱们来看看实现\n新增 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 private CompletableFuture\u0026lt;Consumer\u0026gt; internalSubscribe(final TransportCnx cnx, String subscriptionName, long consumerId, SubType subType, int priorityLevel, String consumerName, boolean isDurable, MessageId startMessageId, Map\u0026lt;String, String\u0026gt; metadata, boolean readCompacted, InitialPosition initialPosition, long startMessageRollbackDurationSec, boolean replicatedSubscriptionStateArg, KeySharedMeta keySharedMeta, Map\u0026lt;String, String\u0026gt; subscriptionProperties, long consumerEpoch, SchemaType schemaType) { .... //创建对应的客户端对象 Consumer consumer = new Consumer(subscription, subType, topic, consumerId, priorityLevel, consumerName, isDurable, cnx, cnx.getAuthRole(), metadata, readCompacted, keySharedMeta, startMessageId, consumerEpoch, schemaType); .... //将消费者对象加到订阅里 return addConsumerToSubscription(subscription, consumer); } protected CompletableFuture\u0026lt;Void\u0026gt; addConsumerToSubscription(Subscription subscription, Consumer consumer) { .... //最终将将创建的消费者放到Dispatcher中进行管理以及加到subscriptions这个Map中进行维护 return subscription.addConsumer(consumer); } 删除 删除的逻辑也是比较简单的，就是从subscriptions中进行移除\n处理消息写入 消息写入的流程相对比较复杂，后面再单独分析，暂时跳过\n跨集群复制 跨集群复制这里实现逻辑其实不难，相当于在Broker上去读取Bookkeeper获取数据并通过启动生产者往其他集群进行数据写入，具体细节后面也单独写文章进行分析\n消息清理 Pulsar的消息TTL、积压、压缩都是在Broker启动时往线程池中加入定时任务轮询触发的\nTTL TTL会循环扫描每个Topic过期的消息位置，并通过改变游标进行标记，而清理是由专门的线程根据游标进行处理的\n积压 消息积压也是会定期检测，分两种情况，一种是消息超过限制额度，另一种是消息时间超过额度。这两种情况都会触发积压策略的处理，具体的策略在BacklogQuota.RetentionPolicy中定义\n压缩 跟上面一样，轮询检测进行处理\n配置更新 在PersistentTopic启动的初始化initialize方法中会调用onUpdate进行策略更新，那Pulsar又是如何做的支持配置动态更新的呢？这个问题保留给大家\n总结 本文主要强调PersistentTopic类的重要性、聊聊它都负责哪些事情以及大概实现，具体每个细节展开都能单独写一篇文章，这个敬请期待～\n","date":"2024-03-27T10:34:55+08:00","image":"https://sherlock-lin.github.io/p/pulsar%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8Bpersistenttopic%E7%B1%BB/image-20240327210450901-6219390_hu18382412844183478919.png","permalink":"https://sherlock-lin.github.io/p/pulsar%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8Bpersistenttopic%E7%B1%BB/","title":"Pulsar源码解析之PersistentTopic类"},{"content":"失效的自律 在人生的每个阶段，我好像都有遇到那种天然就知道自己该做什么事情并且极度“自律”的人，上学那会就有同学雷打不动的学习，哪怕没人监督也照样认真学习。与之相比的我感觉就像是这个世界的NPC一样，做事三分热度，感兴趣的时候也能热血沸腾狠狠学一段时间，但是大部分时候还是想偷懒，一直以来觉得是自己自控力问题导致自己不够自律，也会是不是给自己定制计划并设置奖惩，但最后都不了了之。你是否也有同样的经历并且苦恼呢\n高级的欲望 相比想方设法去让自己自律，我们更应该做的是挖掘自己内心更深层次的高级欲望，打游戏确实可以带来短时间的多巴胺，但是提升学习技能所带来的内啡肽比多巴胺更加吸引人。高级欲望一般难度相比更大，它填满时的喜悦以及成就感远远不是低级欲望所能比拟的。而作为一个需要及时反馈的普通人来说，可以讲高级欲望的目标进行划分，每个时间段攻克其中一小块，这不仅也能带来喜悦同时也能有中升级打怪的成就感。同时也要尽可能找到可以观测到的输出方式，例如输出文章，或者说学乐器的话就是熟练练会一首曲子，学英语的话就是掌握100个单词等等。除了这些外，最好是能发现或者点燃内心对某个东西的热爱，当你在做一件真正热爱的事情时，别说要你坚持，即便很多人都不让你去做，你也会发了疯的去拼命做。最后一点最重要的就是，行动起来，说再多都比不上动作做一次～\n","date":"2024-03-21T15:09:14+08:00","image":"https://sherlock-lin.github.io/p/%E5%85%8B%E6%9C%8D%E6%AC%B2%E6%9C%9B%E7%9A%84%E6%B0%B8%E8%BF%9C%E4%B8%8D%E6%98%AF%E8%87%AA%E5%BE%8B%E8%80%8C%E6%98%AF%E6%9B%B4%E9%AB%98%E7%BA%A7%E7%9A%84%E6%AC%B2%E6%9C%9B/image-20240321191336836-6816731_hu1393761622652813113.png","permalink":"https://sherlock-lin.github.io/p/%E5%85%8B%E6%9C%8D%E6%AC%B2%E6%9C%9B%E7%9A%84%E6%B0%B8%E8%BF%9C%E4%B8%8D%E6%98%AF%E8%87%AA%E5%BE%8B%E8%80%8C%E6%98%AF%E6%9B%B4%E9%AB%98%E7%BA%A7%E7%9A%84%E6%AC%B2%E6%9C%9B/","title":"克服欲望的永远不是自律，而是更高级的欲望"},{"content":"引言 无论你是刚接触Pulsar还是使用Pulsar多年，相信你对下面这段代码都很熟悉，这就是生产者端最常写的代码没有之一，其中最核心的其实就三行代码，分别用红色数字标识出来了，其中对应的就是1、客户端对象创建 2、生产者对象创建 3、消息发送。今天就分别针对这三个步骤进行深入的探索。\n创建客户端对象 无论是写生产者还是消费者端代码，第一步都是要创建客户端对象，那么客户端对象都做了些什么事情呢？这里将客户端对象创建的步骤绘制成以下图\n客户端对象的创建以下几个东西，其中最重要的是前两个\nLookup服务 连接池 线程池 内存控制器MemoryLimitController 创建客户端计数器HashedWheelTimer Lookup服务 Lookup服务负责获取Topic的归属Broker地址以及Schema信息等，非常重要。默认有HTTP和二进制传输两种实现，如果创建客户端对象时.serviceUrl方法传入的地址是http开头则使用HTTP实现，否则就是二进制传输实现。\nLookup服务的创建如下\n1 2 3 4 5 6 7 //初始化LookupService服务 if (conf.getServiceUrl().startsWith(\u0026#34;http\u0026#34;)) { lookup = new HttpLookupService(conf, this.eventLoopGroup); } else { lookup = new BinaryProtoLookupService(this, conf.getServiceUrl(), conf.getListenerName(), conf.isUseTls(), this.scheduledExecutorProvider.getExecutor()); } 进入看看HttpLookupService的构造方法可以看到它是内部套了一个HttpClient对象来对外进行HTTP通信，在继续看HttpClient的构造函数可以它内部实际上是调用的DefaultAsyncHttpClient构造函数来创建，这个对象是外部包 async-http-client-2.12.1.jar的实现，跟了一下代码，底层也是基于Netty来进行实现的HTTP长连接通信\n1 2 3 4 5 6 7 8 9 10 11 public HttpLookupService(ClientConfigurationData conf, EventLoopGroup eventLoopGroup) throws PulsarClientException { this.httpClient = new HttpClient(conf, eventLoopGroup); this.useTls = conf.isUseTls(); this.listenerName = conf.getListenerName(); } protected HttpClient(ClientConfigurationData conf, EventLoopGroup eventLoopGroup) throws PulsarClientException { .... httpClient = new DefaultAsyncHttpClient(config); } 跟到这里就差不多了，我们知道创建客户端对象的时候会初始化Lookup服务，而Lookup服务初始化的时候会创建跟外部进行通信的异步HTTP客户端，用于在创建生产者时去查询Topic的归属Broker的IP地址，这样生产者才知道具体去跟哪台Broker创建TCP连接\n连接池ConnectionPool 连接池是池化技术在网络连接这个场景的使用，使用连接池可以避免重复创建关闭TCP连接造成资源浪费以及提升性能。在创建客户端对象的构造方法中，ConnectionPool的创建如下，可以是通过new方式进行创建，因此我们看下它的构造方法\n1 2 connectionPoolReference = connectionPool != null ? connectionPool : new ConnectionPool(conf, this.eventLoopGroup); ConnectionPool的构造方法如下，可以看到核心逻辑其实就是通过Bootstrap创建Netty客户端对象，通过 pool = new ConcurrentHashMap\u0026lt;\u0026gt;(); 这行代码也很重要，这个HashMap就是存储咱们的网络连接\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public ConnectionPool(ClientConfigurationData conf, EventLoopGroup eventLoopGroup, Supplier\u0026lt;ClientCnx\u0026gt; clientCnxSupplier, Optional\u0026lt;AddressResolver\u0026lt;InetSocketAddress\u0026gt;\u0026gt; addressResolver) throws PulsarClientException { .... //启动Netty客户端 pool = new ConcurrentHashMap\u0026lt;\u0026gt;(); bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup); bootstrap.channel(EventLoopUtil.getClientSocketChannelClass(eventLoopGroup)); //Netty相关配置 bootstrap.option(ChannelOption.CONNECT_TIMEOUT_MILLIS, conf.getConnectionTimeoutMs()); bootstrap.option(ChannelOption.TCP_NODELAY, conf.isUseTcpNoDelay()); bootstrap.option(ChannelOption.ALLOCATOR, PulsarByteBufAllocator.DEFAULT); try { channelInitializerHandler = new PulsarChannelInitializer(conf, clientCnxSupplier); bootstrap.handler(channelInitializerHandler); } catch (Exception e) { log.error(\u0026#34;Failed to create channel initializer\u0026#34;); throw new PulsarClientException(e); } .... } 生产者对象创建 看完了客户端对象创建，再来看看生产者对象的创建，从这条语句进行切入 Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(...).create(); 在创建生产者对象时会进行一下几步\n指定路由策略 这个主要就是根据创建生产者对象时指定的，如果没有配置的话默认使用的轮询路由策略。setMessageRoutingMode 这个方法就是指定路由策略的，然后指定完后下面的代码可以看到，如果有配置拦截器的话在创建生产者对象时也会把它给拦截逻辑加载进去。\n1 2 3 4 5 6 7 8 9 10 11 12 public CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; createAsync() { .... try { setMessageRoutingMode(); } catch (PulsarClientException pce) { return FutureUtil.failedFuture(pce); } return interceptorList == null || interceptorList.size() == 0 ? client.createProducerAsync(conf, schema, null) : client.createProducerAsync(conf, schema, new ProducerInterceptors(interceptorList)); } setMessageRoutingMode 逻辑如下，默认用轮询路由，如果配置其他的就用其他的，其次就是做路由规则的校验，看看用户是否配置的信息有冲突的提前感知并抛出去。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 private void setMessageRoutingMode() throws PulsarClientException { if (conf.getMessageRoutingMode() == null \u0026amp;\u0026amp; conf.getCustomMessageRouter() == null) { messageRoutingMode(MessageRoutingMode.RoundRobinPartition); } else if (conf.getMessageRoutingMode() == null \u0026amp;\u0026amp; conf.getCustomMessageRouter() != null) { messageRoutingMode(MessageRoutingMode.CustomPartition); } else if (conf.getMessageRoutingMode() == MessageRoutingMode.CustomPartition \u0026amp;\u0026amp; conf.getCustomMessageRouter() == null) { throw new PulsarClientException(\u0026#34;When \u0026#39;messageRoutingMode\u0026#39; is \u0026#34; + MessageRoutingMode.CustomPartition + \u0026#34;, \u0026#39;messageRouter\u0026#39; should be set\u0026#34;); } else if (conf.getMessageRoutingMode() != MessageRoutingMode.CustomPartition \u0026amp;\u0026amp; conf.getCustomMessageRouter() != null) { throw new PulsarClientException(\u0026#34;When \u0026#39;messageRouter\u0026#39; is set, \u0026#39;messageRoutingMode\u0026#39; \u0026#34; + \u0026#34;should be set as \u0026#34; + MessageRoutingMode.CustomPartition); } } 获取Topic分区数和Schema 通过Lookup机制去Broker中查询这个Topic的Schema信息，可以看到如果服务端没有配置Schema信息的话则默认用 Schema.BYTES\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 return lookup.getSchema(TopicName.get(conf.getTopicName())) .thenCompose(schemaInfoOptional -\u0026gt; { if (schemaInfoOptional.isPresent()) { SchemaInfo schemaInfo = schemaInfoOptional.get(); if (schemaInfo.getType() == SchemaType.PROTOBUF) { autoProduceBytesSchema.setSchema(new GenericAvroSchema(schemaInfo)); } else { autoProduceBytesSchema.setSchema(Schema.getSchema(schemaInfo)); } } else { autoProduceBytesSchema.setSchema(Schema.BYTES); } return createProducerAsync(topic, conf, schema, interceptors); }); 咱们进去看看 getSchema 的实现，可以看到构造的是HTTP地址并通过GET方式请求Broker进行获取\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public CompletableFuture\u0026lt;Optional\u0026lt;SchemaInfo\u0026gt;\u0026gt; getSchema(TopicName topicName, byte[] version) { CompletableFuture\u0026lt;Optional\u0026lt;SchemaInfo\u0026gt;\u0026gt; future = new CompletableFuture\u0026lt;\u0026gt;(); String schemaName = topicName.getSchemaName(); String path = String.format(\u0026#34;admin/v2/schemas/%s/schema\u0026#34;, schemaName); if (version != null) { if (version.length == 0) { future.completeExceptionally(new SchemaSerializationException(\u0026#34;Empty schema version\u0026#34;)); return future; } path = String.format(\u0026#34;admin/v2/schemas/%s/schema/%s\u0026#34;, schemaName, ByteBuffer.wrap(version).getLong()); } httpClient.get(path, GetSchemaResponse.class).thenAccept(response -\u0026gt; { if (response.getType() == SchemaType.KEY_VALUE) { SchemaData data = SchemaData .builder() .data(SchemaUtils.convertKeyValueDataStringToSchemaInfoSchema( response.getData().getBytes(StandardCharsets.UTF_8))) .type(response.getType()) .props(response.getProperties()) .build(); future.complete(Optional.of(SchemaInfoUtil.newSchemaInfo(schemaName, data))); } else { future.complete(Optional.of(SchemaInfoUtil.newSchemaInfo(schemaName, response))); } }).exceptionally(ex -\u0026gt; { .... }); return future; } 除了读取Schema，还会读取Topic的分区信息，从下面创建的代码可以看到，如果存在分区则创建PartitionedProducerImpl对象，不存在分区则创建ProducerImpl对象。获取分区的方法是 getPartitionedTopicMetadata，咱们进去看看它的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 private \u0026lt;T\u0026gt; CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; createProducerAsync(String topic, ProducerConfigurationData conf, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture = new CompletableFuture\u0026lt;\u0026gt;(); getPartitionedTopicMetadata(topic).thenAccept(metadata -\u0026gt; { .... ProducerBase\u0026lt;T\u0026gt; producer; if (metadata.partitions \u0026gt; 0) { producer = newPartitionedProducerImpl(topic, conf, schema, interceptors, producerCreatedFuture, metadata); } else { producer = newProducerImpl(topic, -1, conf, schema, interceptors, producerCreatedFuture, Optional.empty()); } producers.add(producer); }).exceptionally(ex -\u0026gt; { .... }); return producerCreatedFuture; } getPartitionedTopicMetadata方法的核心逻辑如下，通过一路跟踪下去发现也是通过Lookup服务进行HTTP进行查询读取分区信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 public CompletableFuture\u0026lt;PartitionedTopicMetadata\u0026gt; getPartitionedTopicMetadata(String topic) { CompletableFuture\u0026lt;PartitionedTopicMetadata\u0026gt; metadataFuture = new CompletableFuture\u0026lt;\u0026gt;(); try { TopicName topicName = TopicName.get(topic); AtomicLong opTimeoutMs = new AtomicLong(conf.getLookupTimeoutMs()); .... getPartitionedTopicMetadata(topicName, backoff, opTimeoutMs, metadataFuture, new ArrayList\u0026lt;\u0026gt;()); } catch (IllegalArgumentException e) { .... } return metadataFuture; } private void getPartitionedTopicMetadata(TopicName topicName, Backoff backoff, AtomicLong remainingTime, CompletableFuture\u0026lt;PartitionedTopicMetadata\u0026gt; future, List\u0026lt;Throwable\u0026gt; previousExceptions) { long startTime = System.nanoTime(); lookup.getPartitionedTopicMetadata(topicName).thenAccept(future::complete).exceptionally(e -\u0026gt; { remainingTime.addAndGet(-1 * TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTime)); long nextDelay = Math.min(backoff.next(), remainingTime.get()); // skip retry scheduler when set lookup throttle in client or server side which will lead to // `TooManyRequestsException` boolean isLookupThrottling = !PulsarClientException.isRetriableError(e.getCause()) || e.getCause() instanceof PulsarClientException.AuthenticationException; if (nextDelay \u0026lt;= 0 || isLookupThrottling) { PulsarClientException.setPreviousExceptions(e, previousExceptions); future.completeExceptionally(e); return null; } .... }); } public CompletableFuture\u0026lt;PartitionedTopicMetadata\u0026gt; getPartitionedTopicMetadata(TopicName topicName) { String format = topicName.isV2() ? \u0026#34;admin/v2/%s/partitions\u0026#34; : \u0026#34;admin/%s/partitions\u0026#34;; return httpClient.get(String.format(format, topicName.getLookupName()) + \u0026#34;?checkAllowAutoCreation=true\u0026#34;, PartitionedTopicMetadata.class); } 创建生产者对象 在上一小结可以看到已经查到分区信息，在有分区的情况是会调用 newPartitionedProducerImpl 方法进行生产者对象的初始化，现在就从这里开始进行跟踪，可以看到这个方法只是封装了new PartitionedProducerImpl对象的操作，于是继续看PartitionedProducerImpl的构造函数的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 protected \u0026lt;T\u0026gt; PartitionedProducerImpl\u0026lt;T\u0026gt; newPartitionedProducerImpl(String topic, ProducerConfigurationData conf, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors, CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture, PartitionedTopicMetadata metadata) { return new PartitionedProducerImpl\u0026lt;\u0026gt;(PulsarClientImpl.this, topic, conf, metadata.partitions, producerCreatedFuture, schema, interceptors); } public PartitionedProducerImpl(PulsarClientImpl client, String topic, ProducerConfigurationData conf, int numPartitions, CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { super(client, topic, conf, producerCreatedFuture, schema, interceptors); this.producers = ConcurrentOpenHashMap.\u0026lt;Integer, ProducerImpl\u0026lt;T\u0026gt;\u0026gt;newBuilder().build(); this.topicMetadata = new TopicMetadataImpl(numPartitions); //配置路由策略 this.routerPolicy = getMessageRouter(); stats = client.getConfiguration().getStatsIntervalSeconds() \u0026gt; 0 ? new PartitionedTopicProducerStatsRecorderImpl() : null; //配置最大同时等待的消息数 int maxPendingMessages = Math.min(conf.getMaxPendingMessages(), conf.getMaxPendingMessagesAcrossPartitions() / numPartitions); conf.setMaxPendingMessages(maxPendingMessages); final List\u0026lt;Integer\u0026gt; indexList; if (conf.isLazyStartPartitionedProducers() \u0026amp;\u0026amp; conf.getAccessMode() == ProducerAccessMode.Shared) { // try to create producer at least one partition indexList = Collections.singletonList(routerPolicy .choosePartition(((TypedMessageBuilderImpl\u0026lt;T\u0026gt;) newMessage()).getMessage(), topicMetadata)); } else { // try to create producer for all partitions indexList = IntStream.range(0, topicMetadata.numPartitions()).boxed().collect(Collectors.toList()); } firstPartitionIndex = indexList.get(0); //这里是核心逻辑，从这里进去 start(indexList); // start track and auto subscribe partition increasement if (conf.isAutoUpdatePartitions()) { topicsPartitionChangedListener = new TopicsPartitionChangedListener(); partitionsAutoUpdateTimeout = client.timer() .newTimeout(partitionsAutoUpdateTimerTask, conf.getAutoUpdatePartitionsIntervalSeconds(), TimeUnit.SECONDS); } } private void start(List\u0026lt;Integer\u0026gt; indexList) { .... final ProducerImpl\u0026lt;T\u0026gt; firstProducer = createProducer(indexList.get(0)); firstProducer.producerCreatedFuture().handle((prod, createException) -\u0026gt; { .... }).thenApply(name -\u0026gt; { for (int i = 1; i \u0026lt; indexList.size(); i++) { //循环分区数创建与之对应的ProducerImpl对象 createProducer(indexList.get(i), name).producerCreatedFuture().handle((prod, createException) -\u0026gt; { afterCreatingProducer.accept(false, createException); return null; }); } return null; }); } 继续从 createProducer 方法进行跟踪\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 private ProducerImpl\u0026lt;T\u0026gt; createProducer(final int partitionIndex, final Optional\u0026lt;String\u0026gt; overrideProducerName) { //创建ProducerImpl后会统一放到producers这个Map中，key是分区号，value就是ProducerImpl对象 return producers.computeIfAbsent(partitionIndex, (idx) -\u0026gt; { String partitionName = TopicName.get(topic).getPartition(idx).toString(); //继续进入看看ProducerImpl创建的逻辑 return client.newProducerImpl(partitionName, idx, conf, schema, interceptors, new CompletableFuture\u0026lt;\u0026gt;(), overrideProducerName); }); } public ProducerImpl(PulsarClientImpl client, String topic, ProducerConfigurationData conf, CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture, int partitionIndex, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors, Optional\u0026lt;String\u0026gt; overrideProducerName) { .... this.compressor = CompressionCodecProvider.getCompressionCodec(conf.getCompressionType()); .... //这里是核心逻辑，用于创建ProducerImpl对象跟对应的Broker的TCP网络连接 grabCnx(); } 创建连接 从上一小节可以看到代码逻辑跟到了grabCnx 方法，继续一路跟踪下去，可以看到最后都会调用 state.client.getConnection, 继续往里面看看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 void grabCnx() { this.connectionHandler.grabCnx(); } protected void grabCnx() { grabCnx(Optional.empty()); } protected void grabCnx(Optional\u0026lt;URI\u0026gt; hostURI) { .... cnxFuture = state.client.getConnection(address, address, randomKeyForSelectConnection); .... } public CompletableFuture\u0026lt;ClientCnx\u0026gt; getConnection(final InetSocketAddress logicalAddress, final InetSocketAddress physicalAddress, final int randomKeyForSelectConnection) { //可以看到调用连接池进行网络连接的获取，继续进去看看实现细节 return cnxPool.getConnection(logicalAddress, physicalAddress, randomKeyForSelectConnection); } public CompletableFuture\u0026lt;ClientCnx\u0026gt; getConnection(InetSocketAddress logicalAddress, InetSocketAddress physicalAddress, final int randomKey) { if (maxConnectionsPerHosts == 0) { // 如果配置没开启连接池，则每一次都重新新建一个TCP连接 return createConnection(logicalAddress, physicalAddress, -1); } final ConcurrentMap\u0026lt;Integer, CompletableFuture\u0026lt;ClientCnx\u0026gt;\u0026gt; innerPool = pool.computeIfAbsent(logicalAddress, a -\u0026gt; new ConcurrentHashMap\u0026lt;\u0026gt;()); // 新建TCP网络连接并放在连接池中以备之后的复用,继续进去看createConnection的逻辑 CompletableFuture\u0026lt;ClientCnx\u0026gt; completableFuture = innerPool .computeIfAbsent(randomKey, k -\u0026gt; createConnection(logicalAddress, physicalAddress, randomKey)); .... } private CompletableFuture\u0026lt;ClientCnx\u0026gt; createConnection(InetSocketAddress logicalAddress, InetSocketAddress physicalAddress, int connectionKey) { .... // 继续进去看createConnection的逻辑 createConnection(logicalAddress, physicalAddress).thenAccept(channel -\u0026gt; { .... }).exceptionally(exception -\u0026gt; { .... }); return cnxFuture; } private CompletableFuture\u0026lt;Channel\u0026gt; createConnection(InetSocketAddress logicalAddress, InetSocketAddress unresolvedPhysicalAddress) { CompletableFuture\u0026lt;List\u0026lt;InetSocketAddress\u0026gt;\u0026gt; resolvedAddress; try { .... return resolvedAddress.thenCompose( inetAddresses -\u0026gt; connectToResolvedAddresses( logicalAddress, unresolvedPhysicalAddress, inetAddresses.iterator(), isSniProxy ? unresolvedPhysicalAddress : null) ); } catch (URISyntaxException e) { .... } } private CompletableFuture\u0026lt;Channel\u0026gt; connectToResolvedAddresses(...) { CompletableFuture\u0026lt;Channel\u0026gt; future = new CompletableFuture\u0026lt;\u0026gt;(); // 继续往下跟踪 connectToAddress(logicalAddress, resolvedPhysicalAddress.next(), unresolvedPhysicalAddress, sniHost) .... return null; }); return future; } private CompletableFuture\u0026lt;Channel\u0026gt; connectToAddress(InetSocketAddress logicalAddress, InetSocketAddress physicalAddress, InetSocketAddress unresolvedPhysicalAddress, InetSocketAddress sniHost) { .... //终于跟踪到了，就是bootstrap.register() 这个方法，可以尝试往里面看看实现，从这里开始就是Netty的代码了 return toCompletableFuture(bootstrap.register()) .thenCompose(channelInitializerHandler::initSocks5IfConfig) .thenCompose(ch -\u0026gt; channelInitializerHandler.initializeClientCnx(ch, logicalAddress, unresolvedPhysicalAddress)) .thenCompose(channel -\u0026gt; toCompletableFuture(channel.connect(physicalAddress))); } public ChannelFuture register() { validate(); //继续往下 return initAndRegister(); } final ChannelFuture initAndRegister() { Channel channel = null; try { //新建一条网络通道到Broker channel = channelFactory.newChannel(); init(channel); } catch (Throwable t) { .... } .... return regFuture; } 从上面的代码跟踪可以看到，生产者对象在创建的时候会通过Netty客户端跟Topic所在的Broker建立TCP网络连接，方便后续的通信\n消息发送 消息发送流程大致如下图\n在选择消息发送是，生产者对象会根据路由策略来决定用目标分区所对应的ProducerImpl对象进行处理 发送前会按照顺序对数据进行拦截器链逻辑处理(如果有配置的话)，然后进行压缩最后再进行序列化操作(消息传输/存储必须序列化) 消息发送前会放到待确认队列中进行维护，每个分区都有一个对应的确认队列，在消息写入成功后会从对应的确认队列中将自己删除，否则这条消息不算写入成功。 将消息发送操作封装成一个任务交给线程池中的一个线程进行最后的发送操作 Broker将数据写入成功后向客户端返回ack，客户端通过ack中携带的消息信息到待确认队列中进行消息的删除 那么老规矩，继续一起看下代码实现吧\n发送流程前置操作 就从常见的这条语句进行切入 producer.sendAsync(\u0026quot;hello java API pulsar:\u0026quot;+i+\u0026quot;, 当前时间为：\u0026quot;+new Date()); 。通过代码中可以看到sendAsync 方法是从其父类ProducerBase中继承的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public CompletableFuture\u0026lt;MessageId\u0026gt; sendAsync(Message\u0026lt;?\u0026gt; message) { //继续跟踪进去 return internalSendAsync(message); } //这是一个抽象方法，有分区生产者和非分区生产者两种实现，跟踪分区生产者的实现逻辑 abstract CompletableFuture\u0026lt;MessageId\u0026gt; internalSendAsync(Message\u0026lt;?\u0026gt; message); CompletableFuture\u0026lt;MessageId\u0026gt; internalSendAsync(Message\u0026lt;?\u0026gt; message) { //继续往里面跟踪 return internalSendWithTxnAsync(message, null); } CompletableFuture\u0026lt;MessageId\u0026gt; internalSendWithTxnAsync(Message\u0026lt;?\u0026gt; message, Transaction txn) { .... //还有印象吗，在生产者创建时会维护一个以分区号为key，ProducerImpl为value的Map，现在从里面获取相对应的对象进行处理 return producers.get(partition).internalSendWithTxnAsync(message, txn); } 消息处理以及包装 又回到ProducerImpl对象的逻辑了，相当于分区生产者对象只是个壳，无论分区还是非分区最终都是调用的ProducerImpl进行消息发送的真正逻辑，不废话继续往下看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 CompletableFuture\u0026lt;MessageId\u0026gt; internalSendWithTxnAsync(Message\u0026lt;?\u0026gt; message, Transaction txn) { .... return internalSendAsync(message); } CompletableFuture\u0026lt;MessageId\u0026gt; internalSendAsync(Message\u0026lt;?\u0026gt; message) { .... //拦截器的逻辑就是在这里生效 MessageImpl\u0026lt;?\u0026gt; interceptorMessage = (MessageImpl) beforeSend(message); .... //核心逻辑，继续切入 sendAsync(interceptorMessage, new SendCallback() { .... }); return future; } public void sendAsync(Message\u0026lt;?\u0026gt; message, SendCallback callback) { checkArgument(message instanceof MessageImpl); .... //核心逻辑，从名字可以看到就是对消息进行序列化并进行发送逻辑，继续跟进去 serializeAndSendMessage(msg, payload, sequenceId, uuid, chunkId, totalChunks, readStartIndex, payloadChunkSize, compressedPayload, compressed, compressedPayload.readableBytes(), callback, chunkedMessageCtx, messageId); } catch (PulsarClientException e) { .... } } private void serializeAndSendMessage(....) throws IOException { //核心逻辑，op是OpSendMsg对象，封装了要发送的消息内容，继续往下跟 processOpSendMsg(op); } } 消息发送 数据已经处理包装得差不多了，接下来就是发送的逻辑，咱们顺着 processOpSendMsg 方法继续往下看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 protected void processOpSendMsg(OpSendMsg op) { .... //将消息加到OpSendMsgQueue队列中，这就是“等待确认队列” pendingMessages.add(op); .... //ClientCnx对象维护着Netty实现跟Broker的TCP连接 final ClientCnx cnx = getCnxIfReady(); if (cnx != null) { .... //WriteInEventLoopCallback方法的run方法执行会将数据发送出去，然后队列中维护消息的状态 cnx.ctx().channel().eventLoop().execute(WriteInEventLoopCallback.create(this, cnx, op)); stats.updateNumMsgsSent(op.numMessagesInBatch, op.batchSizeByte); } else { .... } } catch (Throwable t) { .... } } //WriteInEventLoopCallback是一个线程类，被放到线程池里面执行，因此直接看它的run方法 private static final class WriteInEventLoopCallback implements Runnable { public void run() { .... try { //熟悉Netty的朋友相信对writeAndFlush方法不默认，就是通过之间建立好的TCP连接将数据发送到Broker去 cnx.ctx().writeAndFlush(cmd, cnx.ctx().voidPromise()); op.updateSentTimestamp(); } finally { recycle(); } } } 跟踪到这里基本就结束了，对Netty感兴趣的朋友可以再继续往下跟，这里可以简单说一下，其实它内部是对JDK的NIO做了包装和优化，最底层也是通过Java的Socket连接网络端口进行的数据发送。不知道有没有小伙伴发现，咦，怎么没有对返回结果进行处理的逻辑呢？这就是所谓的异步设计，Netty不就是一个异步通信框架吗，客户端发送的逻辑到这里就是彻底结束了，而消息的处理结束就是要等服务端Broker向客户端发起消息ack请求了，想知道Pulsar怎么实现的吗？那就跟着我一起瞅瞅吧～\n消息确认 消息确认流程是由Broker端发起的，那么生产者对象肯定是通过Netty客户端接收的，所以直接看Pulsar实现的ChannelInboundHandlerAdapter类的channelRead的逻辑即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public abstract class PulsarDecoder extends ChannelInboundHandlerAdapter { public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { .... switch (cmd.getType()) { .... //可以看得到消息有写成功的后续处理动作，那就从这里看看 case PRODUCER_SUCCESS: checkArgument(cmd.hasProducerSuccess()); handleProducerSuccess(cmd.getProducerSuccess()); break; case UNSUBSCRIBE: checkArgument(cmd.hasUnsubscribe()); safeInterceptCommand(cmd); handleUnsubscribe(cmd.getUnsubscribe()); break; } } } handleProducerSuccess这个方法的是由ClientCnx对象进行实现的，那就跟进来看看吧\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 protected void handleProducerSuccess(CommandProducerSuccess success) { checkArgument(state == State.Ready); if (log.isDebugEnabled()) { log.debug(\u0026#34;{} Received producer success response from server: {} - producer-name: {}\u0026#34;, ctx.channel(), success.getRequestId(), success.getProducerName()); } long requestId = success.getRequestId(); if (!success.isProducerReady()) { //还记得pendingRequests 这个“待确认队列”吗，现在会从里面查询对应的消息 TimedCompletableFuture\u0026lt;?\u0026gt; requestFuture = pendingRequests.get(requestId); if (requestFuture != null) { //日志进行打印 log.info(\u0026#34;{} Producer {} has been queued up at broker. request: {}\u0026#34;, ctx.channel(), success.getProducerName(), requestId); //标记为成功 requestFuture.markAsResponded(); } return; } //客户端处理的主要逻辑就是从队列中移除此消息 CompletableFuture\u0026lt;ProducerResponse\u0026gt; requestFuture = (CompletableFuture\u0026lt;ProducerResponse\u0026gt;) pendingRequests.remove(requestId); .... } 总结 生产者写数据的流程到这里基本就结束了，怎么样，是不是没想象中那么可怕？也许你对1、客户端对象创建 2、生产者对象创建 3、消息发送 这三步之间的关系还有点迷迷糊糊，那就请允许我给你举个例子，1、客户端对象创建相当于一座新城市的创建，打好城市的地基，2、生产者对象创建 相当于在这个地基的基础上建起了城市，发电厂等等，最重要的是修建其通往其他各个城市的交通要道，最后的3、消息发送 相当于这个新城市的居民们乘坐高铁去其他的城市。这么说相信你一定已经明白了～如果还有其他疑问欢迎在下面的评论区一起讨论\n","date":"2024-03-21T10:34:57+08:00","image":"https://sherlock-lin.github.io/p/%E4%B8%80%E6%96%87%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82producer%E7%AB%AF%E6%B5%81%E7%A8%8B%E4%BB%A5%E5%8F%8A%E5%8E%9F%E7%90%86/image-20240321153410330_hu2501804792718174333.png","permalink":"https://sherlock-lin.github.io/p/%E4%B8%80%E6%96%87%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82producer%E7%AB%AF%E6%B5%81%E7%A8%8B%E4%BB%A5%E5%8F%8A%E5%8E%9F%E7%90%86/","title":"一文彻底搞懂Producer端流程以及原理"},{"content":"机会其实不多 最近一口气看了《飞驰人生》以及《飞驰人生2》，过去是以看喜剧的心态去看沈腾的电影，当如今二刷时发现这不就是生活吗，只不过用喜剧的外壳做了层包装。两部电影给我影响最深的就是最后的那段对白，“张弛，要不咱们放弃吧，只要以后努力还有机会”，张弛回答：“我努力过无数次，可是机会只出现在其中一两次”。相信这段对白也触动了很多人，特别是步入社会后的朋友们相信会更加有自己的共鸣。社会不像学校哪会，只要你努力就会在下次考试中看到效果；而工作后，有时候你发现自己即便做出十倍、百倍的努力，而工作还是一团遭，如此多次可能也想摆烂算了，在社会的这套复杂的评定体系里，有时候仿佛你的努力是失效的，所以这个时候再看到张弛为了比赛胜利甚至不惜冒着生命危险，仿佛能够理解了，因为有些东西真的不想算了，有些东西真的比生命更加可贵，有些东西它错过了真的就错过了，甚至这辈子都不会再有了。这就是我对《飞驰人生》系列的观后感。那么，你是否也有那么一个时刻，让你愿意选择抛弃全部，来选择穿越回到那时拼尽全力的去做的事情吗？\n只要准备好，机会就会出现 如果你也曾因为过去的某个重要时刻没有把握住机会而浑浑噩噩，想摆烂过完余生，那么我想跟你分享一句话，“只要准备好，机会就会出现”。如今随着互联网的发展，每个人都多了很多曝光自己的机会以及认识更多志同道合的朋友，所以只要你足够优秀就会逐渐被推到台前，是金子就一定会发光，前提是你还足不足够优秀。因此咱们需要做的就是不断打磨自己的专业技能、软技能等等，而在机会出现时务必要紧紧抓住并拼尽全力，把握住机会后再继续静下心来沉淀自己如此反复，从而将自己内核修炼得越来强悍。因此，我们应该学会享受沉淀、修炼的过程，就像是在玩一款很长很长的养成系游戏一般，以“十年磨一剑”的匠心精神慢慢积累，而在机会出现时果断的举起“剑”挥向它，同时高喊一句，“去你X的”。\n总结 我们在每一刻都会面临无数个选择，我们在过去无数个选择中的选定绘制出了如今的生活，而站在未来十年、二十年的角度，选择的自己也是一个起点，从现在开始的每一个决定都会不断的改善未来的自己。简单来说就是“种一棵树最好的时间是十年前，其次是现在”，因此从现在这一刻开始，让咱们为了自己感兴趣的事情，疯狂一次吧\n","date":"2024-03-16T15:09:17+08:00","image":"https://sherlock-lin.github.io/p/%E6%80%BB%E8%A6%81%E6%9C%89%E4%B8%80%E6%AC%A1%E4%B8%BA%E8%87%AA%E5%B7%B1%E7%96%AF%E7%8B%82/image-20240316113459701_hu11518649223452192192.png","permalink":"https://sherlock-lin.github.io/p/%E6%80%BB%E8%A6%81%E6%9C%89%E4%B8%80%E6%AC%A1%E4%B8%BA%E8%87%AA%E5%B7%B1%E7%96%AF%E7%8B%82/","title":"总要有一次，为自己疯狂"},{"content":"引言 系统学习Pulsar的大纲\n正文 下图是我绘制的Pulsar大纲 (由于时间缘故花的比较粗糙，这张图会不定期更新)\n学习大纲 一、Pulsar Client 二、生产者 Pulsar消息路由深入剖析 三、消费者 四、Topic pulsar原来是这样操作topic的 五、Function Pulsar3.2 Function的介绍与使用 Pulsar IO实战 Worker调度管理器原理解析 六、Schema Registry Pulsar Schema使用原理介绍 七、存储管理 详解bookkeeper AutoRecovery机制 八、综合 九、高可用 学习资料 官方文档\nPulsar三本书籍\n《Mastering Apache Pulsar》：个人认为是Pulsar系统资料最好的，喜欢Pulsar的伙伴务必阅读此经典\n《深入解析Apace Pulsar》：林琳大佬的作品，兼备使用、调优以及原理的介绍\n《Apache Pulsar in Action》：目录布局不太好，但针对个别知识点讲得比较精细，可作为补充读物\nPulsar官方整理资料大全：收录了大量Pulsar使用、原理、业务场景等精彩的文章资料\n想对本系列文章读者说的话 坦白了说，现如今的网络环境不像过去那么友好(这个观点不做任何讨论，如果你觉得不对，那么你是对的)，如今还能坚持在网络上输出高质量的作者都是值得尊敬的。我之所以写这系列文章的目的有以下三点：1. 接触技术这些年，受到不少大佬文章的熏陶，技术和思维都有了不少的提升，因此也想做些回馈于技术社区的事情 2. 我知道一定有不少对技术充满热情的小白，我希望能够以Pulsar为切入点给你带来技术的乐趣 3. 通过输出文章来倒推自己持续阅读、提升自己提炼抽象能力。\n我能保证的是以下几点\n这个系列的文章永不收费(降低大家阅读成本) 输出的内容都是经过思考的，拒绝复制张贴以及低价值的东西 尽可能以图、精简的话来协助大家伙对某个知识点的理解 不做模凌两可的解释，宁可偏激的给个错误的答案，我相信有一定依据支撑的错误答案价值大于模糊不清的概念 除此之外，由于本人的知识量以及认知有限，如果有某个知识表述不清楚或者表达有误的地方，恳请指出，大家一起学习共同进步～\n","date":"2024-03-16T10:34:56+08:00","image":"https://sherlock-lin.github.io/p/pulsar%E4%BB%8E%E5%85%A5%E8%BF%B7%E5%88%B0%E5%85%A5%E9%AD%94%E4%B9%8B%E8%B7%AF/image-20240316103554735_hu8487334652089443369.png","permalink":"https://sherlock-lin.github.io/p/pulsar%E4%BB%8E%E5%85%A5%E8%BF%B7%E5%88%B0%E5%85%A5%E9%AD%94%E4%B9%8B%E8%B7%AF/","title":"Pulsar从入迷到入魔之路"},{"content":"概述 大数据背景下，分区应该是所有组件必备的基本条件，否则面对海量数据时无论是计算还是存储都容易遇到瓶颈。跟其他消息系统一样，Pulsar通过Topic将消息数据进行业务层面划分管理，同时也支持Topic分区，通过将多个分区分布在多台Broker/机器上从而带来性能上的巨大提升以及无限的横向拓展能力。而一旦有了分区之后就会面临一个问题，但一条数据请求时应该将其发往哪个分区？目前Pulsar跟其他消息系统一样支持以下三种路由模式。\n轮询路由 生产者会按将消息按批为单位轮询发送到不同的分区，这是一种常见的路由策略，具有简单的优势，由于它不需要过多的配置以及考虑但却可以表现不错的性能。如果消息带有key的话会根据key进行哈希运算后再对分区进行取模来决定消息投放的目标分区。 单分区路由 单分区路由提供一种更简单的机制，它会将所有消息路由到同一个分区。这种模式类似非分区Topic，如果消息提供key的话将恢复到轮询哈希路由方式 自定义分区路由 自定义分区路由支持你通过实现MessageRouter接口来自定义路由逻辑，例如将特定key的消息发到指定的分区等 实战 消息路由发生在生产者端，在创建生产者是通过 .messageRoutingMode() 进行指定，下面就分别实战对比下这三种的路由效果\n轮询路由 先试试轮询路由的策略，这是最常见也是默认的路由策略，通过 .messageRoutingMode(MessageRoutingMode.RoundRobinPartition) 进行指定，然后往里面通过同步方式往分区Topic里面写入数据\n1 2 3 4 5 6 7 8 9 10 11 12 String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(Schema.STRING) .topic(\u0026#34;sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_2\u0026#34;) .messageRoutingMode(MessageRoutingMode.RoundRobinPartition) //.messageRoutingMode(MessageRoutingMode.SinglePartition) .create(); for (int i = 0; i \u0026lt; 20000; i++) { producer.send(\u0026#34;hello java API pulsar:\u0026#34;+i+\u0026#34;, 当前时间为：\u0026#34;+new Date()); } 通过管理页面可以看到数据基本均匀的落在各个分区，从这个结果是能够反向验证数据是符合轮询发送后的效果\n单分区路由 现在试试单分区路由的策略，通过 .messageRoutingMode(MessageRoutingMode.SinglePartition) 进行指定，并往分区Topic里面写入一批数据\n1 2 3 4 5 6 7 8 9 10 11 String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(Schema.STRING) .topic(\u0026#34;sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_2\u0026#34;) .messageRoutingMode(MessageRoutingMode.SinglePartition) .create(); for (int i = 0; i \u0026lt; 20000; i++) { producer.send(\u0026#34;hello java API pulsar:\u0026#34;+i+\u0026#34;, 当前时间为：\u0026#34;+new Date()); } 通过管理页面可以看到数据都落在第一个分区，说明这也符合官网中对单分区路由的描述。同时经过反复试验多次发现，生产者会随机选择一个分区并将所有数据发送到这个分区。\n自定义路由 在有些业务场景，我们需要将自己的业务逻辑“融入”路由策略，因此像Pulsar、Kafka等消息中间件都是支持用户进行路由规则的自定义的。这里为了好玩，咱们尝试将数据按照 1:2:3:4 等比例分别落在四个分区如何？说干就干，自定义路由也是比较简单的，只需要实现Pulsar MessageRouter接口的choosePartition方法即可，实现逻辑如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class SherlockRouter implements MessageRouter { Long count = 0L; public int choosePartition(Message\u0026lt;?\u0026gt; msg, TopicMetadata metadata) { count++; count = count % 10; if (count == 0) return 0; if (count \u0026lt; 3) return 1; if (count \u0026lt; 6) return 2; return 3; } } 通过上面代码可以看到，参数msg就是生产者中国呢发送的消息对象，metadata是这条消息的元数据如租户、命名空间等等，而返回值其实就是这个Topic分区的下标，这里需要注意的是不要超过Topic的分区数，同时一些比较复杂的数据处理逻辑代码尽量不要写在这里影响消息发送性能以及不规范。\n写完后通过 .messageRouter() 方法进行指定即可使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public static void customRoundSchemaProducer() throws Exception { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(Schema.STRING) .topic(\u0026#34;sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_3\u0026#34;) .messageRouter(new SherlockRouter()) .create(); for (int i = 0; i \u0026lt; 20000; i++) { producer.send(\u0026#34;hello java API pulsar:\u0026#34;+i+\u0026#34;, 当前时间为：\u0026#34;+new Date()); } producer.close(); pulsarClient.close(); } 在管理页面可以看到，数据是按照咱们预期的逻辑 1:2:3:4等比落在分区里面，嘿嘿～\n源码分析 接口以及父类 Pulsar中所有路由规则都是基于MessageRouter接口进行实现的，这个接口主要提供了choosePartition方法，只要重写这个方法即可自定义任意自己预期的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @InterfaceAudience.Public @InterfaceStability.Stable public interface MessageRouter extends Serializable { /** * * @param msg * Message object * @return The index of the partition to use for the message * @deprecated since 1.22.0. Please use {@link #choosePartition(Message, TopicMetadata)} instead. */ @Deprecated default int choosePartition(Message\u0026lt;?\u0026gt; msg) { throw new UnsupportedOperationException(\u0026#34;Use #choosePartition(Message, TopicMetadata) instead\u0026#34;); } /** * Choose a partition based on msg and the topic metadata. * * @param msg message to route * @param metadata topic metadata * @return the partition to route the message. * @since 1.22.0 */ default int choosePartition(Message\u0026lt;?\u0026gt; msg, TopicMetadata metadata) { return choosePartition(msg); } } MessageRouterBase是路由策略的抽象类，主要定义了消息有key时的哈希算法，像上面提的轮询路由和单分区路由继承了这个抽象类。JavaStringHash和Murmur3Hash32两个都是Pulsar提供的哈希算法的实现类，两者的差异后面再单独进行分析\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public abstract class MessageRouterBase implements MessageRouter { private static final long serialVersionUID = 1L; protected final Hash hash; MessageRouterBase(HashingScheme hashingScheme) { switch (hashingScheme) { case JavaStringHash: this.hash = JavaStringHash.getInstance(); break; case Murmur3_32Hash: default: this.hash = Murmur3Hash32.getInstance(); } } } 轮询路由的实现 主要看choosePartition 方法的逻辑，首先如果消息带有key则针对key进行哈希然后取模，这样可以保证相同key的消息落在同一个分区。然后就是判断消息是否按批次进行发送的，如果是单条消息发送的话则通过一个累加计数器进行轮询分区，即可达到消息按照分区顺序逐个发送的效果；如果是按批次发送的话，则是根据时间戳进行取模，这样达到的效果就是每批数据都会随机发送到某一个分区\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public class RoundRobinPartitionMessageRouterImpl extends MessageRouterBase { @SuppressWarnings(\u0026#34;unused\u0026#34;) private volatile int partitionIndex = 0; private final int startPtnIdx; private final boolean isBatchingEnabled; private final long partitionSwitchMs; .... @Override public int choosePartition(Message\u0026lt;?\u0026gt; msg, TopicMetadata topicMetadata) { // If the message has a key, it supersedes the round robin routing policy if (msg.hasKey()) { return signSafeMod(hash.makeHash(msg.getKey()), topicMetadata.numPartitions()); } if (isBatchingEnabled) { // if batching is enabled, choose partition on `partitionSwitchMs` boundary. long currentMs = clock.millis(); return signSafeMod(currentMs / partitionSwitchMs + startPtnIdx, topicMetadata.numPartitions()); } else { return signSafeMod(PARTITION_INDEX_UPDATER.getAndIncrement(this), topicMetadata.numPartitions()); } } } 单分区路由 可以看到单分区的逻辑是比较简单且清晰的，如果有key就进行哈希取模，否则就发送到partitionIndex这个成员变量指定的分区去，那么这个partitionIndex指定的是哪个分区呢？通过代码能看到是从构造函数里面传进来的，因此跟踪下代码看看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class SinglePartitionMessageRouterImpl extends MessageRouterBase { private final int partitionIndex; public SinglePartitionMessageRouterImpl(int partitionIndex, HashingScheme hashingScheme) { super(hashingScheme); this.partitionIndex = partitionIndex; } @Override public int choosePartition(Message\u0026lt;?\u0026gt; msg, TopicMetadata metadata) { // If the message has a key, it supersedes the single partition routing policy if (msg.hasKey()) { return signSafeMod(hash.makeHash(msg.getKey()), metadata.numPartitions()); } return partitionIndex; } } 通过跟踪可以看到是在PartitionedProducerImpl类的getMessageRouter方法中进行SinglePartitionMessageRouterImpl类的初始化，同时是通过ThreadLocalRandom.current().nextInt(topicMetadata.numPartitions()) 来生成一个小于分区数的随机数，因此单分区路由的分区是随机指定的一个，这个结果跟咱们实战中测试的效果是吻合的。除此之外，咱们还看到 getMessageRouter方法中会根据咱们在创建生产者时 .messageRoutingMode 方法指定的路由模式来创建对应的路由实现类，在这里可以明确的看到没有指定的话默认就是采用的轮询路由规则\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 private MessageRouter getMessageRouter() { MessageRouter messageRouter; MessageRoutingMode messageRouteMode = conf.getMessageRoutingMode(); switch (messageRouteMode) { case CustomPartition: messageRouter = Objects.requireNonNull(conf.getCustomMessageRouter()); break; case SinglePartition: messageRouter = new SinglePartitionMessageRouterImpl( ThreadLocalRandom.current().nextInt(topicMetadata.numPartitions()), conf.getHashingScheme()); break; case RoundRobinPartition: default: messageRouter = new RoundRobinPartitionMessageRouterImpl( conf.getHashingScheme(), ThreadLocalRandom.current().nextInt(topicMetadata.numPartitions()), conf.isBatchingEnabled(), TimeUnit.MICROSECONDS.toMillis(conf.batchingPartitionSwitchFrequencyIntervalMicros())); } return messageRouter; } 总结 通过以上内容相信你对Pulsar的路由规则有一定的了解了，如果想进一步了解可以尝试按照自己喜好实现下路由规则并观测是否按照预期运行，同时也可以跟踪Pulsar的源码看看实现是否符合预期。如果想彻底掌握Pulsar，最好自己跟踪下Pulsar的一些核心逻辑，这样不仅了解其底层是如何运作的，也能加深你对一些设计/特性的印象。\n","date":"2024-03-13T10:34:56+08:00","image":"https://sherlock-lin.github.io/p/pulsar%E6%B6%88%E6%81%AF%E8%B7%AF%E7%94%B1%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90/image-20240306152645225_hu5442259153227018439.png","permalink":"https://sherlock-lin.github.io/p/pulsar%E6%B6%88%E6%81%AF%E8%B7%AF%E7%94%B1%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90/","title":"Pulsar消息路由深入剖析"},{"content":"引言 今天跟着 官方文档 基于docker玩一把Pulsar IO吧\n概要 在用户能够轻松的将消息队列跟其他系统(数据库、其他消息系统)一起使用时，消息队列的作用才是最强大的。而Pulsar IO connectors可以让你很轻松的创建、部署以及管理这些跟外部系统的连接，例如mysql、kafka、cassandra等。\nPulsar connector分为Source和Sink两种，Source connector会将数据从外部系统喂给Pulsar，而Sink connector负责将数据从Pulsar喂给外部系统。\nPulsar connector是一种特殊的Function，只不过这个Function持有其他系统的客户端作为pulsar与其他系统的桥梁，它在处理保证上跟Function是一致的，分别是最多一次、至少一次、精准一次。处理保证不仅依靠Pulsar，还跟外部系统相关以及实现逻辑相关。\n最多一次：发给connector的消息最多处理一次或者不做处理 至少一次：发给connector的消息处理一次或者多次 精准一次：发给connector的消息只处理一次 实战 安装connector 在 这里 下载对应的connector，先选择对应的版本，在点进 connectors 目录选择对应的source或者sink\n将下载的nar文件放到pulsar安装地址的connectors 目录下(没有需要创建)\n启动Pulsar\n通过指令查看服务connector信息，先输出下面这样的信息就说明connector已经注册到Pulsar上面了\n1 curl -s http://localhost:8080/admin/v2/functions/connectors 安装Cassandra 基于 brew install --cask --appdir=/Applications docker 安装docker(仅针对mac环境)\n基于docker运行 cassandra，成功运行后通过 docker ps可以看到Cassandra服务已经起来了\n1 docker run -d --rm --name=cassandra -p 9042:9042 cassandra:3.11 通过 docker exec -ti cassandra cqlsh localhost 进入Cassandra服务的容器，并通过以下指令进行库表的初始化\n1 2 3 4 5 CREATE KEYSPACE pulsar_test_keyspace WITH replication = {\u0026#39;class\u0026#39;:\u0026#39;SimpleStrategy\u0026#39;, \u0026#39;replication_factor\u0026#39;:1}; USE pulsar_test_keyspace; CREATE TABLE pulsar_test_table (key text PRIMARY KEY, col text); 先查询该表确保没有数据 select * from pulsar_test_table;\n功能验证 写配置文件cassandra-sink.yml\n1 2 3 4 5 6 configs: roots: \u0026#34;localhost:9042\u0026#34; keyspace: \u0026#34;pulsar_test_keyspace\u0026#34; columnFamily: \u0026#34;pulsar_test_table\u0026#34; keyname: \u0026#34;key\u0026#34; columnName: \u0026#34;col\u0026#34; 启动写Cassandra的sink，启动后通过指令查看显示sink已经正常启动\n1 2 3 4 5 6 7 pulsar-admin sinks create \\ --tenant public \\ --namespace default \\ --name cassandra-test-sink \\ --sink-type cassandra \\ --sink-config-file examples/cassandra-sink.yml \\ --inputs test_cassandra 执行命令批量往pulsar中写入数据，看是否会正常输出到Cassandra中\n1 for i in {0..9}; do pulsar-client produce -m \u0026#34;key-$i\u0026#34; -n 1 test_cassandra; done 由于上面的操作是有延迟的，所以不断的查询Cassandra的表是可以看到数据在逐步的增加，并最终写满十条数据\n总结 纸上得来终觉浅，绝知此事要躬行。 学习不能仅仅停留在纸面上或者理论，脱离使用去探讨设计或者源码都是不切实际的。因此今天一起体验了一把Pulsar IO，除此之外Pulsar还提供了非常丰富的跟其他系统交互的Connector，详细可以看上面发的下载地址并尝试使用自己感兴趣的Connector感受下实操的快乐～\n","date":"2024-03-13T10:34:55+08:00","image":"https://sherlock-lin.github.io/p/pulsario%E5%AE%9E%E6%88%98/image-20240313191618099_hu9489298534787853151.png","permalink":"https://sherlock-lin.github.io/p/pulsario%E5%AE%9E%E6%88%98/","title":"PulsarIO实战"},{"content":"引言 为何要模块化，这里的主体是人，客体是事物。当事物很小时，人可以很轻松的解决；但是当事物远大于人能处理的范围时，我们就可以考虑对它进行模块化分解。模块化是一种解决复杂问题的方式，放之四海而皆可用，以下就举几个例子聊聊模块化\n分封制 商朝之前，统治者一个人管理硕大的疆域，整个疆域大大小小事都要自己操心亲力亲为，像商朝不就是最后被偷家了，手动狗头。因此周朝的统治者采用分封制进行管理，庞大的帝国不好打理那就划分成多块领域也就是将大的东西分解成不那么大的东西分别进行管理，而随后春秋战国直到秦朝后，开始实行郡县制，本质上就是将模块分解更加精细化，这样打理整个庞大的帝国就方便很多，并且非常大幅提高了整体的效率，这不在短短几十年修长城、统一度量衡、文字等等。这些事情要是放在秦朝前那几乎是难以想象的，但因为模块化了，相当于每个县都是一个独立的单元具备自我管理的功能，而统治者到县之间的这些层级本质上都是在做政令的传达以及结果反馈。这种设计如下图，模块化的符号化其实就是一颗多叉树，原本要一个人管理的国家分成了6个县，原本一个人要打理的事情分成了6个人处理并且这是可以无限扩展的。每一层各司其职，从而整体达到最大的效率。\n大数据 大数据时代迎来的数据的爆炸式增长，庞大的数据在存储和计算上都成了很大的难题，因此大佬们通过设计最终采用了目前下面这种方案，将数据进行拆解，各个小数据集合再落在具体的某台Linux机器上进行存储或者计算\n总结 上面就是模块化的好处，原本看似不可能完成的东西通过模块化分解后都是可以完成的。因此在这里想表达的是，虽然学海无涯，但是对于咱们热爱的事情，可以利用好这个方式，将要做的大任务或者要学习的大的方向进行分解，然后在逐个攻克一个个小单元，因此不用害怕任何事情，大胆折腾起来\n","date":"2024-03-05T15:09:15+08:00","image":"https://sherlock-lin.github.io/p/%E4%B8%87%E7%89%A9%E7%9A%86%E5%8F%AF%E6%A8%A1%E5%9D%97%E5%8C%96%E5%88%86%E8%A7%A3/image-20240920152044106_hu17856311937218138140.png","permalink":"https://sherlock-lin.github.io/p/%E4%B8%87%E7%89%A9%E7%9A%86%E5%8F%AF%E6%A8%A1%E5%9D%97%E5%8C%96%E5%88%86%E8%A7%A3/","title":"万物皆可模块化分解"},{"content":"引言 Pulsar Function中最重要的角色是Worker，而Worker中最核心的类就是调度管理器SchedulerManager，本篇博客就专门对它进行剖析\n正文 SchedulerManager职责有以下两点\n任务调度 重平衡 任务调度是SchedulerManager最重要的逻辑，咱们通过 pulsar-admin functions create xxx 启动一个function实例时，Pulsar 如何决定在哪台worker所在节点启动这个function实例，这就是任务调度要做的事情，除此之外任务调度还包含function更新、删除以及任务的生命周期管理。而重平衡是稳定性相关的，当咱们的function实例分布不均衡时，可能会导致涝的涝死，旱的旱死，因此需要咱们做function实例的重新分配，这也是SchedulerManager要做的事情\n任务调度 在开始讲解任务调度之前，咱们先看看下面这张图\n图中的上下方分别是两个worker进程实例，在咱们通过 bin/pulsar functions-worker 启动worker进程时服务内部会启动consumer并以failover方式来在Broker进行消息订阅消费，failover机制保证同一时间内只有一个Consumer能进行订阅，因此Worker通过这个机制来进行选主，成功订阅的worker晋升为leader负责任务的派发，未成功订阅的worker变为follwer并通过Reader监听Broker中的任务。其中Leader和Follwer的主要逻辑分别如下\nworker-leader 通过Consumer-failver机制成为leader后，会立马启动一个Producer负责往Broker中发送任务消息，每当有新建启动function请求进来，就会往Broker中写入一条对应的任务消息。除此之外Leader还会通过Consumer消费Broker中保存的元数据，并以Map数据结构存在内存中作为Leader的元数据管理的数据来源，这里的数据有Function信息、Instance信息等，在接受到我不请求是也会同步更新这里，同时也会定期的根据元数据检查来排查出异常的Function、Instance等。\nworker-follwer 在worker成为follower后，会启动Reader监听Broker中的任务，当监听到有新任务时会调用任务管理器进行处理，例如在有新增启动Function时，如果是配置的线程级别则以线程方式启动Instance，如果是配置的进程级别则会拉起一个独立的Instance进程进行Function任务的执行。\n源码解析 调度逻辑的主要入口在SchedulerManager的invokeScheduler方法\n获取所有Function 以及Instance 删除所有不存在的Function/Instance的分配器，同时更新分配器的信息 如果元数据管理器中存有Function，但是这个Function对应的InstanceID不存在于Runtime管理器的Runtime列表 则从Master元数据中删除，并通过内部Topic发送一条清除这个Instance给Worker 通过元数据比较获取需要新增的Instance并包装成Assignment，通过内部Topic发送这条新增Instance给Worker 获取还未启动的Instance实例，并通过RoundRobinScheduler的schedule方法派发到对应的Worker节点进行启动 循环通过FunctionRuntimeManager.processAssignment启动Instance 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 void invokeScheduler() { long startTime = System.nanoTime(); //获取当前worker集群中可用的worker节点信息 Set\u0026lt;String\u0026gt; availableWorkers = getCurrentAvailableWorkers(); //获取所有Function List\u0026lt;FunctionMetaData\u0026gt; allFunctions = functionMetaDataManager.getAllFunctionMetaData(); //获取所有Instance Map\u0026lt;String, Function.Instance\u0026gt; allInstances = computeAllInstances(allFunctions, functionRuntimeManager.getRuntimeFactory().externallyManaged()); //获取当前每个 workerId-\u0026gt;(InstanceId—\u0026gt;任务) Map\u0026lt;String, Map\u0026lt;String, Assignment\u0026gt;\u0026gt; workerIdToAssignments = functionRuntimeManager .getCurrentAssignments(); // 初始化调度状态记录器 SchedulerStats schedulerStats = new SchedulerStats(workerIdToAssignments, availableWorkers); //delete assignments of functions and instances that don\u0026#39;t exist anymore Iterator\u0026lt;Map.Entry\u0026lt;String, Map\u0026lt;String, Assignment\u0026gt;\u0026gt;\u0026gt; it = workerIdToAssignments.entrySet().iterator(); while (it.hasNext()) { Map.Entry\u0026lt;String, Map\u0026lt;String, Assignment\u0026gt;\u0026gt; workerIdToAssignmentEntry = it.next(); Map\u0026lt;String, Assignment\u0026gt; functionMap = workerIdToAssignmentEntry.getValue(); // remove instances that don\u0026#39;t exist anymore functionMap.entrySet().removeIf(entry -\u0026gt; { String fullyQualifiedInstanceId = entry.getKey(); boolean deleted = !allInstances.containsKey(fullyQualifiedInstanceId); if (deleted) { Assignment assignment = entry.getValue(); MessageId messageId = publishNewAssignment(assignment.toBuilder().build(), true); // Directly update in memory assignment cache since I am leader log.info(\u0026#34;Deleting assignment: {}\u0026#34;, assignment); functionRuntimeManager.deleteAssignment(fullyQualifiedInstanceId); // update message id associated with current view of assignments map lastMessageProduced = messageId; // 更新状态 schedulerStats.removedAssignment(assignment); } return deleted; }); // update assignment instances in case attributes of a function gets updated for (Map.Entry\u0026lt;String, Assignment\u0026gt; entry : functionMap.entrySet()) { String fullyQualifiedInstanceId = entry.getKey(); Assignment assignment = entry.getValue(); Function.Instance instance = allInstances.get(fullyQualifiedInstanceId); if (!assignment.getInstance().equals(instance)) { functionMap.put(fullyQualifiedInstanceId, assignment.toBuilder().setInstance(instance).build()); Assignment newAssignment = assignment.toBuilder().setInstance(instance).build().toBuilder().build(); MessageId messageId = publishNewAssignment(newAssignment, false); // Directly update in memory assignment cache since I am leader log.info(\u0026#34;Updating assignment: {}\u0026#34;, newAssignment); functionRuntimeManager.processAssignment(newAssignment); // update message id associated with current view of assignments map lastMessageProduced = messageId; //update stats schedulerStats.updatedAssignment(newAssignment); } if (functionMap.isEmpty()) { it.remove(); } } } List\u0026lt;Assignment\u0026gt; currentAssignments = workerIdToAssignments .entrySet() .stream() .filter(workerIdToAssignmentEntry -\u0026gt; { String workerId = workerIdToAssignmentEntry.getKey(); // remove assignments to workers that don\u0026#39;t exist / died for now. // wait for failure detector to unassign them in the future for re-scheduling return availableWorkers.contains(workerId); }) .flatMap(stringMapEntry -\u0026gt; stringMapEntry.getValue().values().stream()) .collect(Collectors.toList()); //获取未分配的Function任务 Pair\u0026lt;List\u0026lt;Function.Instance\u0026gt;, List\u0026lt;Assignment\u0026gt;\u0026gt; unassignedInstances = getUnassignedFunctionInstances(workerIdToAssignments, allInstances); workerStatsManager.scheduleStrategyExecTimeStartStart(); //触发任务真正调度派发的入口 List\u0026lt;Assignment\u0026gt; assignments = scheduler.schedule(unassignedInstances.getLeft(), currentAssignments, availableWorkers); workerStatsManager.scheduleStrategyExecTimeStartEnd(); assignments.addAll(unassignedInstances.getRight()); if (log.isDebugEnabled()) { log.debug(\u0026#34;New assignments computed: {}\u0026#34;, assignments); } isCompactionNeeded.set(!assignments.isEmpty()); //更新到Leader内存中的元数据 for (Assignment assignment : assignments) { MessageId messageId = publishNewAssignment(assignment, false); // Directly update in memory assignment cache since I am leader log.info(\u0026#34;Adding assignment: {}\u0026#34;, assignment); functionRuntimeManager.processAssignment(assignment); // update message id associated with current view of assignments map lastMessageProduced = messageId; // update stats schedulerStats.newAssignment(assignment); } log.info(\u0026#34;Schedule summary - execution time: {} sec | total unassigned: {} | stats: {}\\n{}\u0026#34;, (System.nanoTime() - startTime) / Math.pow(10, 9), unassignedInstances.getLeft().size(), schedulerStats.getSummary(), schedulerStats); } 总结 以上就是Pulsar中Worker的调度过程，这里主要是以Worker独立部署的方式进行讲解的，基于Broker启动的Worker感兴趣的朋友可以自行跟踪下代码。\n","date":"2024-03-05T10:34:57+08:00","image":"https://sherlock-lin.github.io/p/worker%E8%B0%83%E5%BA%A6%E7%AE%A1%E7%90%86%E5%99%A8%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/image-20240305140605827-9618768_hu15216224640484705908.png","permalink":"https://sherlock-lin.github.io/p/worker%E8%B0%83%E5%BA%A6%E7%AE%A1%E7%90%86%E5%99%A8%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/","title":"Worker调度管理器原理解析"},{"content":"引言 《皮囊》里告诉我们，每个人都是由一副皮囊和一个灵魂组成，皮囊就像一辆汽车，灵魂就像这辆汽车的驾驶员，从母亲的羊水中离开那一刻就启动汽车，而终点就是我们离开这个世界的那一刻。但这个生命周期是这副皮囊的，那么灵魂呢？围绕这个问题几千年来的智者们已经给出很多结论，以及根据一些理论创办对应自洽的的宗教信仰，这篇文章并不是要讨论是否有来世的问题(以我目前的知识量也没法想到什么有用的结论)，但这篇文章想表达的是，在汽车行驶的这段路程，咱们作为驾驶员有哪些值得我们思考的。\n皮囊-灵魂依赖反转 人开车，理想情况下应该是开往人想去的地方，而不是任由车“自动驾驶”，或者是参考车是什么颜色，开了多少里程数了来决定自己想去的地方。那么，为什么要用年龄、环境等东西来捆绑住自己的“皮囊”呢，例如以下问题\n我今年已经三十一了，喜欢编程想学编程是不是有些晚了 要是三年前就XXX就好了 我热爱并且想做A，但是我的工作内容却一直是B，好苦恼 \u0026hellip;. 更多的例子就不一一例举了，本质上都是用皮囊束缚住了灵魂，如果你想透彻了你会发现这件事情没那么复杂，喜欢去做就完了，就这 么简单，如下图所示，灵魂不该给皮囊服务或让步，应该折腾好皮囊，利用它服务好我们的灵魂\n使命感 \u0026ldquo;人活着是为了什么？\u0026quot;，对于这个问题，我听过无数个答案，听过一个比较认同的就是，“每个人来到这个世界上都有自己的一个使命，每个灵魂要做的就是发现这个使命，并为它奉上自己的一切”。这就是我在这里想提的使命感，如今的生活是个物质充裕但精神匮乏的世界，虽然不少朋友已经解决温饱过上所谓的“小资”生活，但成天闷闷不乐，甚至有个别患上精神疾病如抑郁症。针对精神问题我也曾经饱受困扰，现如今逐渐醒悟过来也希望能够帮到你，我的方式就是找到自己热爱的东西也就是自己的使命，然后彻底贯彻它，但你将全部的精力集中在这个点时，分散在其他不快乐的精力就会少很多很多，同时由于你在做一件你热爱的事情时，你不会容易觉得累甚至会觉得有无穷的精力。\n我的使命 在大学接触编程时，也曾幻想用代码改变世界，同时也在钻研沉淀自己的专业技能。但是在工作后发现公司是个盈利的机构，有时候它所追求的东西会限制住你的发挥，甚至会有很多你不喜欢的东西充斥着你的生活，开不完的会、干不完的活以及很多消耗你精气神的事情，经过这些东西消磨后你发现自己对技术的热情不再，但是当你真的一段时间不接触技术后你陷入了更大的痛苦，仿佛丢了罗盘的航海船一般浑浑噩噩。\n最终经过分析发现，其实是违背了自己的使命以及给自己的不努力找借口，当你真的热爱一个东西的时候，即便是神明都无法阻挡你去追求它。如今在技术层面我不会再去思考这个东西会不会给自己带来收益，只要感兴趣的那就学以及深挖；在形式上，我不在局限于给开源社区贡献代码，像输出技术文章，输出心得体会也是一种创造价值的方式，只要能够多影响到一个就够了，这是我的初衷，至于结果会是什么交给未来吧，只要坚持做自己热爱的事情就够了。说了这么多，我的使命是什么呢，简单的说就是创造，写软件改变世界也是创造，输出文章影响读者也是一种创造，这就是我热爱的事情并不断在这个过程中提升自己的思维，用皮囊服务于灵魂。\n总结 最后与各位读者分享一句话：人有两次生命的诞生，一次是你肉体出生，一次是你灵魂觉醒。\n","date":"2024-02-28T15:09:16+08:00","image":"https://sherlock-lin.github.io/p/%E4%B8%80%E5%89%AF%E7%9A%AE%E5%9B%8A%E7%9A%84%E4%BD%BF%E5%91%BD/image-20240228143652107_hu11282021279702529150.png","permalink":"https://sherlock-lin.github.io/p/%E4%B8%80%E5%89%AF%E7%9A%AE%E5%9B%8A%E7%9A%84%E4%BD%BF%E5%91%BD/","title":"一副皮囊的使命"},{"content":"引言 随着生活水平的提高，不少人的目标从原先的解决温饱转变为追求内心充实，但由于现在的时间过得越来越快以及其他外部因素，我们 对很多东西的获取越来越没耐心，例如书店经常会看到《7天精通Java》、《3天掌握XXX》等等之类的书籍，然后大致翻阅后以及经过短暂的实操后，多巴胺充斥着身体，在获得享受后又贪婪的将目光转向下一个目标～我们总在忙，总在不断的学，但是最后大概率会被社会的棒子打回原型，我们其实什么都不会。在获得这样的答案后，我们会变的更贪婪，觉得是自己不够努力，于是更加的拼命学，周而复始直到精疲力竭，最后悲愤高呼“世道如此，悲夫XXX”，因此在这篇文章想表达的是，“欲速则不达，慢就是快”\n技术迷张三的悲哀 张三是个对技术充满热情的程序员，在大学时期先接触的Java编程语言，于是照着《7天精通Java》敲完上面的例子后，感觉已经掌握了Java。于是看着安卓风吐火如荼时，也跟着下载个AndroidStudio开发一些小的APP，开发出一些小case后感觉安卓也差不多掌握了，但是如果再继续精进的话就要付出更多的精力，于是在全栈流行的时候，又跟着学习前端Web知识，自己搭建一个完整的Web服务等等。在毕业时，当他信心满满的去面试时，被面试官狠狠的按在地上摩擦～于是他只能选择一个不上不下的公司先干着，这个时候大数据风头来了，于是他趁着风头转了大数据行业，由于之前面试的经验告诉它，学技术一定要深挖，于是呼在简单学习一些基础知识后，就立马吭哧吭哧的啃起了大数据组件的源码，味涩难懂没关系，看不懂就看别人讲解甚至尝试去背，他觉得只要把这些热门的组件的代码都“搞懂”那自己就是香饽饽了，最终他工作上还是四处碰壁，他不服气，觉得是自己懂得不够多，于是乎啃完Hadoop、啃HBase、Kafka、RocksDB、ClickHouse等等，甚至编程语言还去学Python、Rust、Go等等，他觉得自己已经还可以了，但是最终在社会的毒打中，身心疲惫的转行做其他的了\n悲从何来 张三很忙，也很努力，但是悲哀的是，他在每一个方向都只学了最简单的知识，并错误的认为自己以为掌握了，实则只是冰上一角。 因此虽然他很努力，但是悲剧是必然的。张三只是宇宙中的一粒沙子，他的理想伴随着宇宙规律堙灭无人知晓，但如果我们在他身上看到了自己的影子，那么我们可能需要注意下，避免这种逃避困难的盲目的努力，这本质上也是一种“奶头乐”。用张一鸣的话来说，“他们为了避开思考，愿意付出一切努力”\n解法 静，勿燥 这跟 有点矛盾，一句话说清两者的关系“战略上要静，战术上要快”。指导员在抗战时期提出的“论持久战”就是战略上的静，我们知道目的地在那里，但也要承认这段路是比较漫长的，只靠冲刺是永远到达不了的。因此在长远目标定下来后，心要静下来，然后针对眼前的一个个要做的事情，莽起来，大胆去做，因为即便一两件事搞崩了也远不会影响你的最终目标。\n规划性 在有目标后，要定一个大纲来进行指导，每一个阶段的小方向是什么，再到小方向上要做的事，最后再逐个攻克。在攻克的过程中识别到新的信息后，可以反过来调整计划。\n跟外界保持联系 要承认的一件事，我们很难独立于这个世界，我们需要得到这个世界的认可从而获得收益例如金钱、权力、名誉等等，因此我们不能闭门造车。要跟世界保持联系，例如学某个技术不要自己埋头硬啃，也要关注一下别的公司业务是如何用的，遇到了哪些问题，一定要记住一件事，你学一个东西一定是要用来解决某个“问题的，因此你要了解这个“问题”。如果只是埋头硬啃除了可能会脱离业务，还可能会钻入一些牛角尖的地方徒徒耗费时间等，因此务必要跟外界保持联系\n总结 在最后，跟大家分享一句很喜欢的话，“如果有一天，你不再寻找爱情，只是去爱;你不再渴望成功，只是去做;你不再追求空泛的成长，只是开始修养自己的性情;你的人生一切，才真正开始”\n","date":"2024-02-20T15:09:17+08:00","image":"https://sherlock-lin.github.io/p/%E6%AC%B2%E9%80%9F%E5%88%99%E4%B8%8D%E8%BE%BE%E6%85%A2%E5%B0%B1%E6%98%AF%E5%BF%AB/image-20240920152413904_hu15743919184586671853.png","permalink":"https://sherlock-lin.github.io/p/%E6%AC%B2%E9%80%9F%E5%88%99%E4%B8%8D%E8%BE%BE%E6%85%A2%E5%B0%B1%E6%98%AF%E5%BF%AB/","title":"欲速则不达，慢就是快！"},{"content":"引言 随着知识量的增加，有时会减低执行力。小时候很多东西不知道，但是就很莽，大胆的撬开电风扇，哪怕组装回来多了几个零件也很自豪；小时候好奇海的那边有什么，跟着小伙伴们沿着海岸线跑一下午都不觉得累。但是随着对这个世界的了解的增加，我们逐渐变得“理智”，喜欢的人还没开始去追就内心觉得反正也成功不了就放弃了，有些工作还没开始就觉得这个设计方案行不通甚至开发好的代码迟迟不上线，因为不发布就不会有问题等等呢个。为了避免失败，于是乎放弃所有开始，但是今天我想说的是，莽，就完事拉~\n知行合一 第一次听到“知行合一”这个词，是从王阳明《传习录》里看到的，但开始理解是从董宇辉的直播上，而在近期真正实战中有了进一步的领悟，因此把它排在第一。我是一个喜欢思考的人，在内心中觉得几乎所有事情都可以在心中的沙盘进行推演然后得出最优解，最后再将最优解进行落地。这在上学时期没问题，但工作后如果你也跟我一样的话，在工作中会很容易吃瘪，因为很多时候但一项工作派给你时，很多信息是不确定的，你无法根据这些信息获得所有的“最优解”，例如要开发一个新功能，你如果先花大量时间去基于技术实现的角度来思考哪个方案更好，那么在实现时大概率会受阻。比如这个方案在真正落地时很有可能遇到以下阻力：① 依赖外部服务，但目前外部还不支持这种功能 ② 你设计的是回调方式同步数据，但等你设计好落地时发现由于安全问题网络知识单向的 ③ 这个方案在实现时复杂度比原来想的要高的多得多。除了上诉的几个场景，相信你可能还遇到其他的，有想过为什么吗？\n上诉最大的问题就是，花了过多的时间在方案设计上，除非一些特别特别核心的点，大部分情况下拿到活了直接开干，代码能跑就行，先实现起来调试，因为“行”就是“知”，或者说只有当你真正的行动起来，你才能获取更多相关的信息，视野才能更加的开阔起来。单纯靠想是永远都想不明白，与其花大量时间纠结走哪一条路，不如先挑一条走起来，也许你走个几分钟就发现这条路走不下去折返去另一条路呢，这不比你花几个小时在哪里纠结强吗？这个时候的“走错路”难道是真的错吗？不见得吧。因此当你真的内心有想法时，大胆去尝试；当觉得某个方案有问题时，大胆的提出来；不要被所谓的“犯错”限制住了自己，哪怕你想法是错误的，你也只有尝试了，提出来了才能获取到这个结论不是吗，“行”就是“知”。所以如果有喜欢的东西，趁着热情还没消散，莽一点，相信你会有不一样的体验~\n你遇见的所有人，都是你自己 “你遇见的所有人，都是不同平行世界里的自己。” 这句话送给内向型敏感性格的朋友，不用害怕他们，他们其实都是“你”。这里的意思是，这个世界上所有你看到的人都是由“你”组成的，这是属于你的世界。所以不用在意“自己”的看法，大胆去去做你想做的事情，可能会有人说三道四，但是记住，这是另一个世界的你对当前的这个你的看法。就如同是现在的你批评过去某个阶段的自己一样，没什么差别。为什么这么我要在这里提一下呢，因为内向型敏感性格的人，有自己的优缺点。如果过多去关注外界的看法会大量消耗自己的精气神，但是精气神这玩意是固定的，少了之后就会降低你做其他事情的动力。内向型敏感人格有自己的优点，从产出最大化的角度来看，应该将更多的精力专注于自己感兴趣的事情上，记住，是感兴趣的事情上而不是自己身上。例如应该关注的是自己的表达水平是否提升而不是关注有人说你不近人情，应该关注想学的东西例如英语今天背了多少词汇量而不是关注自己的衣服脏了一点之类的，大致意思相信你能明白\n热爱，可克服一切阻碍 除了上诉两点，最后我想说的是，保持你热爱的东西，这是你动力的源泉，如果不能保持，那就尽量去“爱上”你在做的事情。很多东西做一两天容易，但是长期坚持的话，如果不是发自内心的热爱，那就是一件很折磨人的事情。并且特别是在坚持件事的前两年，由于不熟悉你会遇到各种各样的阻碍，这是劝退大部分人的时期，但是如果你坚持下来了，收获一般相对也比较丰富，同时你内心也会变强。这样再去做其他有难度的事情时，你能比别人更皮实。例如李笑来大佬小时候被逼着抄新华词典，等真的抄完后，之后他感觉遇到的所有事仿佛都没那么难熬了。因此保持热爱，去坚持做的想做的事情~\n总结 以上就是我想说的，最后，还是用标题来结尾，哥们，莽就完事了，开干吧~\n","date":"2024-02-04T15:09:14+08:00","image":"https://sherlock-lin.github.io/p/%E8%8E%BD%E5%B0%B1%E5%AE%8C%E4%BA%8B%E5%95%A6/the-creative-exchange-d2zvqp3fpro-unsplash_hu5876398126655421130.jpg","permalink":"https://sherlock-lin.github.io/p/%E8%8E%BD%E5%B0%B1%E5%AE%8C%E4%BA%8B%E5%95%A6/","title":"莽，就完事啦"},{"content":"引言 在谈绩效后，我收获了一些心得，在此梳理出来，加深印象并且共勉\n基本信息 在步入职场后，你可能跟我一样虽然技术水平有在上升，但是在处理一些事情上可能偶尔没能获得预期的成果。我在通过绩效沟通以及自我反思后得出以下几点，如果你也有类似的情况可能需要加强些\n沟通方式 工作中完整的沟通链路 何时沟通 风险汇报 心态方面 皮实 客观 师傅领进门 人无远虑，必有近忧 沟通方式 沟通链路\n在国内大部分互联网公司不会需要你有非常强悍的专业技能，但如果想把事情做好那必不可少的就是沟通能力，沟通技能不像编码那样有明确的测量指标，但这并不代表没有沟通模板。下面这是工作中通用的沟通链路模板，分别由主体、客体以及相关方构成，在工作中我们常常扮演其中的一个角色\n主体：事件的发起者，工作中常见的例如：你的直接上级给你派活、某些专项的事情需要你支持、产品/PM等\n客体：主体命令的执行者，工作中常见的例如：你、你派发任务的那个人等\n相关方：所有事件推进过程中需要配合的人员，例如：事件变更会影响到的人员、事件推进中需要配合你们的人员等\n理清这三个角色后，接下来让我们看看正常一个事件的完整流程，事件假设为：你们的大数据集群下个月需要扩容\n你的上级将这个任务派发给你，指定下个月要进行完成 收到信息跟上级确认细节以及相关事项，同时给上级承诺完成 梳理出本次变更中会影响到的下游使用方、运维同学等等，并同步要变更的信息，同时要明确每个相关方都收到通知 在相关方响应后，要跟他们确定需要他们做操作的事件，例如需要运维同学在两周后提前扩好机器备用等并推进 在跟相关方都梳理清楚后跟你的上级进行同步，如果梳理的时间比较长可以每隔一段时间进行同步 你的上级在确认好后，如果本次事件的影响是比较大的话。应该跟相关方或者相关方的上级在check下 相关方对这件事情做二次确认，例如邮件、群消息或者文档跟进 以上基本就是一个事件发生的完整链路，不复杂但是工作中往往会忘记，浓缩成一句心得就是，“事事有着落，件件有着落”\n何时汇报\n大致分为以下三种情况\n某一方信息有变动时\n当你知道的任何一方包括你自己存在信息变动时，可以考虑进行汇报。例如你下个月突然有事要请假，那么你应该第一时间跟你的上级沟通，看看这个事情能够前置或者后置，或者能够换个人来接替这件事情等\n定期汇报\n例如在周会、日例会上进行定期进展同步，如果这件事比较重要可以单独拉个专项，针对这个专项定期进行开会对进展\n存在风险时\n当你识别到可能存在风险时，例如要运维扩机器，但是跟第三方的商务合约还没谈拢导致无法按期分配机器，那么这样的风险就应该第一时间同步给你的上级以及相关方\n风险汇报\n风险汇报务必要做到快、准、客观。快就是第一时间要立马同步，准是要先快速确认下信息的准确性，客观就是风险跟你相关或者由你引起的时候，不要担心担责或者被责备，应该保持客观的心态去汇报这个事情。一定要谨记一件心得，就是鼓足干劲把事件做好才是最重要的，其他的都是其次。\n心态方面 皮实\n简单来说遇到困难、责备、复盘的时候，要耐C。无论生活工作把你摁在地上摩擦成什么样，你站起来继续做你该做的事情。你要明确自己来这里的目的是什么，为钱、为成长、为实现自我价值等，你要牢记这一点，工作只是辅助工具，辅助你实现你的目的。因此即便工作一时做不好或者被责备，不要觉得天塌了，其实真的没有那么严重，你但凡把自己人生的时间线拉长一些你都会发现，过去某个时间段你觉得非常重要的事情在现在看到是不是基本都是风轻云淡了？例如期末考试考砸了、打游戏被父母抓到、失恋、足球比赛失误等等等等，哪一件不是当时觉得非常重要甚至世界崩了，但是回首都轻的像是一片羽毛，像是别人的故事一般，就像苏东坡的那句“回首向来萧瑟处，也无风雨也无晴”。因此牢记自己的初心，你一定可以越来越耐C～\n不皮实可能可能会给你带来的缺点\n由于害怕责备，重要的事情一拖再拖，遇到风险迟迟不敢汇报 由于害怕别人指出自己的缺点，会议上迟迟不敢发表自己的想法 工作复盘几次后，自己觉得世界塌了，工作干脆开始摆烂 情绪变的暴躁，会觉得是全世界的错，看谁都不爽，路边的狗都想上去扇两个耳光～ 除了以上种种还有很多，就像那句话说的一样 “勇敢的人先享受世界～”，你不妨大胆些去试试，换句话说，这个世界其实没有太多人关注你，你为啥不follwer自己的内心，做自己想做的事情呢\n客观\n这个词可能不是很准确的表达，大致意思就是我们在看待很多跟自己相关的事情的时候要尽量客观，例如自己的项目搞砸了，自己可能存在的那部分就是做得不好，没有什么好狡辩的，这里指的狡辩不是对任何人，而是对你自己，你自己心里要清楚自己不足的地方以及积极改进，否则每次事情做不好都怨这怨那的最终自己没有得到任何的提升\n师傅领进门\n这个也可以理解为独立做事能力，很多事情不要过多依赖别人，例如排查到一个跟数据库相关的问题，不要直接丢给dba，要自己检查下SQL写的是否有问题，连接mysql的参数是否有误等等，如果遇到事情就要依赖别人，不仅自己得不到成长，还会退化，最后任何事情都要别人手把手指导，这恐怕也不是你想成为的那个人吧？如果遇到事情自己积极主动处理，会让你不断的扩充自己的知识，提升自己的做事能力，形成一个良性循环\n人无远虑，必有近忧\n我们工作中经常会遇到各种各样的问题，因此我们应该透过现象看本质，知道本质的问题是什么后尽可能的从本质上处理了。例如你负责的A项目的某个模块经常出问题，你排查后发现代码设计一塌糊涂，那么与其不停的话时间不断的修bug，那为啥不直接抽时间梳理清楚逻辑然后重构下呢？头疼医头脚疼医脚永远不会健康，很多时候做事情眼光也要放长远些\n写在最后 本篇写给自己以及所有可能跟我一样，有时候过多专注于编码，忽略了一些软技能导致工作没有达成预期的朋友，同时也感谢一路上给我提供技术指导、做事指导的朋友们\n","date":"2024-01-25T15:09:13+08:00","image":"https://sherlock-lin.github.io/p/%E5%B7%A5%E4%BD%9C%E8%BD%AF%E6%8A%80%E8%83%BD%E7%AC%AC%E4%B8%80%E5%BC%B9%E5%85%B3%E4%BA%8E%E8%81%8C%E5%9C%BA%E6%B2%9F%E9%80%9A%E6%88%90%E9%95%BF%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/image-20240920151741705_hu3661236691741607602.png","permalink":"https://sherlock-lin.github.io/p/%E5%B7%A5%E4%BD%9C%E8%BD%AF%E6%8A%80%E8%83%BD%E7%AC%AC%E4%B8%80%E5%BC%B9%E5%85%B3%E4%BA%8E%E8%81%8C%E5%9C%BA%E6%B2%9F%E9%80%9A%E6%88%90%E9%95%BF%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/","title":"工作软技能第一弹，关于职场沟通、成长的那些事"},{"content":"在各种武侠文化的渲染下，我从小萌生了一种奇怪的想法，就是弄任何事都要偷偷摸摸的钻研，最后惊艳所有人；因此无论是大学还是毕业工作中，很多事情都希望做到“完美”再同步给“外界”，如以下几个例子\n学习SpringMVC，想说等学透了再输出文章博客 做某项任务，想说等拿到自己觉得合格的成果再同步给上级 做技术分享，由于想做得好一些，既要..又要..还要..等等，结果导致分享无限 \u0026hellip; 但是以上种种最终都没有达到很好的效果，如何算学透？你永远都不会彻底准备好的，就像董宇辉说的，给你三年你准备不好高考，就算给你三十年也准备不好的，最好的方式是边学边输出博客，通过输出来反思自己所学的内容；拿到任务要定期汇报给上级，我们所生活的世界是一个错综复杂的体系，也许过了几天这个任务已经变得“不重要”了，你如果早点与上级汇报与沟通就会早点把更多精力投入到更有用的任务上，或者说在得到上级的帮助，你可以更加快速的攻克这个任务，再不济你也要让上级知道这些事情的进展等。以上种种思考的模式是一种单向的方式，个人认为是不太健康的，理想的情况下应该是一个螺旋环状的结构，如下图 通过上图可以看到，从开始到结束那么一长条线都是一条孤独的路，如果只是自己埋头从头到尾去做一件事很有可能是枯燥、痛苦的事，这会导致很多人中途放弃，这也是上面几个例子失败的最主要根因；因此再来看第二种方式，在有目标之后的第一件事是先将其拆分为独立的一个个小任务，每攻克一个小点时，我们都可以适当做下输出，无论是输出技术博客、还是技术分享或者是其他方式，在接受到“鲜花与掌声”后我们会拥有更多的动力对下一个“堡垒”发起进攻，在接受“鲜花与掌声”的同时，我们也能听到更多的反馈，例如可能会评论说你对IOC的某个理解是错误的、领导觉得你的这个方案一可能存在某个缺陷等等，那么在这个基础下，你在开启下一个环是有个更多的“注意事项”，这能让你更有条不紊的朝着最终目标前行。\n除了大目标要进行拆分，我们也要重视外界反馈的“作用”，曾经我觉得别人的看法不重要，等自己把大目标做好了大家对你的看法自然而然的就会变好。这些也没错但是大部分人都是普通人，都是“俗人”，被人表扬会开心，收到点赞会兴奋，因此我们要合理的应用自己的“人性”的这个利器。不用害怕暴露自己，收到正向反馈就把它当作是“燃料”继续驱动自己朝着目标方向飞行，收到负面反馈就适当反思有没有需要调整的，如果不是就不用太过在意。下面是我绘制的“个体”与“外界”的关系图\n封闭世界指的是我们每个人的内心世界，而无限宇宙指的是除了自身以外的一切事物。 封闭世界 优点\n这个世界的任何东西只要你想可以随便改动 当进入心流状态时，你大脑会运转得飞快，一些困难点可以快速攻克以及快速弄明白很多知识点 缺点\n孤独 偏执 无限宇宙 优点\n反馈 缺点\n嘈杂 过多无法改变的事物 通过简单比较并不是想表达哪个更优秀，而是我们个体应该跟这个世界“链接”起来，借用“无限宇宙”中强大的反馈能力驱动“封闭世界”，消除由于孤独带来的执行力低下，修正由于个体视角所带来的偏执，同时再利用好“封闭世界”的优点。可以让我们就像驱使一辆无限燃料的飞船，有条不紊的朝着我们梦想的星球飞去～\n","date":"2024-01-10T15:09:16+08:00","image":"https://sherlock-lin.github.io/p/%E4%BB%B0%E6%9C%9B%E6%98%9F%E7%A9%BA%E4%B9%9F%E8%A6%81%E9%B2%9C%E8%8A%B1%E4%B8%8E%E6%8E%8C%E5%A3%B0/image-20240109182221111-4795743_hu1133924793650014166.png","permalink":"https://sherlock-lin.github.io/p/%E4%BB%B0%E6%9C%9B%E6%98%9F%E7%A9%BA%E4%B9%9F%E8%A6%81%E9%B2%9C%E8%8A%B1%E4%B8%8E%E6%8E%8C%E5%A3%B0/","title":"仰望星空，也要鲜花与掌声"},{"content":"概述 这几年在迷茫中看了不少资料，有觉得写得很棒的，也有写的很糟糕的。所以一直想写这块的总结来进行归纳，同时也希望能给其他处于迷茫中的朋友提供一份高质量的资料列表(也许一个读者也没有)，以下清单个人觉得值得反复看以及思考\n关于学习这件事 博主觉得自己对学习的热情最高的时候恰恰是在小学二三年级左右，入学的第一天，大家都坐在座位上，眼巴巴的看着隔壁班的同学领着书过去，迫不及待的等着老师发新课本。在听到老师点到自己名字时会兴奋的跑到讲台，然后激动的抱着属于自己的一垒书📚在其他人的目光下像个凯旋的大将军，回到座位上后像猎豹贪婪的吸着自己的猎物一般闻着新书的味道，然后一页页翻开新书，看着上面讲阅的新的故事，以及有趣好玩的插画，仿佛自己也置身其中，等回过神来已经是下课铃了。但随着年纪的增长以及应试教育的背景下，学习逐渐成为了一件功利心很强的事情，学习—\u0026gt; 分数—\u0026gt; 学府—\u0026gt; good 公司—\u0026gt; money。这条链路很直观也很现实，特别是对一些家境不太好的人来说是一条“必须”走的路。步入社会后，通过几年逐步熟悉工作内容后，慢慢的进入了所谓的舒适圈。每天日复一日做着重复的工作，当然也赚到了一些钱，就认为这条链路已经到达尽头了这辈子都不需要学习了，就像一个工具感觉用不到了就直接丢弃。博主写这篇博客目的并不是劝学，仅仅单纯从自己的视角做些分析，孰对孰错不重要，重要的是在分析或者讨论后对咱们自身的思维想法有什么作用不是吗？\n关于如何学习这件事 在刚开始学习时，本人以为学习是有捷径的，但在经过投机取巧被现实按在地上摩擦后并被告知学习是没有捷径的，唯一的捷径就是努力。之后就各种无脑卷，死记硬背，各种压榨时间投入到里面。在经过多年后，现在博主想说，学习，tmd是真的有捷径的！\n上面这张图就是我总结的四步，且构成循环\n提升效率 效率这块分为工作和学习两方面；如果你已经在工作了，务必先提升工作这方面的效率，这样在完成工作后你才能腾出足够多的时间精力来提升学习方面的效率\n工作效率的提升\n在聊工作效率之前你要想明白工作的本质是什么，无他，就是要解决你上级的问题(包括你上级自己可能都没意识到的)，各行各业皆是如此包括你上级，他一样也要给他的上级解决问题。那么问题来了，如果你帮你上级解决了重要的问题，那么如果有升职加薪的机会他会不会优先考虑你呢。方向对了之后，下一步呢，如果你是一个产品的开发，那么你就应该对这个产品足够了解以及了解业内的玩法。这样在产品提出一些耗时并且不合理的设计时，你是不是就能合理的拒绝或裁剪呢；如果一定要做，那么是否可以反馈这个成本并且加入自己的一些想法让产品做trade off呢；其次，在具体的日常工作时，咱们是不是可以利用好自己程序员的身份，将一些繁琐的工作进行自动化/半自动化呢。最后要强调的一点也就是很多程序员容易忽略的，就是务必要搞好人际关系，这对工作的顺利展开的帮助是非常大的也能避免一些无效内耗。随着现在经济这个情况，各个公司都在强调降本增效，其实博主觉得，最应该降本增效的是咱们开发，这里博主推荐的是《程序员修炼之道》以及极客时间的《10x程序员工作法》\n学习效率的提升\n学习效率这块可以参考下面“关于卡bug的这件事”的内容，这里就不详细展开\n培养好习惯 好习惯这块我觉得《认知觉醒》中就总结得很好，在微信读书就能看，博主在这里就不板门弄斧了。博主就分享两个自己常用的两点，第一个是“只做一个”，比如要坚持做俯卧撑，那么每天只坚持做一个就算完成目标，如果要多做就当附赠的；另一点就是“心理暗示”，对于要培养的习惯可以适当的心理暗示，比如要坚持练小提琴，可以心理暗示自己“我比谁都热爱小提琴”、“我是小提琴专业选手”等等，这样频繁暗示之后等再去做的时候就觉得这一切都是理所应当的\n开阔眼界、提升思维 这一点应该是最让博主感慨并且后怕的，在这些年的职业生涯中接触过很多同事，有优秀到让博主想献上膝盖的技术大佬如于某前辈，也有不了解技术但是思维很清晰，分析问题能入目三分的产品前辈；但其中也有不少的同事的眼界和思维让博主感到有些惊讶。有个工作将近十年的测试前辈，经常一个小问题要反复跟你掰扯二三十分钟ta才明白，思维处于一种混沌的状态。也有一些同事一件事情给你描述半天你都无法知道ta表达什么；更有个别同事工作多年不知道倒排索引、分布式等知识。博主提这些并不是要指责任何人，在生活中他们都是很好的人并且相处得也很愉快。但是博主在这里提出来主要是想表达 “有道无术，术可求；有术无道止于术” 这个观点，如果做技术知识混口饭吃那无所谓，但如果你是一个对技术有追求的热血青年，到老了忙碌一辈子以为是技术的东西原来只是皮毛，此时醒悟会不会有些悲凉。这也是博主自己后怕的点，具体的解法可以看下陈老师的 别让自己“墙”了自己\n成事，并构成循环 成事简单来说就是将一件事情做好，哪怕是一件小事。这点是博主四个里面做的最烂的，这也直接导致了博主颠簸的职业生涯。无论上级给你交代什么事情，务必要事事有着落，事事有回响，哪怕是做不了也要第一时间同步。在职场中，你的绩效不会跟你的技术挂钩，只会跟你的成事呈正相关。借用易经的话“天行健，君子以自强不息；地势坤，君子以厚德载物”，无论你技术再牛，如果上级给你派的活你总是做得很糟糕让他不省心，那想必他以后也不敢再派重要的任务给你并且也不会给你太好看的绩效。因此在职场中德行是非常重要的，这里面就包括上级对你的信任。通过不断成事来增强上级对你的信心、依赖，你会获得更多丰厚的资源(除了工资还包括能让你提升打磨技术的任务等)。在不断积累这些经验后，距离下一个跃迁也是很快的一件事。在这里推荐《冯唐成事心法》以及《跃迁: 成为高手的技术》\n关于卡bug的这件事 如果你要问我这个世界能不能卡bug，我会毫不犹豫的告诉你，能。大致举几点\nps：博主对卡bug的定义是，如果别人学习一个东西要十天，你通过这个方式只要五天那就是；如果别人成为专业的要8年，你通过这个方式只要5年甚至更少，那就是。\n增强回路 这个词是在《认知觉醒》中接触到的，简单来说就是人体本身是存在一定的机制，在你不断的刻意训练某项能力，它会不停的增强，甚至有时候增强的幅度是跃迁级别的。那如果咱们将这个机制用在学习某些专业能力很强的技能点上，是不是可以获得意向不到的结果？\n也许有读者要开喷了，这不是废话吗，我不断的付出肯定是不断的有收获啊~\n但请注意，博主这里想强调的是增长的幅度，可能不是平时大家所想的线性增长，它有时候可能是呈现爆炸式的跳跃增长。以前有看过故事，就一个打铁匠日复一日的打铁过去了好多年，突然有一天他悟了然后就有了绝世神功(不知道编剧是怎么想的哈哈哈)。不过博主有比较深的体会，博主刚毕业那会坚持阅读源码，一开始晦涩难懂，虽然中途也断过几次，但幸运的是一直都坚持过来了。就在毕业的第三年那样突然就悟了，对源码不再恐惧。哪怕是一个全新的从没看过的大数据新组件，也能很快的通过看源码理清其内部原理。后来跳槽的时候大概很快上手了一个稍微复杂的项目，上级略为惊讶，殊不知这纯纯的降维打击。\n自己也仔细在这方面做过总结，并不是什么玄学。其实在我们不停的学某个领域的新东西时，咱们自身会对其进行抽象归纳。在咱们吸收这个抽象模型后，再去学这个领域的新东西时，会自动的用上这个抽象模型，那速度相比新学一个东西肯定是要快上很多。\n举个简单的例子，如果现在再让博主去学一个新的组件。博主会分三步，第一步阅读官方文档/博客，主要看都有哪些功能以及机制；第二步自己搭建起来玩一下各个功能顺带验证巩固第一步的知识；第三步就是翻看源码看看功能、机制是怎么实现的，都有哪些优缺点等等。这三板斧下去不敢说精通但至少说熟练还是没什么大问题的。\n借用周总理的话，不会咱们可以学嘛，即便再难借用好增强回路机制一定也能攻克\n复利 这个词是投资里面经常提到的词，比如每年投资赚10%，如果想要资产翻倍并不需要10年，而是八年。这就是复利的作用\n那在学习生涯中是否也可以利用复利的能力呢，仔细去推算复利增长的核心就会知道，除了本金更重要的就是时间，时间可以放大很多东西，除了咱们的资产，还可以放大咱们得学识。并且更重要的就是要坚持，在复利中如果断断续续的投资效果会大打折扣，学习也是如此。更何况还要考虑到《跃迁: 成为高手的技术》中所说的资源倾斜问题，那收益就会损失得更多。因此学习最重要的就是要坚持，越到后期效果会越加显著，相信到时候的收益一定不会让你失望\n元认知 这个词也是在《认知觉醒》中接触到的，形象一点说就是分裂出一个自己像蜘蛛侠一样粘在天花板上俯视一切，这个分裂出来的自己只做一件事情，就是观测下面发生的所有事情，包括宿主的一举一动，一言一行以及思考的内容。你的灵魂在这两幅躯壳中来回进行切换，你依然像平时一样改干嘛干嘛。只不过每隔十多分钟左右都要有意的切换到\u0026quot;蜘蛛侠\u0026quot;上进行观测。这样做的目的是啥嘞，其实就是为了自我纠正，自我修复。比如说你虽然坐在书桌前拿着本书，但是可能你的思绪已经故国神游了，在你切换灵魂后就能看到宿主躯壳这幅傻模样并且切换回去后专注回来继续看书；比如说你地铁上遇到素质差的人让你情绪暴躁想冲上去跟他打一架，在你切换灵魂后就可以清晰冷静的分析 双方的体型差距、是否有摄像头、除了动手是否还有其他更优解等等；比如说在给大家讲东西的时候，适当的切换灵魂看看宿主现在讲话语速是否过快，是否简单的知识复杂化等等来进行自我调整。\n简单来说就是调整自身的状态提高做事情的效率以及获得更好的结果。当然以上只是博主对这个概念的见解，感兴趣的读者可以阅读原著\n读书飞轮 两年前在学习“学习方法”时刷到这个： 书魔的学习方法-1:读书的飞轮 ，当时的第一个感觉就是相见恨晚，即便是现在再看也一样会很激动。这对博主这种具有严重的拖延症简直就是一剂强心剂，博主学习做事经常会受困于想太多，这个东西学了会不会以后永不上，这本书和那本书应该看哪一本，钻研这个事情还是留到某个长假再学等等。但这篇博客指导你，别怕，大胆去学，不仅要学还要飞快的学。时间会自动过滤掉用不上的，你最重要的是要学起来，保持学习的劲头(是不是对应上回路增强和复利了)。但你保持学习的劲头后，你会发现东西越学越快，同样的时间你能看完更多的书，弄懂更多知识。\n心流 这个词是在《心流》书籍里接触的，其实就是进入某种状态，在这个状态中你全身心的投入的当前的这个事情，此时的效率是最高的。\n相信很多人都有过这样的感觉，博主印象最深的就是高中做数学题的时候，戴上耳机摒弃外界一切干扰。刷题刷着刷着仿佛进入了一个空白的世界，在这个世界中你可以通过意念绘制立体图形，然后再通过意念补足辅助线，再在空气中列出公式求导得出值，ok，清空世界再开始下一题。此时我感觉不到你我他以及外部一切，大脑就像一台高速运转的cpu一样，导入题后快速预设好的逻辑对数据进行加工处理然后输入，如果处理不来就查阅外部存储(书籍)进行分析并自我完善。等反应过来已经是天黑了，虽然觉得时间过得很快，但同时也觉得自己弄懂了不少东西。\n在工作中遇到一些难题时，我也常常使用这种方式，找个安静的会议室或者回家路上，进入这个自我世界，清空一切杂念，然后输入已知信息，并不断地快速推导可行的解决方案，这个方式替我解决了不少问题。如果繁杂的工作环境下没法得出答案，可以试试这个方式。\n当然这个方式更多的可以应用在学习上，尽量避免过多的干扰物。可以考虑去图书馆或者一些自习室沉浸式的学习，当你进入这个状态时学习的速度和深度往往会远超平时的状态。\n话又说回来，你说为啥古时候或者武侠小说里动不动就去山洞里修炼，要我说很大程度上就是要进入心流状态来达到高效的结果。王守仁在龙场悟道我相信也是在心流的基础上，也许他所进入的那个世界真的有老子、孔夫子、朱熹等等跟他聚在一块探讨世界的奥秘。不过这些就扯远了，但对于咱们普通人来说，合理利用心流一定是学习路上的一大杀器。\n注意事项 切记陷入自我满足的学习，如果你重读书轻技能、重输入轻输出的话很有可能你的学习是不被这个世界认可并无法获得对应的“酬劳” 切记不要闭门造车，很有可能你闷头做了几年的东西再业内已经有了很好的产品或者说方向有可能是夕阳方向，相信我这不值得，你的热血值得挥洒在于你而言更有价值的东西并且你的努力理应获得更多的回报。可以试试加一些技术交流群、拥抱开源社区、以及看一些高质量的技术网站如：medium.com、InfoQ等，且尽量用google来进行知识的探索 切记学东西浅尝辄止，学任何东西都不要仅仅停留在皮毛，那样只会浪费你的大好时光以及消磨你的技术热情。学任何东西都分 道术器三个层次，对于你热爱的东西理应不断打磨，不断地探索下一个层次。要时不时将灵魂跳出肉身，俯视当前的这个人思维所处的环境是那个级别，是否过于死扣细节而忽略原理等。相信你跟我一样，对于大部分粗糙看的东西都忘得差不多了，但你一定清晰得记得，在你苦思冥想分析的那个bug的根因是什么，在你挑灯夜读所探索某个技术的原理是什么，这些哪怕到了退休都会清楚的记得。因为这是你在脑海里反复推敲过的东西，这些东西才是真正属于你的。而外面熙熙攘攘的那些所谓看了很多的知识其实并不属于你，它们与你而言只是匆匆赶路时的一抹景色，并不是你心中的东西。因此学习务必要有深度 切勿拖延，完美主义是成事的天敌。像博主之前每件事都想做到极致，每个功能都想彻底开发好再上线，每个优化都想优化到极致再上线等，这些在职场上都是致命的，要学会拥抱瑕疵。要知道对于公司什么最重要，功能可否迭代发布，先上线用户急迫需要的那部分开放出去，在慢慢开发剩下的？优化是否可以优化提升最显著的那部分，剩下的在慢慢搞？你学习可以没有输出或者晚点输出，但是公司不行，归根结底公司是个营利组织，是以结果为导向的。想起大气帝国崛起中嬴驷对嬴华说的 “何为战之本？曰，为国取利益”；那么何为 工作之本？曰，为公司取利。即便你的设计、代码不完美，只要能解决公司的问题那么就可以先上线，进一步优化的事对公司来说至少没那么紧急重要。先解决问题，先成事才是头等大事。 对于未来我想说的 对于未来，我想说持续保持热爱，不忘初心，就像二三年级时候拿到课本那般热情去学习新的知识。在这里安利下此专访，每当我偶尔迷茫时都会翻来看，时刻提醒自己，只要这是你热爱的东西就去做，当做一辈子的事情那样去做 专访“MySQL 之父”：我曾创造 MySQL，也将颠覆 MySQL 。至于路怎么走继续参考陈皓老师的 如何超过大多数人 以及技术人员的发展之路。\n总结 看完的读者会发现，博主通篇没有给任何东西定义对错。应试教育、功利心强、毕业了赚到钱不学习、投机取巧走捷径、只读书不转化为技能、不看完这篇博客或者否定这篇博客中所有的观点，可能都是“对”的。但所谓的对错真的重要吗？人生是一场求仁得仁的路程，你追求的是什么并且最终得到相信你的内心一定是富足的(前提是你以为你要的真的是你内心真正的追求)。有人追求金钱、有人追求名望、也有人心怀天下等等都没错，写这篇博客只是想给赶路的你我整理一篇“章法”。这个世界是符合一定规律的，你我置身其中岂能外乎？因此一定是有某个子规律，在符合它时能放大你努力的收益，这里所说的“章法”就是这个子规律。在“章法”这块其实已经有很多优秀的资料了，博主写得算不上好，但即便如此博主还是要写，借用一句诗来表达所想，“他强任他强，清风拂山岗”。最后，祝你我在人生的最终阶段，都能不留遗憾。爱你所爱，行你所行，听从你心，无问西东！\n参考博客/书籍 书魔的学习方法-1:读书的飞轮\n别让自己“墙”了自己\n程序员如何把控自己的职业\n如何超过大多数人\n技术人员的发展之路\n专访“MySQL 之父”：我曾创造 MySQL，也将颠覆 MySQL\n《认知觉醒》\n《认知驱动》\n《冯唐成事心法》\n《曾国藩家书》\n《跃迁: 成为高手的技术》\n","date":"2024-01-05T15:09:13+08:00","image":"https://sherlock-lin.github.io/p/%E5%AD%A6%E4%B9%A0%E5%BD%95/the-creative-exchange-d2zvqp3fpro-unsplash_hu5876398126655421130.jpg","permalink":"https://sherlock-lin.github.io/p/%E5%AD%A6%E4%B9%A0%E5%BD%95/","title":"学习录"},{"content":"引言小故事 张三在一家小型互联网公司上班，由于公司实行的996，因此经常有同事“不辞而别”，为了工作的正常推进，团队内达成了某种默契，这种默契就是通过某个规则来选出一个同事，这个同事除了工作之余还有额外看看每天是否有同事“不辞而别”，当发现有同事李四离职时，就会去把李四负责的工作的内容进行拆分给其他的同事进行处理。整个过程大致如下图\n由上图可以看到这个公司通过一个签到本和工作进度表来完成整个流程，每个同事上班时都要在签到本上进行签到，每天下班前要在工作进度表上同步今天的工作进展；例如今天李四“不辞而别”溜了，张三在签到本上看到李四没有签到记录，就判定这家伙不干了同时在工作进度表中把李四的任务进行拆分给大狗和二狗来做\u0026hellip;\n通过上面的故事会发现有几个问题\n张三是通过什么规则被选成“监督者”的？ 如果张三也不辞而别呢？ 为啥要通过签到本的方式，而不是张三直接去挨个挨个看？ \u0026hellip;. 咱们可以带着这些种种心里的疑惑看下面的文章，这个故事其实是一个分布式存储组件的雏形，刚刚所讨论的那些问题也是这些组件所会遇到的且大部分都是有解法的，所以咱们接下来就来看看bookkeeper这个分布式存储组件是如何解决上述问题的\nbookkeeper基础 “硬件无法保证不故障”，在这个大前提下，所有运行在硬件上的存储组件都一定会做一件很重要的事情，这件事就是数据恢复，要么是在组件内部来做，要么是在组件外部来做。\nbk是一个具有容错的分布式存储组件，同一份数据会有多个副本，分别存在多个bookie中来提供容错保证，那么当一台bookie不可用时，其上面保存的数据都少了一个副本，如果不进行数据恢复/复制的话再有其他的bookie不可用就很容易造成数据的丢失。因此bk自身内部提供了数据恢复的机制，今天通篇大论都是围绕bk的这个机制进行展开的\n数据恢复一般分为手动和自动，bk同时支持这两种方式，接下来就看看具体怎么操作的\n手动恢复\n1 2 bin/bookkeeper shell recover 192.168.1.10:3181 指定bookie机器来恢复 bin/bookkeeper shell recover 192.168.1.10:3181 --ledger ledgerID 指定bookie机器上的某个ledgerId进行恢复 在执行手动恢复时，会发生以下四个步骤\n客户端从zookeeper读取Ledger信息 根据Ledger信息确定需要做数据复制的Ledger(根据Ledger中存有被哪些bookie存储的元信息来确定) 在客户端启动一个做数据恢复的进程，针对需要做数据复制的Ledger进行数据恢复 一旦所有Ledger被标记为全副本了，则恢复动作完成 自动恢复\n1 2 3 bin/bookkeeper autorecovery bookie 集群开启自动恢复机制 bin/bookkeeper shell autorecovery -disable 关闭自动恢复机制 bin/bookkeeper shell autorecovery -enable 关闭恢复后再重新开启 除了通过指令的方式启动，bk还支持配置的方式，只需要在bookie节点配置autoRecoveryDaemonEnabled为false，这个bookie节点在启动的时候也同样会启动autorecovery服务\nautorecovery机制 上一章节讲了怎么使用，本章节主要讲明autorecovery这个机制\n自动恢复机制中有两个角色Auditor和replication worker，在启动自动恢复机制后，会在每个bookie实例中启动这两个角色\nAuditor\nbk集群中的Auditor们会通过zookeeper选举产生一位leader，这个leader负责监听 zookeeper /ledgers/available 节点变化情况来判定是否要做数据恢复动作，因为所有节点启动都会注册在上面，如果有服务不可用由于zookeeper的临时目录机制，会自动删除在此目录下自己节点的信息，因此leader通过watch机制可以轻松感知到有节点不可用，当Auditor leader感知到有节点不可用时，会将此bookie所负责的所有Ledger加在zookeeper /ledgers/underreplicated 路径下，通过这种方式通知replication worker做数据恢复过程\nreplication worker\n每个replication worker都会监听 /ledgers/underreplicated 地址，在监听到有数据恢复任务时，会在 /ledgers/underreplication/locks下添加锁从而避免并发问题；如果在开始恢复前发下当前Ledger的Fragment还处于写入中的状态，replication worker会先尝试等待它写完再做数据恢复动作，但如果等了一段时间还没写完会通过Fence机制处理再做复制，同时开启一个新的Fragment给客户端做数据的继续写入\n启动工作流程\n参照上图，在服务器节点上执行bin/bookkeeper autorecovery bookie后会发生以下步骤\n通过exec shell指令调用操作系统拉起AutoRecoveryMain 这个Java进程 AutoRecoveryMain进程启动时会同时启动Auditor线程和ReplicationWorker线程，由于环境中可能会启动多个AutoRecoveryMain进程来做HA高可用，因此多个Auditor线程会通过zookeeper选举来产生一个Auditor Leader 由于bookie集群用zookeeper来做集群感知，因此Auditor Leader只需要通过watch监听zookeeper上 bookie所注册的地址就能感知到是否有bookie节点不可用；当bookie节点不可用时一般就不会上报心跳给zookeeper，zookeeper就会将该节点创建的临时目录进行删除并告知添加watch的Auditor Leader Auditor Leader收到通知后会去zookeeper查询该不可用bookie所负责的Ledger列表，理论上这些Ledger都是需要做数据恢复的，因此会将它们放在zookeeper的/ledgers/underreplicated 目录下来通知ReplicationWorker ReplicationWorker通过watch监听到此目录有需要做数据恢复的Ledger后，会先在zk加锁再进行数据恢复逻辑；通过将Ledger划分为多个Fragment来轮训进行数据恢复，通过读取其他正常bookie上该Ledger的数据并写到其他没有该数据的bookie的节点上从而保证每份数据都有多个副本，直到将/ledgers/underreplicated 下的所有Ledger进行复制完，本次 autorecovery就算完成了。而Auditor线程和ReplicationWorker线程会不停的监听zookeeper直到下一个bookie节点不可用 通过此机制给bookkeeper提高了稳定性以及高可用能力，在有个别节点挂掉的时候依然能自动做到数据完备不丢，这种设计是一个成熟的组件该具备的能力\nautorecovery启动源码 源码主要分 启动流程以及工作流程进行讲解，同时在这里给需要阅读的朋友提供一个可能会用上的“词典”\n1 2 3 4 5 6 7 8 9 10 AutoRecoveryMain核心类, 主要负责启动AutoRecovery服务 AutoRecoveryService核心类，主要负责AutoRecovery相关的服务 LedgerManager 对外提供一个管理ledger的api，对内负责如何将ledger的元数据存储在kv存储上。提供增删、读写、注册/注销六个核心接口 AbstractZkLedgerManager 抽象类 LedgerIdGenerator：基于zk实现全局唯一递增的ledgerId ZkLedgerUnderreplicationManager：管理未完成复制的Ledger ZkLedgerAuditorManager：管理Auditor ReplicationWorker：负责从ZkLedgerUnderreplicationManager中获取未完成复制的Ledger并进行复制，每隔rwRereplicateBackoffMs触发一次 LedgerFragment：组成Ledger的单元，也是恢复复制的单元 EmbeddedServer：启动bk实例的节点 从现在开始跟踪启动的源码，在客户端执行 bin/bookkeeper autorecovery bookie 后会走到 bookkeeper/bin/bookkeeper 这个脚本下面的这行逻辑\n1 2 if [ ${COMMAND} == \u0026#34;autorecovery\u0026#34; ]; then exec \u0026#34;${JAVA}\u0026#34; ${OPTS} ${JMX_ARGS} org.apache.bookkeeper.replication.AutoRecoveryMain --conf ${BOOKIE_CONF} $@ 逻辑非常清晰，其实就是通过shell启动AutoRecovery 这样一个独立的Java进程，专门负责做故障数据恢复。JVM会从启动类的main方法进行引导执行，因此咱们接下来从AutoRecoveryMain的main方法作为入口来看看后面会发生哪些事情\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 public static void main(String[] args) { //调用真正执行的方法，开源项目中真正执行某个操作会以do前缀来进行修饰 int retCode = doMain(args); .... } static int doMain(String[] args) { ServerConfiguration conf; try { //根据shell启动命令中指定的配置地址加载成配置对象 conf = parseArgs(args); } catch (IllegalArgumentException iae) { .... } LifecycleComponent server; try { //构建AutoRecoveryServer对象，比较重要的方法 server = buildAutoRecoveryServer(new BookieConfiguration(conf)); } catch (Exception e) { .... } try { //启动AutoRecoveryServer对象 ComponentStarter.startComponent(server).get(); } catch (InterruptedException ie) { .... } return ExitCode.OK; } 通过这里可以发现AutoRecoveryMain的main方法只是做一个引导的动作，最终启动的是AutoRecoveryServer对象。因此让我们深入看看这个服务的构造以及启动的流程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public static LifecycleComponentStack buildAutoRecoveryServer(BookieConfiguration conf) throws Exception { LifecycleComponentStack.Builder serverBuilder = LifecycleComponentStack.newBuilder() .withName(\u0026#34;autorecovery-server\u0026#34;); // 1. 创建StatsProviderService对象，主要用来记录AutoRecovery服务的各项指标状态 StatsProviderService statsProviderService = new StatsProviderService(conf); .... // 2. 通过构造函数的方式创建AutoRecoveryService对象，这是核心的代码 AutoRecoveryService autoRecoveryService = new AutoRecoveryService(conf, rootStatsLogger); .... // 3. 创建BKHttpServiceProvider对象，主要用来对外提供http服务，支持通过http方式读取内部状态信息等 if (conf.getServerConf().isHttpServerEnabled()) { BKHttpServiceProvider provider = new BKHttpServiceProvider.Builder() .setAutoRecovery(autoRecoveryService.getAutoRecoveryServer()) .setServerConfiguration(conf.getServerConf()) .setStatsProvider(statsProviderService.getStatsProvider()).build(); HttpService httpService = new HttpService(provider, conf, rootStatsLogger); .... } return serverBuilder.build(); } 再看AutoRecoveryService的构造函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 public AutoRecoveryService(BookieConfiguration conf, StatsLogger statsLogger) throws Exception { super(NAME, conf, statsLogger); //通过构造函数创建AutoRecoveryMain，AutoRecoveryMain是AutoRecoveryService的成员变量 //进入看看它的实现 this.main = new AutoRecoveryMain( conf.getServerConf(), statsLogger); } public AutoRecoveryMain(ServerConfiguration conf, StatsLogger statsLogger) throws IOException, InterruptedException, KeeperException, UnavailableException, CompatibilityException { .... //创建AuditorElector对象，负责选举产生Auditor Leader auditorElector = new AuditorElector( BookieImpl.getBookieId(conf).toString(), conf, bkc, statsLogger.scope(AUDITOR_SCOPE), false); //创建ReplicationWorker对象，负责做数据的拷贝工作 replicationWorker = new ReplicationWorker( conf, bkc, false, statsLogger.scope(REPLICATION_WORKER_SCOPE)); deathWatcher = new AutoRecoveryDeathWatcher(this); } 服务构造的逻辑差不多就跟到这了，我们知道最终是为了创建AuditorElector和ReplicationWorker这两个对象就够了。服务启动这块从上面的 ComponentStarter.startComponent(server).get(); 进行跟踪\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public static CompletableFuture\u0026lt;Void\u0026gt; startComponent(LifecycleComponent component) { .... //调用start方法，这里涉及上采用了模版方法设计模式以及闭包，本质上就是就是调用创建的 //StatsProviderService、\tAutoRecoveryService、HttpService这三个服务的doStart方法 component.start(); .... } protected void doStart() { //还是调的AutoRecoveryMain方法的start方法 this.main.start(); } public void start() { //启动auditorElector服务 auditorElector.start(); //启动replicationWorker服务 replicationWorker.start(); .... deathWatcher.start(); } 结合上面的可以发现AutoRecovery的启动本质上就是启动AuditorElector和ReplicationWorker这两个服务，因此接下来咱们就来看看这两个服务的start过程，先来看看AuditorElector\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public Future\u0026lt;?\u0026gt; start() { running.set(true); //提交选举任务 return submitElectionTask(); } Future\u0026lt;?\u0026gt; submitElectionTask() { Runnable r = new Runnable() { @Override public void run() { .... //创建一个Auditor对象并进行启动 auditor = new Auditor(bookieId, conf, bkc, false, statsLogger); auditor.start(); } }; try { //异步执行以上逻辑 return executor.submit(r); } catch (RejectedExecutionException e) { .... } } 在这里其实就是对Auditor对象进行初始化以及启动，再进一步跟踪\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public Auditor(final String bookieIdentifier, ServerConfiguration conf, BookKeeper bkc, boolean ownBkc, BookKeeperAdmin admin, boolean ownAdmin, StatsLogger statsLogger) throws UnavailableException { .... //调用初始化Auditor对象逻辑 initialize(conf, bkc); .... } private void initialize(ServerConfiguration conf, BookKeeper bkc) throws UnavailableException { try { LedgerManagerFactory ledgerManagerFactory = bkc.getLedgerManagerFactory(); ledgerManager = ledgerManagerFactory.newLedgerManager(); this.bookieLedgerIndexer = new BookieLedgerIndexer(ledgerManager); this.ledgerUnderreplicationManager = ledgerManagerFactory .newLedgerUnderreplicationManager(); .... lostBookieRecoveryDelayBeforeChange = this.ledgerUnderreplicationManager.getLostBookieRecoveryDelay(); } catch (CompatibilityException ce) { .... } } 看完了初始化逻辑，再继续看下Auditor的启动逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 public void start() { LOG.info(\u0026#34;I\u0026#39;m starting as Auditor Bookie. ID: {}\u0026#34;, bookieIdentifier); synchronized (this) { .... try { //1. 监听bookie变更事件，本质上就是在zk /ledgers/available 目录下增加watch监听节点的变动 //这里还会监听 只读bookie 节点的变动 watchBookieChanges(); //从zk获取处于可用的bk节点列表 knownBookies = getAvailableBookies(); } catch (BKException bke) { .... } try { //1. 在感知到有bookie节点不可用时回调LostBookieRecoveryDelayChangedCb进行逻辑处理 this.ledgerUnderreplicationManager .notifyLostBookieRecoveryDelayChanged(new LostBookieRecoveryDelayChangedCb()); } catch (UnavailableException ue) { .... } try { //1. 感知到有Ledger的副本少时触发，跟上面一样也是通过回调方式进行处理 this.ledgerUnderreplicationManager.notifyUnderReplicationLedgerChanged( new UnderReplicatedLedgersChangedCb()); } catch (UnavailableException ue) { .... } scheduleBookieCheckTask(); //启动一个线程检查Ledger的状态 scheduleCheckAllLedgersTask(); schedulePlacementPolicyCheckTask(); scheduleReplicasCheckTask(); } } 这些就是Auditor启动的逻辑，接下来再看看ReplicationWorker的启动逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 public void start() { //workerThread实际上就是一个BookieThread对象 this.workerThread.start(); } public void run() { workerRunning = true; while (workerRunning) { try { //核心逻辑就是循环调用rereplicate方法 if (!rereplicate()) { LOG.warn(\u0026#34;failed while replicating fragments\u0026#34;); waitBackOffTime(rwRereplicateBackoffMs); } } catch (InterruptedException e) { .... } } LOG.info(\u0026#34;ReplicationWorker exited loop!\u0026#34;); } private boolean rereplicate() throws InterruptedException, BKException, UnavailableException { //获取需要做数据恢复的Ledger long ledgerIdToReplicate = underreplicationManager .getLedgerToRereplicate(); Stopwatch stopwatch = Stopwatch.createStarted(); boolean success = false; try { //进行数据恢复 success = rereplicate(ledgerIdToReplicate); } finally { .... } return success; } autorecovery工作源码 这块由于逻辑相对较多，因此针对autorecovery工作流程单独开一章。经过上面我们可以清晰的知道在经过启动后都发生了哪些事情，接下来咱们看看autorecovery真正工作的逻辑。在Auditor start的时候，会通过监听zookeeper来感知数据的动态变化\n1 2 3 4 5 6 7 8 public void start() { //感知bookie节点下线，将这些bookie上管理的Ledger标记为需要备份放到zookeeper上 this.ledgerUnderreplicationManager .notifyLostBookieRecoveryDelayChanged(new LostBookieRecoveryDelayChangedCb()); //感知Ledger副本变动，统计到指标里 this.ledgerUnderreplicationManager.notifyUnderReplicationLedgerChanged( new UnderReplicatedLedgersChangedCb()); } 上述两个唤醒方法主要是通过watch感知zookeeper事件，所以咱们主要看回调类里面的处理逻辑，先看下LostBookieRecoveryDelayChangedCb类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 private class LostBookieRecoveryDelayChangedCb implements GenericCallback\u0026lt;Void\u0026gt; { @Override public void operationComplete(int rc, Void result) { .... Auditor.this.ledgerUnderreplicationManager .notifyLostBookieRecoveryDelayChanged(LostBookieRecoveryDelayChangedCb.this); .... //提交事件变动处理任务，进去看看 Auditor.this.submitLostBookieRecoveryDelayChangedEvent(); } } synchronized Future\u0026lt;?\u0026gt; submitLostBookieRecoveryDelayChangedEvent() { return executor.submit(() -\u0026gt; { int lostBookieRecoveryDelay = -1; try { waitIfLedgerReplicationDisabled(); lostBookieRecoveryDelay = Auditor.this.ledgerUnderreplicationManager .getLostBookieRecoveryDelay(); .... //核心逻辑,进去看看都做了些什么 auditorBookieCheckTask.startAudit(false); } else if (auditTask != null) { LOG.info(\u0026#34;lostBookieRecoveryDelay has been set to {}, so rescheduling AuditTask accordingly\u0026#34;, lostBookieRecoveryDelay); auditTask = executor.schedule(() -\u0026gt; { auditorBookieCheckTask.startAudit(false); auditTask = null; bookiesToBeAudited.clear(); }, lostBookieRecoveryDelay, TimeUnit.SECONDS); auditorStats.getNumBookieAuditsDelayed().inc(); } } catch (InterruptedException ie) { .... } finally { if (lostBookieRecoveryDelay != -1) { lostBookieRecoveryDelayBeforeChange = lostBookieRecoveryDelay; } } }); } void startAudit(boolean shutDownTask) { try { //看起来是开始做Auditor的主要任务了，继续往下 auditBookies(); shutDownTask = false; } catch (BKException bke) { .... } } void auditBookies() throws ReplicationException.BKAuditException, InterruptedException, BKException { .... List\u0026lt;String\u0026gt; availableBookies = getAvailableBookies(); // find lost bookies Set\u0026lt;String\u0026gt; knownBookies = ledgerDetails.keySet(); //通过之前内存中存的bookie集合减去 zk当前bookie集合即可得出都有哪些bookie节点不可用了 Collection\u0026lt;String\u0026gt; lostBookies = CollectionUtils.subtract(knownBookies, availableBookies); .... //如果本次变动涉及到bookie节点不可用，则调用handleLostBookiesAsync方法处理不可用的节点 if (lostBookies.size() \u0026gt; 0) { try { FutureUtils.result( handleLostBookiesAsync(lostBookies, ledgerDetails), ReplicationException.EXCEPTION_HANDLER); } catch (ReplicationException e) { .... } .... } .... } private CompletableFuture\u0026lt;?\u0026gt; handleLostBookiesAsync(Collection\u0026lt;String\u0026gt; lostBookies, Map\u0026lt;String, Set\u0026lt;Long\u0026gt;\u0026gt; ledgerDetails) { LOG.info(\u0026#34;Following are the failed bookies: {},\u0026#34; + \u0026#34; and searching its ledgers for re-replication\u0026#34;, lostBookies); return FutureUtils.processList( Lists.newArrayList(lostBookies), //看方法名大概能猜得出来计算这些bookie上的Ledger，并针对这些Ledger进行数据恢复 //由于之前有存节点跟Ledger的映射关系，因此直接通过ledgerDetails映射表来获取这些不可用节点所负责的Ledger bookieIP -\u0026gt; publishSuspectedLedgersAsync( Lists.newArrayList(bookieIP), ledgerDetails.get(bookieIP)), null ); } protected CompletableFuture\u0026lt;?\u0026gt; publishSuspectedLedgersAsync(Collection\u0026lt;String\u0026gt; missingBookies, Set\u0026lt;Long\u0026gt; ledgers) { .... LongAdder underReplicatedSize = new LongAdder(); FutureUtils.processList( Lists.newArrayList(ledgers), ledgerId -\u0026gt; //通过读取这些Ledger的元数据，方便后续的数据恢复动作 ledgerManager.readLedgerMetadata(ledgerId).whenComplete((metadata, exception) -\u0026gt; { if (exception == null) { underReplicatedSize.add(metadata.getValue().getLength()); } }), null).whenComplete((res, e) -\u0026gt; { .... }); return FutureUtils.processList( Lists.newArrayList(ledgers), //主流程，继续往下 ledgerId -\u0026gt; ledgerUnderreplicationManager.markLedgerUnderreplicatedAsync(ledgerId, missingBookies), null ); } public CompletableFuture\u0026lt;Void\u0026gt; markLedgerUnderreplicatedAsync(long ledgerId, Collection\u0026lt;String\u0026gt; missingReplicas) { .... final String znode = getUrLedgerZnode(ledgerId); //标记需要做备份的Ledger tryMarkLedgerUnderreplicatedAsync(znode, missingReplicas, zkAcls, createFuture); return createFuture; } private void tryMarkLedgerUnderreplicatedAsync(final String znode, final Collection\u0026lt;String\u0026gt; missingReplicas, final List\u0026lt;ACL\u0026gt; zkAcls, final CompletableFuture\u0026lt;Void\u0026gt; finalFuture) { .... //将需要做数据恢复的副本进行进行proto编码 missingReplicas.forEach(builder::addReplica); .... ZkUtils.asyncCreateFullPathOptimistic( zkc, znode, urLedgerData, zkAcls, CreateMode.PERSISTENT, (rc, path, ctx, name) -\u0026gt; { if (Code.OK.intValue() == rc) { FutureUtils.complete(finalFuture, null); } else if (Code.NODEEXISTS.intValue() == rc) { //要在zookeeper将这些Ledger标记为需要做数据恢复 handleLedgerUnderreplicatedAlreadyMarked(znode, missingReplicas, zkAcls, finalFuture); } else { FutureUtils.completeExceptionally(finalFuture, KeeperException.create(Code.get(rc))); } }, null); } private void handleLedgerUnderreplicatedAlreadyMarked(final String znode, final Collection\u0026lt;String\u0026gt; missingReplicas, final List\u0026lt;ACL\u0026gt; zkAcls, final CompletableFuture\u0026lt;Void\u0026gt; finalFuture) { // get the existing underreplicated ledger data zkc.getData(znode, false, (getRc, getPath, getCtx, existingUrLedgerData, getStat) -\u0026gt; { if (Code.OK.intValue() == getRc) { // deserialize existing underreplicated ledger data final UnderreplicatedLedgerFormat.Builder builder = UnderreplicatedLedgerFormat.newBuilder(); try { TextFormat.merge(new String(existingUrLedgerData, UTF_8), builder); } catch (ParseException e) { .... } UnderreplicatedLedgerFormat existingUrLedgerFormat = builder.build(); boolean replicaAdded = false; for (String missingReplica : missingReplicas) { if (existingUrLedgerFormat.getReplicaList().contains(missingReplica)) { continue; } else { builder.addReplica(missingReplica); replicaAdded = true; } } .... //盲猜这里在zk将Ledger标志为需要做数据同步 zkc.setData(znode, newUrLedgerData, getStat.getVersion(), (setRc, setPath, setCtx, setStat) -\u0026gt; { if (Code.OK.intValue() == setRc) { FutureUtils.complete(finalFuture, null); } else if (Code.NONODE.intValue() == setRc) { tryMarkLedgerUnderreplicatedAsync(znode, missingReplicas, zkAcls, finalFuture); } else if (Code.BADVERSION.intValue() == setRc) { handleLedgerUnderreplicatedAlreadyMarked(znode, missingReplicas, zkAcls, finalFuture); } else { FutureUtils.completeExceptionally(finalFuture, KeeperException.create(Code.get(setRc))); } }, null); } else if (Code.NONODE.intValue() == getRc) { tryMarkLedgerUnderreplicatedAsync(znode, missingReplicas, zkAcls, finalFuture); } else { FutureUtils.completeExceptionally(finalFuture, KeeperException.create(Code.get(getRc))); } }, null); } 从ReplicationWorker的rereplicate方法开始就是真正做数据恢复的过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 private boolean rereplicate(long ledgerIdToReplicate) throws InterruptedException, BKException, UnavailableException { .... //获取需要做数据恢复的Ledger的处理对象LedgerHandle try (LedgerHandle lh = admin.openLedgerNoRecovery(ledgerIdToReplicate)) { //通过对Ledger进行分解成更小数据恢复单位LedgerFragment，后续分别对LedgerFragment进行数据恢复 Set\u0026lt;LedgerFragment\u0026gt; fragments = getUnderreplicatedFragments(lh, conf.getAuditorLedgerVerificationPercentage()); .... for (LedgerFragment ledgerFragment : fragments) { .... try { //对LedgerFragment进行数据恢复 admin.replicateLedgerFragment(lh, ledgerFragment, onReadEntryFailureCallback); numFragsReplicated++; if (ledgerFragment.getReplicateType() == LedgerFragment .ReplicateType.DATA_NOT_ADHERING_PLACEMENT) { numNotAdheringPlacementFragsReplicated++; } } catch (BKException.BKBookieHandleNotAvailableException e) { .... } } .... fragments = getUnderreplicatedFragments(lh, conf.getAuditorLedgerVerificationPercentage()); .... } catch (BKNoSuchLedgerExistsOnMetadataServerException e) { .... } finally { .... } } 继续进一步看admin.replicateLedgerFragment的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 public void replicateLedgerFragment(LedgerHandle lh, final LedgerFragment ledgerFragment, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException, BKException { .... //继续往下跟踪 replicateLedgerFragment(lh, ledgerFragment, targetBookieAddresses, onReadEntryFailureCallback); } private void replicateLedgerFragment(LedgerHandle lh, final LedgerFragment ledgerFragment, final Map\u0026lt;Integer, BookieId\u0026gt; targetBookieAddresses, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException, BKException { .... //在这里看到这个恢复其实是异步处理的过程，继续往下 asyncRecoverLedgerFragment(lh, ledgerFragment, cb, targetBookieSet, onReadEntryFailureCallback); .... } private void asyncRecoverLedgerFragment(final LedgerHandle lh, final LedgerFragment ledgerFragment, final AsyncCallback.VoidCallback ledgerFragmentMcb, final Set\u0026lt;BookieId\u0026gt; newBookies, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException { //发现会调用LedgerFragmentReplicator对象进行数据恢复，继续往下 lfr.replicate(lh, ledgerFragment, ledgerFragmentMcb, newBookies, onReadEntryFailureCallback); } void replicate(final LedgerHandle lh, final LedgerFragment lf, final AsyncCallback.VoidCallback ledgerFragmentMcb, final Set\u0026lt;BookieId\u0026gt; targetBookieAddresses, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException { Set\u0026lt;LedgerFragment\u0026gt; partionedFragments = splitIntoSubFragments(lh, lf, bkc.getConf().getRereplicationEntryBatchSize()); .... //继续往下看实现 replicateNextBatch(lh, partionedFragments.iterator(), ledgerFragmentMcb, targetBookieAddresses, onReadEntryFailureCallback); } private void replicateNextBatch(final LedgerHandle lh, final Iterator\u0026lt;LedgerFragment\u0026gt; fragments, final AsyncCallback.VoidCallback ledgerFragmentMcb, final Set\u0026lt;BookieId\u0026gt; targetBookieAddresses, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) { if (fragments.hasNext()) { try { //来了，一般有Internal的地方都会有实现的干货，继续进去 replicateFragmentInternal(lh, fragments.next(), new AsyncCallback.VoidCallback() { @Override public void processResult(int rc, String v, Object ctx) { if (rc != BKException.Code.OK) { ledgerFragmentMcb.processResult(rc, null, null); } else { replicateNextBatch(lh, fragments, ledgerFragmentMcb, targetBookieAddresses, onReadEntryFailureCallback); } } }, targetBookieAddresses, onReadEntryFailureCallback); } catch (InterruptedException e) { ..... } } else { ledgerFragmentMcb.processResult(BKException.Code.OK, null, null); } } private void replicateFragmentInternal(final LedgerHandle lh, final LedgerFragment lf, final AsyncCallback.VoidCallback ledgerFragmentMcb, final Set\u0026lt;BookieId\u0026gt; newBookies, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException { .... //针对每个Entry对象循环做数据恢复，Entry是BK里最小的数据单元 for (final Long entryId : entriesToReplicate) { recoverLedgerFragmentEntry(entryId, lh, ledgerFragmentEntryMcb, newBookies, onReadEntryFailureCallback); } } private void recoverLedgerFragmentEntry(final Long entryId, final LedgerHandle lh, final AsyncCallback.VoidCallback ledgerFragmentEntryMcb, final Set\u0026lt;BookieId\u0026gt; newBookies, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException { .... long startReadEntryTime = MathUtils.nowInNano(); /* * Read the ledger entry using the LedgerHandle. This will allow us to * read the entry from one of the other replicated bookies other than * the dead one. */ //到了真正读取Entry的逻辑，继续往下 lh.asyncReadEntries(entryId, entryId, new ReadCallback() { .... } }, null); } public void asyncReadEntries(long firstEntry, long lastEntry, ReadCallback cb, Object ctx) { .... //调用异步读取逻辑 asyncReadEntriesInternal(firstEntry, lastEntry, cb, ctx, false); } void asyncReadEntriesInternal(long firstEntry, long lastEntry, ReadCallback cb, Object ctx, boolean isRecoveryRead) { if (!clientCtx.isClientClosed()) { //继续往下跟踪 readEntriesInternalAsync(firstEntry, lastEntry, isRecoveryRead) .whenCompleteAsync(new FutureEventListener\u0026lt;LedgerEntries\u0026gt;() { .... }, clientCtx.getMainWorkerPool().chooseThread(ledgerId)); } else { cb.readComplete(Code.ClientClosedException, LedgerHandle.this, null, ctx); } } CompletableFuture\u0026lt;LedgerEntries\u0026gt; readEntriesInternalAsync(long firstEntry, long lastEntry, boolean isRecoveryRead) { //构造读数据的对象 PendingReadOp op = new PendingReadOp(this, clientCtx, firstEntry, lastEntry, isRecoveryRead); //运行起来，跟进去瞅瞅 op.run(); return op.future(); } public void run() { //无他，继续往下 initiate(); } void initiate() { .... do { //决定是串行读取数据还是并行读取数据 if (parallelRead) { entry = new ParallelReadRequest(ensemble, lh.ledgerId, i); } else { entry = new SequenceReadRequest(ensemble, lh.ledgerId, i); } seq.add(entry); i++; } while (i \u0026lt;= endEntryId); // read the entries. for (LedgerEntryRequest entry : seq) { //核心逻辑，这里进行数据读取操作 entry.read(); } } void read() { //继续往下看 sendNextRead(); } synchronized BookieId sendNextRead() { .... try { BookieId to = ensemble.get(bookieIndex); //发送读取请求的操作 sendReadTo(bookieIndex, to, this); .... } catch (InterruptedException ie) { .... } } void sendReadTo(int bookieIndex, BookieId to, LedgerEntryRequest entry) throws InterruptedException if (isRecoveryRead) { .... } else { //调用BK客户端进行数据的读取 clientCtx.getBookieClient().readEntry(to, lh.ledgerId, entry.eId, this, new ReadContext(bookieIndex, to, entry), BookieProtocol.FLAG_NONE); } } default void readEntry(BookieId address, long ledgerId, long entryId, ReadEntryCallback cb, Object ctx, int flags) { //继续往下 readEntry(address, ledgerId, entryId, cb, ctx, flags, null); } default void readEntry(BookieId address, long ledgerId, long entryId, ReadEntryCallback cb, Object ctx, int flags, byte[] masterKey) { //继续往下 readEntry(address, ledgerId, entryId, cb, ctx, flags, masterKey, false); } public void readEntry(final BookieId addr, final long ledgerId, final long entryId, final ReadEntryCallback cb, final Object ctx, int flags, byte[] masterKey, final boolean allowFastFail) { //获取要访问的客户端对象 final PerChannelBookieClientPool client = lookupClient(addr); .... client.obtain((rc, pcbc) -\u0026gt; { if (rc != BKException.Code.OK) { completeRead(rc, ledgerId, entryId, null, cb, ctx); } else { //调用读取逻辑 pcbc.readEntry(ledgerId, entryId, cb, ctx, flags, masterKey, allowFastFail); } }, ledgerId); } public void readEntry(final long ledgerId, final long entryId, ReadEntryCallback cb, Object ctx, int flags, byte[] masterKey, boolean allowFastFail) { //看到Internal就知道要有东西了，继续往下 readEntryInternal(ledgerId, entryId, null, null, false, cb, ctx, (short) flags, masterKey, allowFastFail); } private void readEntryInternal(final long ledgerId, final long entryId, final Long previousLAC, final Long timeOutInMillis, final boolean piggyBackEntry, final ReadEntryCallback cb, final Object ctx, int flags, byte[] masterKey, boolean allowFastFail) { .... //构造请求对象 ReadRequest.Builder readBuilder = ReadRequest.newBuilder() .setLedgerId(ledgerId) .setEntryId(entryId); .... request = withRequestContext(Request.newBuilder()) .setHeader(headerBuilder) .setReadRequest(readBuilder) .build(); .... //继续往下 writeAndFlush(channel, completionKey, request, allowFastFail); } private void writeAndFlush(final Channel channel, final CompletionKey key, final Object request, final boolean allowFastFail) { .... try { .... //跟到这里就知道最终调用了Netty的客户端来发起请求 channel.writeAndFlush(request, promise); } catch (Throwable e) { .... } } 到这里数据就发出去了，我们也能知道AutoRecovery进程是通过Netty向BK的服务端进行数据读取，那么服务端在接收到请求后又是怎么处理的呢，这里咱们从服务端接收请求的逻辑开始跟，由于BK本身也是通过Netty实例进行网络请求处理的，因此可以轻松找到BookieRequestHandler的channelRead方法监听外部网络请求\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { .... //职责分离做得很好，BookieRequestHandler只负责接收请求，逻辑处理相关的全权交给requestProcessor对象 requestProcessor.processRequest(msg, this); } public void processRequest(Object msg, BookieRequestHandler requestHandler) { Channel channel = requestHandler.ctx().channel(); if (msg instanceof BookkeeperProtocol.Request) { BookkeeperProtocol.Request r = (BookkeeperProtocol.Request) msg; restoreMdcContextFromRequest(r); try { BookkeeperProtocol.BKPacketHeader header = r.getHeader(); //非常好的一种设计，kafka的KafkaApis类里也是这样设计，服务端支持的操作在这里就能很清晰的看到 switch (header.getOperation()) { case ADD_ENTRY: processAddRequestV3(r, requestHandler); break; case READ_ENTRY: //在这里可以处理的读取请求，从这里进去看看 processReadRequestV3(r, requestHandler); break; case FORCE_LEDGER: processForceLedgerRequestV3(r, requestHandler); break; .... case WRITE_LAC: processWriteLacRequestV3(r, requestHandler); break; case READ_LAC: processReadLacRequestV3(r, requestHandler); break; case GET_BOOKIE_INFO: processGetBookieInfoRequestV3(r, requestHandler); break; case START_TLS: processStartTLSRequestV3(r, requestHandler); break; case GET_LIST_OF_ENTRIES_OF_LEDGER: processGetListOfEntriesOfLedgerProcessorV3(r, requestHandler); break; default: .... break; } } finally { MDC.clear(); } } else { .... } } private void processReadRequestV3(final BookkeeperProtocol.Request r, final BookieRequestHandler requestHandler) { //能看到BK也同时支持长轮询的方式读取数据 if (RequestUtils.isLongPollReadRequest(r.getReadRequest())) { .... read = new LongPollReadEntryProcessorV3(r, requestHandler, this, fenceThread, lpThread, requestTimer); } else { read = new ReadEntryProcessorV3(r, requestHandler, this, fenceThread); .... } if (null == threadPool) { //跟进去看看实现 read.run(); } else { .... } } public void run() { .... //执行读取操作 executeOp(); } protected void executeOp() { //这里感觉设计得不是很清晰，应该先读取数据出来再构造返回对象的，你们觉得呢？ ReadResponse readResponse = getReadResponse(); if (null != readResponse) { sendResponse(readResponse); } } protected ReadResponse getReadResponse() { // 读取Entry数据的地方，在此处深入探索下 return readEntry(readResponse, entryId, startTimeSw); } catch (Bookie.NoLedgerException e) { .... } } protected ReadResponse readEntry(ReadResponse.Builder readResponseBuilder, long entryId, Stopwatch startTimeSw) throws IOException, BookieException { //继续深入 return readEntry(readResponseBuilder, entryId, false, startTimeSw); } protected ReadResponse readEntry(ReadResponse.Builder readResponseBuilder, long entryId, boolean readLACPiggyBack, Stopwatch startTimeSw) throws IOException, BookieException { //调用Bookie的readEntry来读取数据 ByteBuf entryBody = requestProcessor.getBookie().readEntry(ledgerId, entryId); .... } public ByteBuf readEntry(long ledgerId, long entryId) throws IOException, NoLedgerException, BookieException { .... try { LedgerDescriptor handle = handles.getReadOnlyHandle(ledgerId); .... //调用真正读数据的逻辑，因为在这里能看到获取的值entry也是对外返回的 //这里调用的是LedgerDescriptor类的readEntry方法 ByteBuf entry = handle.readEntry(entryId); .... return entry; } finally { .... } } ByteBuf readEntry(long entryId) throws IOException, BookieException { //调用LedgerStorage接口，SingleDirectoryDbLedgerStorage实现类来读取Entry return ledgerStorage.getEntry(ledgerId, entryId); } public ByteBuf getEntry(long ledgerId, long entryId) throws IOException, BookieException { long startTime = MathUtils.nowInNano(); try { //继续往下跟踪 ByteBuf entry = doGetEntry(ledgerId, entryId); recordSuccessfulEvent(dbLedgerStorageStats.getReadEntryStats(), startTime); return entry; } catch (IOException e) { .... } } private ByteBuf doGetEntry(long ledgerId, long entryId) throws IOException, BookieException { .... //尝试从BK本地缓存中读取数据 ByteBuf entry = localWriteCache.get(ledgerId, entryId); //尝试从本地缓存flush中进行数据命中数据 entry = localWriteCacheBeingFlushed.get(ledgerId, entryId); // 尝试从读缓存中进行数据读取 entry = readCache.get(ledgerId, entryId); //从磁盘文件中进行数据读取, 调用的是EntryLogger接口，DefaultEntryLogger对象的readEntry方法 entry = entryLogger.readEntry(ledgerId, entryId, entryLocation); //写到读缓存中 readCache.put(ledgerId, entryId, entry); .... return entry; } public ByteBuf readEntry(long ledgerId, long entryId, long entryLocation) throws IOException, Bookie.NoEntryException { //再进一步探索 return internalReadEntry(ledgerId, entryId, entryLocation, true /* validateEntry */); } private ByteBuf internalReadEntry(long ledgerId, long entryId, long location, boolean validateEntry) throws IOException, Bookie.NoEntryException { //获取entry所在的LogId long entryLogId = logIdForOffset(location); long pos = posForOffset(location); BufferedReadChannel fc = null; int entrySize = -1; try { fc = getFCForEntryInternal(ledgerId, entryId, entryLogId, pos); ByteBuf sizeBuff = readEntrySize(ledgerId, entryId, entryLogId, pos, fc); entrySize = sizeBuff.getInt(0); if (validateEntry) { validateEntry(ledgerId, entryId, entryLogId, pos, sizeBuff); } } catch (EntryLookupException e) { .... } ByteBuf data = allocator.buffer(entrySize, entrySize); //进行数据读取 int rc = readFromLogChannel(entryLogId, fc, data, pos); .... data.writerIndex(entrySize); return data; } private int readFromLogChannel(long entryLogId, BufferedReadChannel channel, ByteBuf buff, long pos) throws IOException { BufferedLogChannel bc = entryLogManager.getCurrentLogIfPresent(entryLogId); .... //继续往下 return channel.read(buff, pos); } public int read(ByteBuf dest, long pos) throws IOException { //继续往下 return read(dest, pos, dest.writableBytes()); } public synchronized int read(ByteBuf dest, long pos, int length) throws IOException { .... while (length \u0026gt; 0) { // Check if the data is in the buffer, if so, copy it. if (readBufferStartPosition \u0026lt;= currentPosition \u0026amp;\u0026amp; currentPosition \u0026lt; readBufferStartPosition + readBuffer.readableBytes()) { int posInBuffer = (int) (currentPosition - readBufferStartPosition); int bytesToCopy = Math.min(length, readBuffer.readableBytes() - posInBuffer); dest.writeBytes(readBuffer, posInBuffer, bytesToCopy); currentPosition += bytesToCopy; length -= bytesToCopy; cacheHitCount++; } else { // We don\u0026#39;t have it in the buffer, so put necessary data in the buffer readBufferStartPosition = currentPosition; int readBytes = 0; //从磁盘读取数据到readBuffer中，再将readBuffer的数据写到 dest中作为返回值 //这里调用的是Java NIO FileChannel类的read方法来从磁盘进行数据的读取 if ((readBytes = validateAndGetFileChannel().read(readBuffer.internalNioBuffer(0, readCapacity), currentPosition)) \u0026lt;= 0) { throw new IOException(\u0026#34;Reading from filechannel returned a non-positive value. Short read.\u0026#34;); } readBuffer.writerIndex(readBytes); } } return (int) (currentPosition - pos); } 总结 在这里解答下引言小故事\n张三是通过什么规则被选成“监督者”的？\n张三是通过zookeeper的Paxos算法选举产生的\n如果张三也不辞而别呢？\n大狗和二狗也会通过zookeeper监听张三的状态，如果张三不辞而别的话，大狗二狗会通过zookeeper选举成为新的“监督者”\n为啥要通过签到本的方式，而不是张三直接去挨个挨个看？\n通过签到本的方式比较节约张三的时间，否则当员工比较多的时候并且对感知时间比较快的时候，张三就要每隔几分钟就要跑去挨个挨个看，这样没多久张三也要“不辞而别”了。通过签到本如果某个同事不签到了张三就能很轻松感知到并做相对应的处理了\n参考资料 https://bookkeeper.apache.org/docs/admin/autorecovery/ bk项目 site3/website/docs/admin/* 指令使用说明 ","date":"2024-01-04T12:00:10+08:00","image":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3bookkeeper%E4%B9%8Bautorecovery%E6%9C%BA%E5%88%B6/image-20231215102118251_hu13366877165211677337.png","permalink":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3bookkeeper%E4%B9%8Bautorecovery%E6%9C%BA%E5%88%B6/","title":"详解bookkeeper之AutoRecovery机制"},{"content":"不知不觉一年又快结束了，今天写了不少文章，那就专门写一份给自己的信\n工作 2023年整体过得还算平稳，先说说工作上的事，岗位上顺利升职，有个比较大的感慨，就是写好PPT是真的很有必要的，看了一些大佬们的晋升PPT发现人家对产品的剖析非常清晰并且很多结果都是有数据指标来支撑的，通过PPT就能明显感觉到大佬对产品的理解是很透彻的。因此自己也给自己定了个任务，就是每年都要给自己写当年的答辩PPT以及简历，通过写这些东西能够帮你思考提炼这个年度的收获以及指导未来的方向。除此之外还有三个觉得比较值得写得点\n对业务务必要深入了解 对业务理解这个好处有3点，第一就是当你对业务有足够的了解时，无论发生任何事情你都心里有底，即便有故障如果你能清晰的知道没有影响到线上流量那就不用太担心， 第二就是当对业务足够了解时，在设计程序以及跟产品battle时能够更好的抓主要矛盾，有的放矢，从而设计出更适合业务的软件。 第三就是当你对业务足够了解时，有些工作(非产品)上你可以大胆的跟各种大佬们极限拉扯，拉扯赢了有些非必要并且耗费精力的事情就可以不做或者慢慢做 先紧后驰 这点是我需要改进的点，在接到任务时我喜欢慢慢展开，喜欢花更多前期的时间去熟悉任务以及放松一下，这有时会影响任务的推进。因此在接手一些重要的任务时， 应该尽快的进行任务的展开，不用可以追求完美，先做了再说，不对再在做的过程中做调整，行就是知，知就是行，两者本就是一体的，有些东西不去做是永远也想不明白的 定期reset 这是听了郭宇大佬的专访后得出的体会，如果每天都日复一日的工作，会发现时间过得很快并且心情也容易烦躁，应该定期的给自己重置下，例如花两三天去一个陌生 城市放空自己，忘记自己的工作与生活，就跟自己跟世界对话，然后像看其他人的人生一样看自己这段时间的经历，客观的思考哪些是自己追求的，哪些是自己不需要的 等等。每年都要如此几次，通过这样来调整自己的节奏，让自己整体有条不絮的往自己的终极目标前进 学习 说完工作，再聊聊学习上的事情，今年比较大的收获是坚持了半年的背单词，每天保持背50～70个单词，不追求每个都背下来，更多的是背一个感觉，就是看到这个单词能猜到一个大概，这是从“学习飞轮”那片文章上得到的灵感，就是有些东西一定要快起来，背单词也是一样的，与其追求完美导致止步不前，不如先保持背的习惯以及对大部分单词有个认识，后续要用到的单词也会频繁的出现在自己面前加深自己的印象。除了背单词之外还有比较好的点是终于重新拾起写博客的习惯，相比于前两年的犹豫不决，今年的自己算是从迷茫逐渐走向坚定，虽然自己现在还没想明白自己的使命，但是可以确定的事就是，我希望能够继续深挖技术能力，不设边界，任何编程语言，任何领域知识只要感兴趣都可以学习以及深挖，这个过程中主要看的是这个领域的大佬是如何设计解决方案来解决对应场景的问题的。因此在这个背景下希望自己能够多输出高质量的文章，不仅能够倒逼自己学习、思考提炼，还能够通过这种方式影响更多的人，让读者能够爱上编程、爱上大数据、爱上设计、爱上生活\u0026hellip;. 除了这些以外还发现了一个拦路虎，就是写文章喜欢拖延，很多东西想一下子写到最后，然后反复推敲某个点，或者希望能够等绘制出更详细的图辅助说明等等导致无限期拖延，今年就有十多篇文章因为这个导致无限延期，因此明年希望自己在这方面能够快速闭环，有合适的想法的时候写得差不多就发出来，先闭环，等后续有新的见解可以再更新或者输出新的文章等，简单来说还是要知行合一，有时候不发出来你也会永远不知道差在哪里\n开心与遗憾 在这里罗列三件开心和遗憾的事，先说说开心的事，第一件是终于想清楚要认真学习以及输出文章这件事了，相比前两年犹豫要不要放弃编程，也许设计代码这件事情更符合自己内心的追求，认真学技术以及输出文章也算是 follower 自己的内心了。第二件就是改用VSCode来写文章，这个真的太爽了，不用再纠结markdown的语法，不用纠结于文件不好管理维护等等，就沉浸于码字，用字将自己的思绪串联起来然后通过手指敲动以字节流的方式持久化到设备上，有时能进入心流的状态，这种感觉可能只有懂得才懂。第三件就是要去湖南浏阳跨年了，也就是后天，在漫天的烟花中送走2023年，并迎来新的一年的自己，对于多年没看到烟花以及自己放烟花的自己来说这是 莫大值得激动的事情～ 说了开心的事，再说说遗憾的事吧，第一件是自己今年始终没有勇敢的迈出那一步，大学室友勇敢的去自己创业开奶茶店，同事豪哥也去尝试自己喜欢生意，而我还是选择了相对稳定工作，希望明年我也能勇敢的去尝试更多自己喜欢的事情。第二件就是在国庆时候跟家人去香港澳门游玩，由于自己没有提前做好攻略，感觉有些地方有些遗憾，毕竟家人很少离开小岛。第三件事就是改补的牙由于忙导致一直拖着，现在的牙洞逐渐扩展，内心开始有些紧张了，还是穿了又要做根管治疗了～\n展望 然后也希望明天的这个时候，能够在更多自己喜欢的领域深耕并输出更多优质的文章，然后最后想给大家分享的是，大胆去做你想做的那个人吧，把你的想法写出来然后 去实现，不用在乎别人的看法说法，首先真的没有那么多的人去关注我们，其次，在我们的世界我们才是主角，做让自己开心的事就好了。走正确的路， 就是做自己真正热爱的事，哪条是正确的路？只有自己能定义。路不止一条，坚持做你在乎的事，走出自己在乎的路，给出你对这个世界独有的回答！\n再见2023，你好2024！\n","date":"2023-12-25T15:09:16+08:00","image":"https://sherlock-lin.github.io/p/%E4%B8%80%E5%B0%81%E5%86%99%E7%BB%99%E5%B9%B4%E6%9C%AB%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BF%A1/matt-le-SJSpo9hQf7s-unsplash_hu10664154974910995856.jpg","permalink":"https://sherlock-lin.github.io/p/%E4%B8%80%E5%B0%81%E5%86%99%E7%BB%99%E5%B9%B4%E6%9C%AB%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BF%A1/","title":"一封写给年末自己的信"},{"content":"引言 距离第一次写博客已经过去将近10年时间了，今年就专门以问答形式写写为什么要写下这些玩意\n正文 为什么要写这些玩意 作者是一个什么事情都喜欢思考有没有意义的功利性的人，但本次写作的源头却事出反常，就单纯觉得把弄懂的东西以某种媒介记录下来，在未来的某个时间点回过头来看以时间轴将这些思想穿起来的这样一部“电影”一定很有意思，如果诸君感兴趣也可一同观看\n打算写哪些方向的文章 这块没有给自己设限，目前个人知识方向主要是 编程、大数据、操作系统、历史、以及读书心得这几个方向，正如一千个读者有一千个哈姆雷特，作者也是一样，即便是现在已经有很多现成的文章了，但通过不同作者的视角下看到的东西可能也不太一样。本人擅长抽象以及提炼，对不少东西都有自己的一番见解，因此有信心将一些普罗大众觉得复杂的东西 提炼成简单的图或者故事来给自己以及读者解惑。不要求文章多，但一定是要经过自己大脑思考、咀嚼消化过后的东西，流水账的东西尽量不写或少些。一言以蔽之，就是写值得写的东西\n使命感 最近在跟朋友聊天有聊到这个，他希望自己能找到自己来到这个世界的使命，本人其实也一样在寻找，写文章不是使命感驱动的，更多的是作为找到使命前的“历练”，如果要功利点算的话，那就是通过写作来磨练自己的思维框架这把利剑，并将它送给未来找到使命的自己，也许写着写着就找到了也说不定呢\n会不会觉得浪费时间 说实话，曾经有段时间觉得挺浪费时间的，写了自己也不怎么会看，也不会有什么读者去看，那写这些东西的意义何在呢？所以最终放弃了一段时间，但后来发现不写文章的时间要么也是刷短视频、打游戏、发呆等等，那好像更没啥意义，对于那些日复一日年复一年没任何积累的东西，写作反而可爱得像个养成游戏，每天陪伴它一会，它就会茁壮成长，长成一颗苍天大树，供自己安放那颗躁动的心以及给行人遮阴凉\n孤独吗 有句话说，每个作者都是孤独的。本人也不例外，但很幸运的是可以通过文字这种工具来跨时空地域跟很多人聊天，像远在贵阳龙场的王阳明，在岳阳楼上写下“博，一介书生，三尺微命”的那个少年，“也无风雨也无晴”的东坡先生，甚至有那么一刻，我是我，也是他们，也懂了他们的孤独、惆怅、无奈，如今经历得越多那份情绪就更加浓郁几分。现如今，我也想通过文字跟不同时空的自己沟通，以及另一个时空的你，因为我，就是你啊\n小结 好像写着写着有些跑题了，主要是结合近期思考的知识回答下一些自己心中的疑问，同时也提醒自己勿忘初心并保持写作思考的节奏\n","date":"2023-12-12T15:09:15+08:00","image":"https://sherlock-lin.github.io/p/%E4%B8%BA%E5%95%A5%E8%A6%81%E5%86%99%E8%BF%99%E4%BA%9B%E7%8E%A9%E6%84%8F/the-creative-exchange-d2zvqp3fpro-unsplash_hu5876398126655421130.jpg","permalink":"https://sherlock-lin.github.io/p/%E4%B8%BA%E5%95%A5%E8%A6%81%E5%86%99%E8%BF%99%E4%BA%9B%E7%8E%A9%E6%84%8F/","title":"为啥要写这些玩意"},{"content":"写在前面 过去的十多年，随着国内互联网的蓬勃发展，信息呈现爆炸式增长；一方面，咱们接受信息的渠道变得多样化如搜索引擎、博客、朋友圈、短视频等等，另一方面，投入制造这些内容的人也多了，效率也更高了，因此我们每天接受的信息数不胜数。有些 制造者 为了自己的内容能够在众多内容中脱颖而出，因此诞生了不少 标题党、贩卖焦虑的制造者。这些内容会在用户刷手机时，仅通过0.5秒的时间就能抓住用户的眼球或者击中用户的心理，从而创造流量进而从中获利。久而久之就成了这批人在用户的大脑内圈地跑马，往用户大脑内注入自己大量的信息以及暗示，最最重要的是他们注入的都是经过他们进行“消化”过的N手知识，用户如果长期习惯于这种“被投喂”的方式，逐渐会丧失独立思考能力，从而彻底依赖于他们。借用《肖申克的救赎》里的一句话，“一开始，你痛恨周围的高墙，满满地，你习惯了生活在其中，最终你会发现自己不得不依靠它而生活”。\n经历 说说自己的经历吧，几年前在房价腾飞的时候，当时很流行一套方案，“借钱付首付，然后房租抵月供，然后每个月的工资用来还借的首付，过几年卖了房子相当于白赚几十万”，通过这种空手套白狼的方式来达到快速赚钱。当时人云亦云，相信不少朋友那个时候周围或多或少都有这样的声音，博主当时也是深信不疑，一方面是大家都这么说久而久之大脑内部也深度相信这个理论，另一方面是随着房价xiuxiuxiu往上涨自己也说服自己，这不是现实验证理论吗，但随后几年就给大众啪啪啪打脸了，甚至也不少人血本无亏。现在反思这个事情，大家都能清楚的知道，这套方案的成立是有大前提的，“经济要一直蓬勃发展”，没有什么稳赚不赔的买卖。但当时没有看清的最主要原因就是，没有什么独立批判的能力，别人说什么，只要说的频繁以及说的人多，自己就全信，这样的人出了社会往往就成了所谓的“韭菜”。在现如今，这样的群里依然不在少数，人云亦云，你只要稍微多问几个问题他就答不上来，或者说的全是“标准答案”。如果说每个人都是一辆行驶的汽车，那这样做无疑是将方向盘毫无疑问的交给一个未知的人，你希望这样吗？\n思考 前两段主要描述的是我们遇到的情况，这一段主要是聊聊如何去应对。首先最重要的是，一定要保持充电也就是知识的学习，方式不重要，其中包括但不仅限于 书籍、视频、与人探讨等等，最主要的是要自己搭建属于自己的一套“知识系统”并且不断自我完善。在有新的知识或者新的风口跳出来时，你可以基于自己的“知识系统”对其进行批判性的学习以及了解，不要人云亦云，今天追逐短视频、明天追逐gpt，这样除了了解新东西的皮毛，最终只剩下一颗焦虑的心，因为你会发现虽然不断的学新东西，不停的追逐新事物，但属于你手中，脑中的东西会越来越少，与日俱增的只有无穷的空虚以及焦虑。\n保持充电，构建自己的“知识系统” 切忌盲目追逐新事物，新东西有新东西的好，老东西有老东西的好，毕竟经典的东西都是经过时间洗礼的，减少试错成本 保持批判能力，任务事物一定一定一定有两面性，就像 道家的阴阳鱼一样，有阳光的地方就会有阴影。别人越是大肆宣扬什么，我们越是要谨慎思考他没说的那部分，要系统性全面的去思考某个观点，而不是放弃思考完全跟着别人走 避免过多的沉溺于“多巴胺陷阱”，比如花过多的时间在短视频、游戏等等上面 定期锻炼，保持愉悦积极的心态 总结 最后，虽然本篇博客提出了一个问题，已经提出了自认为的“应对方法”，但也不是每个人都必须这么去做的。首先第一点，随波逐流和独立思考也有各自的两面性，有些人选择随波逐流一样可以过得好好，这就看个人选择。第二点，选择独立思考的背景下，应对博客提出的问题其实也有很多解法，建议读者也批判性的看待这篇博客，如果觉得这篇博客也是“标题党”或者“制造焦虑”也正常，重要的是，保持独立思考，特别是深度思考，在沉浸其中时，你会忘了时间、忘了一切，仅仅沉浸于你的世界，去构造属于你自己的系统，也就是你自己。最后，祝大家都能成为自己想成为的那种人。\n","date":"2023-08-07T15:09:15+08:00","image":"https://sherlock-lin.github.io/p/%E9%9A%8F%E6%B3%A2%E9%80%90%E6%B5%81or%E7%8B%AC%E7%AB%8B%E6%80%9D%E8%80%83/matt-le-SJSpo9hQf7s-unsplash_hu10664154974910995856.jpg","permalink":"https://sherlock-lin.github.io/p/%E9%9A%8F%E6%B3%A2%E9%80%90%E6%B5%81or%E7%8B%AC%E7%AB%8B%E6%80%9D%E8%80%83/","title":"随波逐流or独立思考"}]