[{"content":"一、概述 大数据背景下，分区应该是所有组件必备的基本条件，否则面对海量数据时无论是计算还是存储都容易遇到瓶颈。跟其他消息系统一样，Pulsar通过Topic将消息数据进行业务层面划分管理，同时也支持Topic分区，通过将多个分区分布在多台Broker/机器上从而带来性能上的巨大提升以及无限的横向拓展能力。而一旦有了分区之后就会面临一个问题，但一条数据请求时应该将其发往哪个分区？目前Pulsar跟其他消息系统一样支持以下三种路由模式。\n轮询路由 生产者会按将消息按批为单位轮询发送到不同的分区，这是一种常见的路由策略，具有简单的优势，由于它不需要过多的配置以及考虑但却可以表现不错的性能。如果消息带有key的话会根据key进行哈希运算后再对分区进行取模来决定消息投放的目标分区。 单分区路由 单分区路由提供一种更简单的机制，它会将所有消息路由到同一个分区。这种模式类似非分区Topic，如果消息提供key的话将恢复到轮询哈希路由方式 自定义分区路由 自定义分区路由支持你通过实现MessageRouter接口来自定义路由逻辑，例如将特定key的消息发到指定的分区等 二、实战 消息路由发生在生产者端，在创建生产者是通过 .messageRoutingMode() 进行指定，下面就分别实战对比下这三种的路由效果\n1. 轮询路由 先试试轮询路由的策略，这是最常见也是默认的路由策略，通过 .messageRoutingMode(MessageRoutingMode.RoundRobinPartition) 进行指定，然后往里面通过同步方式往分区Topic里面写入数据\n1 2 3 4 5 6 7 8 9 10 11 12 String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(Schema.STRING) .topic(\u0026#34;sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_2\u0026#34;) .messageRoutingMode(MessageRoutingMode.RoundRobinPartition) //.messageRoutingMode(MessageRoutingMode.SinglePartition) .create(); for (int i = 0; i \u0026lt; 20000; i++) { producer.send(\u0026#34;hello java API pulsar:\u0026#34;+i+\u0026#34;, 当前时间为：\u0026#34;+new Date()); } 通过管理页面可以看到数据基本均匀的落在各个分区，从这个结果是能够反向验证数据是符合轮询发送后的效果\n2. 单分区路由 现在试试单分区路由的策略，通过 .messageRoutingMode(MessageRoutingMode.SinglePartition) 进行指定，并往分区Topic里面写入一批数据\n1 2 3 4 5 6 7 8 9 10 11 String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(Schema.STRING) .topic(\u0026#34;sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_2\u0026#34;) .messageRoutingMode(MessageRoutingMode.SinglePartition) .create(); for (int i = 0; i \u0026lt; 20000; i++) { producer.send(\u0026#34;hello java API pulsar:\u0026#34;+i+\u0026#34;, 当前时间为：\u0026#34;+new Date()); } 通过管理页面可以看到数据都落在第一个分区，说明这也符合官网中对单分区路由的描述。同时经过反复试验多次发现，生产者会随机选择一个分区并将所有数据发送到这个分区。\n3. 自定义路由 在有些业务场景，我们需要将自己的业务逻辑“融入”路由策略，因此像Pulsar、Kafka等消息中间件都是支持用户进行路由规则的自定义的。这里为了好玩，咱们尝试将数据按照 1:2:3:4 等比例分别落在四个分区如何？说干就干，自定义路由也是比较简单的，只需要实现Pulsar MessageRouter接口的choosePartition方法即可，实现逻辑如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class SherlockRouter implements MessageRouter { Long count = 0L; public int choosePartition(Message\u0026lt;?\u0026gt; msg, TopicMetadata metadata) { count++; count = count % 10; if (count == 0) return 0; if (count \u0026lt; 3) return 1; if (count \u0026lt; 6) return 2; return 3; } } 通过上面代码可以看到，参数msg就是生产者中国呢发送的消息对象，metadata是这条消息的元数据如租户、命名空间等等，而返回值其实就是这个Topic分区的下标，这里需要注意的是不要超过Topic的分区数，同时一些比较复杂的数据处理逻辑代码尽量不要写在这里影响消息发送性能以及不规范。\n写完后通过 .messageRouter() 方法进行指定即可使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public static void customRoundSchemaProducer() throws Exception { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(Schema.STRING) .topic(\u0026#34;sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_3\u0026#34;) .messageRouter(new SherlockRouter()) .create(); for (int i = 0; i \u0026lt; 20000; i++) { producer.send(\u0026#34;hello java API pulsar:\u0026#34;+i+\u0026#34;, 当前时间为：\u0026#34;+new Date()); } producer.close(); pulsarClient.close(); } 在管理页面可以看到，数据是按照咱们预期的逻辑 1:2:3:4等比落在分区里面，嘿嘿～\n三、源码分析 1. 接口以及父类 Pulsar中所有路由规则都是基于MessageRouter接口进行实现的，这个接口主要提供了choosePartition方法，只要重写这个方法即可自定义任意自己预期的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @InterfaceAudience.Public @InterfaceStability.Stable public interface MessageRouter extends Serializable { /** * * @param msg * Message object * @return The index of the partition to use for the message * @deprecated since 1.22.0. Please use {@link #choosePartition(Message, TopicMetadata)} instead. */ @Deprecated default int choosePartition(Message\u0026lt;?\u0026gt; msg) { throw new UnsupportedOperationException(\u0026#34;Use #choosePartition(Message, TopicMetadata) instead\u0026#34;); } /** * Choose a partition based on msg and the topic metadata. * * @param msg message to route * @param metadata topic metadata * @return the partition to route the message. * @since 1.22.0 */ default int choosePartition(Message\u0026lt;?\u0026gt; msg, TopicMetadata metadata) { return choosePartition(msg); } } MessageRouterBase是路由策略的抽象类，主要定义了消息有key时的哈希算法，像上面提的轮询路由和单分区路由继承了这个抽象类。JavaStringHash和Murmur3Hash32两个都是Pulsar提供的哈希算法的实现类，两者的差异后面再单独进行分析\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public abstract class MessageRouterBase implements MessageRouter { private static final long serialVersionUID = 1L; protected final Hash hash; MessageRouterBase(HashingScheme hashingScheme) { switch (hashingScheme) { case JavaStringHash: this.hash = JavaStringHash.getInstance(); break; case Murmur3_32Hash: default: this.hash = Murmur3Hash32.getInstance(); } } } 2. 轮询路由的实现 主要看choosePartition 方法的逻辑，首先如果消息带有key则针对key进行哈希然后取模，这样可以保证相同key的消息落在同一个分区。然后就是判断消息是否按批次进行发送的，如果是单条消息发送的话则通过一个累加计数器进行轮询分区，即可达到消息按照分区顺序逐个发送的效果；如果是按批次发送的话，则是根据时间戳进行取模，这样达到的效果就是每批数据都会随机发送到某一个分区\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public class RoundRobinPartitionMessageRouterImpl extends MessageRouterBase { @SuppressWarnings(\u0026#34;unused\u0026#34;) private volatile int partitionIndex = 0; private final int startPtnIdx; private final boolean isBatchingEnabled; private final long partitionSwitchMs; .... @Override public int choosePartition(Message\u0026lt;?\u0026gt; msg, TopicMetadata topicMetadata) { // If the message has a key, it supersedes the round robin routing policy if (msg.hasKey()) { return signSafeMod(hash.makeHash(msg.getKey()), topicMetadata.numPartitions()); } if (isBatchingEnabled) { // if batching is enabled, choose partition on `partitionSwitchMs` boundary. long currentMs = clock.millis(); return signSafeMod(currentMs / partitionSwitchMs + startPtnIdx, topicMetadata.numPartitions()); } else { return signSafeMod(PARTITION_INDEX_UPDATER.getAndIncrement(this), topicMetadata.numPartitions()); } } } 3. 单分区路由 可以看到单分区的逻辑是比较简单且清晰的，如果有key就进行哈希取模，否则就发送到partitionIndex这个成员变量指定的分区去，那么这个partitionIndex指定的是哪个分区呢？通过代码能看到是从构造函数里面传进来的，因此跟踪下代码看看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class SinglePartitionMessageRouterImpl extends MessageRouterBase { private final int partitionIndex; public SinglePartitionMessageRouterImpl(int partitionIndex, HashingScheme hashingScheme) { super(hashingScheme); this.partitionIndex = partitionIndex; } @Override public int choosePartition(Message\u0026lt;?\u0026gt; msg, TopicMetadata metadata) { // If the message has a key, it supersedes the single partition routing policy if (msg.hasKey()) { return signSafeMod(hash.makeHash(msg.getKey()), metadata.numPartitions()); } return partitionIndex; } } 通过跟踪可以看到是在PartitionedProducerImpl类的getMessageRouter方法中进行SinglePartitionMessageRouterImpl类的初始化，同时是通过ThreadLocalRandom.current().nextInt(topicMetadata.numPartitions()) 来生成一个小于分区数的随机数，因此单分区路由的分区是随机指定的一个，这个结果跟咱们实战中测试的效果是吻合的。除此之外，咱们还看到 getMessageRouter方法中会根据咱们在创建生产者时 .messageRoutingMode 方法指定的路由模式来创建对应的路由实现类，在这里可以明确的看到没有指定的话默认就是采用的轮询路由规则\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 private MessageRouter getMessageRouter() { MessageRouter messageRouter; MessageRoutingMode messageRouteMode = conf.getMessageRoutingMode(); switch (messageRouteMode) { case CustomPartition: messageRouter = Objects.requireNonNull(conf.getCustomMessageRouter()); break; case SinglePartition: messageRouter = new SinglePartitionMessageRouterImpl( ThreadLocalRandom.current().nextInt(topicMetadata.numPartitions()), conf.getHashingScheme()); break; case RoundRobinPartition: default: messageRouter = new RoundRobinPartitionMessageRouterImpl( conf.getHashingScheme(), ThreadLocalRandom.current().nextInt(topicMetadata.numPartitions()), conf.isBatchingEnabled(), TimeUnit.MICROSECONDS.toMillis(conf.batchingPartitionSwitchFrequencyIntervalMicros())); } return messageRouter; } 四、总结 通过以上内容相信你对Pulsar的路由规则有一定的了解了，如果想进一步了解可以尝试按照自己喜好实现下路由规则并观测是否按照预期运行，同时也可以跟踪Pulsar的源码看看实现是否符合预期。如果想彻底掌握Pulsar，最好自己跟踪下Pulsar的一些核心逻辑，这样不仅了解其底层是如何运作的，也能加深你对一些设计/特性的印象。\n","date":"2024-09-20T10:34:56+08:00","image":"https://sherlock-lin.github.io/p/pulsar%E6%B6%88%E6%81%AF%E8%B7%AF%E7%94%B1%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90/image-20240306152645225_hu5442259153227018439.png","permalink":"https://sherlock-lin.github.io/p/pulsar%E6%B6%88%E6%81%AF%E8%B7%AF%E7%94%B1%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90/","title":"Pulsar消息路由深入剖析"},{"content":"一、引言 关于Pulsar Schema，咱们要想想以下几个问题\nPulsar 中的 Schema 是什么？ Pulsar Schema Registry的作用是什么？ 怎么使用？ 原理是什么？ 二、Schema 是什么 Schema是定义结构化数据和二进制字节数组之间转换的逻辑，Pulsar的消息是以非结构化的二进制数组进行存储的，Schema只有在读写时才会被应用于数据上，因此生产者和消费者需要对Schema达成一致。Pulsar通过Schema Registry作为一个中央仓库存储Schema信息，它可以协调生产者和消费者保证相同的Schema，它可以存储多个版本的Schema，支持不同的兼容性配置以及根据兼容性的要求进行Schema的演进。Pulsar将Schema存储在Bookie上，Schema的写入、读取都通过Broker和Bookie交互，这个逻辑跟消息的读写操作是一只的，因此不需要额外考虑Schema的可用性和可靠性问题，因此整体看Pulsar实现Schema Registry的方式非常优雅\n类型安全在所有数据应用中都非常重要，生产者和消费者需要某种机制协调数据类型来避免各种潜在的问题，比如序列化和反序列化方式不一致。数据安全通常有两种处理方式client-side和service-side，本质上就是客户端用时决定和服务端提前保证\nclient-side：将一切交给用户，客户端自行负责消息的序列化和反序列化并且保证生产消费时消息的类型安全，这种方式的最大问题就是类型是通过约定的，一旦生产者写入非约定的数据，下游的消费者将没有办法解析数据\nserver-side：数据安全由服务端保证，生产者和消费者都需要跟服务端提前确定数据类型。这种方式真正意义上保证了数据的类型安全，避免了生产者写入非法数据的问题\n两种差异如下图\n三、怎么使用 1. client-side 生产者代码逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 //schema在第一次写入的时候就已经决定好了，后续用其他的schema消息类型会写入失败 public static void customSchemaProducer() { try { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;byte[]\u0026gt; producer = pulsarClient.newProducer() .topic(\u0026#34;persistent://sherlock-api-tenant-1/sherlock-namespace-1/topic_8\u0026#34;) .create(); User user = new User(); user.setName(\u0026#34;老六\u0026#34;); user.setAge(21); user.setAddress(\u0026#34;海南\u0026#34;); //由用户自行做序列化逻辑 producer.send(JSON.toJSONString(user).getBytes()); producer.close(); pulsarClient.close(); } catch (Exception e) { } } 消费者逻辑代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public static void customSchemaConsumer() { try { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Consumer\u0026lt;byte[]\u0026gt; consumer = pulsarClient.newConsumer() .topic(\u0026#34;persistent://sherlock-api-tenant-1/sherlock-namespace-1/topic_8\u0026#34;) .subscriptionName(\u0026#34;sub_03\u0026#34;) .subscribe(); while(true) { Message\u0026lt;byte[]\u0026gt; message = consumer.receive(); //由用户自行做序列化逻辑 byte[] user = message.getValue(); System.out.println(\u0026#34;消息数据为：\u0026#34;+JSON.parseObject(user, User.class).toString()); consumer.acknowledge(message); //consumer.negativeAcknowledge(message); } } catch (Exception e) { } } 执行效果如下\n1 消息数据为：User{name=\u0026#39;老六\u0026#39;, age=21, address=\u0026#39;海南\u0026#39;} 2. server-side 生产者代码逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 //schema在第一次写入的时候就已经决定好了，后续用其他的schema消息类型会写入失败 public static void customSchemaProducer() { try { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); //由Pulsar做序列化逻辑 Producer\u0026lt;User\u0026gt; producer = pulsarClient.newProducer(AvroSchema.of(User.class)) .topic(\u0026#34;persistent://sherlock-api-tenant-1/sherlock-namespace-1/topic_7\u0026#34;) .create(); User user = new User(); user.setName(\u0026#34;王武\u0026#34;); user.setAge(36); user.setAddress(\u0026#34;海南\u0026#34;); producer.send(user); producer.close(); pulsarClient.close(); } catch (Exception e) { } } 消费者逻辑代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public static void customSchemaConsumer() { try { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); //由Pulsar做序列化逻辑 Consumer\u0026lt;User\u0026gt; consumer = pulsarClient.newConsumer(AvroSchema.of(User.class)) .topic(\u0026#34;persistent://sherlock-api-tenant-1/sherlock-namespace-1/topic_7\u0026#34;) .subscriptionName(\u0026#34;sub_03\u0026#34;) .subscribe(); while(true) { Message\u0026lt;User\u0026gt; message = consumer.receive(); User user = message.getValue(); System.out.println(\u0026#34;消息数据为：\u0026#34;+user); consumer.acknowledge(message); //consumer.negativeAcknowledge(message); } } catch (Exception e) { } } 执行效果如下\n1 消息数据为：User{name=\u0026#39;王武\u0026#39;, age=36, address=\u0026#39;海南\u0026#39;} 3. 小结 分别查看两个Topic的Schema信息如下图\n通过查询client-side的Schema信息，会发现Pulsar服务端其实并没有进行存储，相当于不指定Schema的话Pulsar默认都用byte数组\n再来看看server-side的Schema信息，可以看到打印如下，namespace是pojo类的包路径，name是pojo类名，然后fields就是pojo类的各个字段的属性(像不像mysql里面的表结构，不少场景Topic就是当作表来用的)，然后type是AVRO是由于咱们是用的avro进行序列化的。\n除了在读写数据时指定Schema，Pulsar还支持通过admin管理流提前指定好，具体指令在这里。如果是用Pulsar来作为实时数仓场景，强烈建议提前通过admin管理流进行指定好，配置isSchemaValidationEnforced可以考虑开启。如果条件允许可以考虑做成服务化，例如通过Web页面提供新建Schema、修改Schema操作并接入公司内部的审批流等\n1 2 3 pulsar-admin schemas upload --filename POST /admin/v2/schemas/:tenant/:namespace/:topic/schema pulsar-admin schemas get sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_3 四、原理解析 Schema相关的流程咱们需要关注以下几个\n注册Schema流程 生产者端侧 消费者端侧 指定服务器 Schema生效流程 更新Schema流程 1. 注册Schema流程 生产者端侧\n生产者实例会在内部构造schema实例，生产者会通过它对数据进行转换 生产者会请求连接Broker，并传递schema信息 SchemaInfo Broker会在schema registry中查找这个schema是否被注册，如果已经注册了就将注册的schema版本返回给生产者 Broker检查是否支持自动更新schema，如果配置不允许自动更新，则这个schema不能被注册并且拒绝生产者 Broker进行schema兼容性检查，如果通过检查则将此schema存储在schema registry并返回版本给生产者，生产者所有消息以这个schema格式进行发送；若是检查没通过则拒绝生产者 消费者端\n消费者实例会在内部构造schema实例 消费者请求连接Broker，并传递schema信息 SchemaInfo Broker检查这个Topic是否已经在使用，有的话跳到第五步，否则跳到第四步 Broker检查是否支持自动更新schema，如果支持则注册这个schema，否则拒绝客户端 Broker进行schema兼容性检查，通过则连接否则拒绝客户端 五、源码解析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public static void customSchemaProducer() { try { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;User\u0026gt; producer = pulsarClient.newProducer(AvroSchema.of(User.class)) .topic(\u0026#34;persistent://sherlock-api-tenant-1/sherlock-namespace-1/topic_7\u0026#34;) .create(); User user = new User(); user.setName(\u0026#34;王武\u0026#34;); user.setAge(36); user.setAddress(\u0026#34;海南\u0026#34;); producer.send(user); producer.close(); pulsarClient.close(); } catch (Exception e) { } } 文献 Pulsar：Schema Registry介绍 官方文档 深度解读 Pulsar Schema ","date":"2024-09-20T10:34:56+08:00","image":"https://sherlock-lin.github.io/p/schema%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/image-20240308104136110_hu13493609005795593452.png","permalink":"https://sherlock-lin.github.io/p/schema%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/","title":"Schema深度解析"},{"content":"一、简析 Broker的启动流程框架基本如下\n触发启动 初始化 读取配置、检测、赋值 启动 Bookie启动 Broker启动 启动Netty 启动后台监控任务 二、何时、如何触发启动 Broker的启动基本都是靠维护人员主动触发的，入口是Broker提供的脚本 bin/pulsar、bin/pulsar-daemon。常见的启动指令有 bin/pulsar standalone、 bin/pulsar broker、bin/pulsar-daemon start broker等，今天就从bin/pulsar broker流程进行探讨。先来看看bin/pulsar的脚本逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 pulsar_help() { cat \u0026lt;\u0026lt;EOF //在这里可以看到pulsar支持的操作，当然也可以通过bin/pulsar help 指令进行查看 Usage: pulsar \u0026lt;command\u0026gt; where command is one of: //启动Broker服务 broker Run a broker server //启动bookie服务 bookie Run a bookie server zookeeper Run a zookeeper server configuration-store Run a configuration-store server discovery Run a discovery server proxy Run a pulsar proxy websocket Run a web socket proxy server functions-worker Run a functions worker server sql-worker Run a sql worker server sql Run sql CLI standalone Run a broker server with local bookies and local zookeeper autorecovery Run an autorecovery service .... } //如果执行的是bin/pulsar broker则会走到这里，可以清楚的看到最终会调用PulsarBrokerStarter类进行启动 if [ $COMMAND == \u0026#34;broker\u0026#34; ]; then PULSAR_LOG_FILE=${PULSAR_LOG_FILE:-\u0026#34;pulsar-broker.log\u0026#34;} exec $JAVA $LOG4J2_SHUTDOWN_HOOK_DISABLED $OPTS -Dpulsar.log.file=$PULSAR_LOG_FILE org.apache.pulsar.PulsarBrokerStarter --broker-conf $PULSAR_BROKER_CONF $@ elif [ $COMMAND == \u0026#34;bookie\u0026#34; ]; then PULSAR_LOG_FILE=${PULSAR_LOG_FILE:-\u0026#34;bookkeeper.log\u0026#34;} exec $JAVA $OPTS -Dpulsar.log.file=$PULSAR_LOG_FILE org.apache.bookkeeper.server.Main --conf $PULSAR_BOOKKEEPER_CONF $@ 三、Broker启动流程 通过脚本能够看到是通过PulsarBrokerStarter进行启动，因此现在就直接从它的main方法进行跟踪吧\n这里的初始化和启动两条链路都值得看，首先跟踪下初始化Broker的链路\n可以看到初始化的过程中分别做了 配置加载、初始化Broker、初始化Bookkeeper以及AutoRecoveryMain服务等。接下来就就看服务启动链路。\n可以看到启动入口很简洁，就是启动上面初始化好的Broker、Bookkeeper、AutoRecoveryMain，这里先看Broker的启动流程也就是276行\n可以看到这个方法非常大，里面内容非常丰富，一起来详细看看\n简单总结下这个方法，它主要创建了以下几个对象\nCoordinationServiceImpl对象，用于协调Broker选主\nBrokerService对象，这个是启动Broker对象的核心\nLoadManager对象，用于管理Broker对象的负载均衡\nSchemaStorage对象，负责处理读写schema的请求\nOffloadPoliciesImpl，负责分层存储操作\n并启动WebService服务，负责对外提供http服务\nWorkerService服务，负责处理function计算操作\n这里面的BrokerService是最核心的，在这里进去看下它创建的逻辑\n此方法主要做了下面几件事\n创建Netty服务端，用于处理生产者/消费者/代理的TCP请求 创建定期检测服务 不活跃的检测 消息过期检测 压缩检测 消费者检测 初始化五个Map容器 维护Topic对象 维护集群副本复制的客户端 维护连接当前Broker的管理流 维护Topic归属信息 维护多层级的Topic信息？ 启动DelayedDeliveryTrackerLoader跟踪延迟消息 启动Broker拦截器BrokerEntryMetadataInterceptors 启动限额管理对象BundlesQuotas 启动Netty服务端(此时Pulsar服务具备处理所有客户端请求能力) 启动定期检测服务 不活跃的检测 消息过期检测 压缩检测 消费者检测 消息积压检测 四、总结 PulsarService是Pulsar服务启动的核心类，其内置了七大重要的对象如下图\nBrokerService: 核心是启动Netty，处理客户端的TCP连接，同时通过多个Map容器维护例如Topic信息、Topic归属信息等等，除此之外还启动一批定时线程定期检测(消息过期、压缩、客户端活跃等) LoadManager: 负责处理Broker服务的负载均衡 WebService: 对外提供HTTP服务，例如管理流的操作(元数据)等 CoordinationService: 给Broker提供协调服务，例如Broker选主操作 SchemaStorage: 提供schema相关的一切服务，常见的就是schema的读写 OffloadPolicies: 提供分层存储，冷数据自动迁移到外部服务中 WorkerService: 管理Worker实例，用来执行Function计算任务 以上就是Pulsar启动流程所做的事情，其中Bookkeeper的启动以及其余的功能例如CoordinationService、SchemaStorage等等都值得单独新开一篇文章进行讲解，这里就不混在一起讲了。\n","date":"2024-09-18T10:05:57+08:00","image":"https://sherlock-lin.github.io/p/%E5%90%AF%E7%A8%8Bpulsar%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E9%AB%98%E9%80%9F%E5%90%AF%E5%8A%A8%E5%BC%95%E6%93%8E%E6%8F%AD%E7%A7%98%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%B7%A8%E5%85%BD%E7%9A%84%E8%AF%9E%E7%94%9F/image-20240913141630196_hu5121917752973153749.png","permalink":"https://sherlock-lin.github.io/p/%E5%90%AF%E7%A8%8Bpulsar%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E9%AB%98%E9%80%9F%E5%90%AF%E5%8A%A8%E5%BC%95%E6%93%8E%E6%8F%AD%E7%A7%98%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%B7%A8%E5%85%BD%E7%9A%84%E8%AF%9E%E7%94%9F/","title":"启程Pulsar：深入剖析高速启动引擎，揭秘消息中间件巨兽的诞生"},{"content":"一、Pulsar客户端简析 pulsar服务是经典的C/S架构，由客户端和服务端构成。服务端提供处理读写请求服务，客户端负责发起读写请求。pulsar将客户端按照读写分成了生产者和消费者，但是无论怎么分，它们本质上都是Pulsar客户端并有很多相同的地方，本篇就针对客户端进行分析。\n下图是客户端的组成部分\nConnectionPool连接池(值得深挖)\n连接池是客户端的核心，维护着跟服务端的TCP连接，客户端的读写(生产者/消费者)都强依赖网络连接，因为最后数据还是要通过底层的网络进行通信\nLookupService\n具有路有属性的客户端角色，负责查找Topic归属的Broker节点\n线程池\n事件线程池\n外部线程池\n内部线程池\n二、创建客户端对象 1、创建流程 先看看下面一段代码\n这是通过生产者写数据到Pulsar服务端的case，相信大家不会感到陌生，其中34行就是创建客户端的逻辑，因此就从这里进行跟踪\n通过跟踪PulsarClient.builder方法可以看到这是一个静态方法，可以看到返回值是ClientBuilder对象，具体实现逻辑咱们先不关心，咱们只需要知道这个方法会创建一个ClientBuilder就够了，结合之前的创建的逻辑可以知道最终会走到ClientBuilder#build方法\nbuild方法中核心就是50行的通过PulsarClientImpl构造函数创建客户端对象，其他的都是一些配置校验工作。下面就直接进入PulsarClientImpl的构造函数进行分析\n这个构造方法虽然逻辑不少，但实际上重要的就是201行的初始化\n三、创建网络连接池 1. 初始化 这里的ClientCnx对象非常重要，后面再细讲。现在先继续跟踪构造函数的逻辑\n这里初始化了Netty客户端，在跟外部创建网络连接时直接通过Netty客户端进行即可。客户端初始化阶段，网络连接池基本上就做了这些事，那么它是怎么工作的呢，让我们来往下看\n2. 工作流程 在创建生产者对象时，会走到PulsarClientImpl#getConnection这个方法，从这里开始进行分析\n此方法第一行是通过Lookup服务查找Topic归属的Broker节点信息，Lookup相关的感兴趣的可以看 Apache Pulsar源码解析之Lookup机制 这篇文章，在969行会将Lookup查到的Broker地址进行创建网络连接\n可以看到获取网络连接方法最终会调用ConnectionPool类的方法进行获取\n这里会根据参数randomKey(Broker IP地址)进行网络连接的复用，如果还没创建的话则进行创建，同时检查清理不可用的网络连接进行释放\n调用createConnection方法进行连接创建，同时也在通道创建通道成功后绑定监听器，在通道被关闭时一起关闭当前的网络连接。\n将unresolvedPhysicalAddress解析出正确的地址，现在还不太清楚已经有了logicalAddress(目标Broker地址)，还解析这个的用途是什么\n调用connectToAddress方法创建网络连接，如果出现异常时，如果服务端在Lookup阶段返回不止一个地址，那么就尝试跟下一个地址创建网络连接\n这里应该就很熟悉了，就是通过Netty客户端创建跟目标Broker节点的TCP连接。\n3. 小结 连接池是一个ConcurrentHashMap，key是IP+端口，value是ClientCnx来缓存这些连接。ClientCnx除了管理连接还管理所有的业务命令，例如发送消息是Send命令，服务端会对应一个handleSend方法来处理这个命令。\n连接池是客户端最重要的内容，无论是生产者还是消费者在创建的时候，都会去连接池获取/创建跟Broker的网络连接，后续的数据读写本质上都是通过Netty的channel进行的，因此可见它是相当重要的。连接池的设计将网络连接跟其他读写功能剥离开来达到职责分离的效果，同时通过池化概念，在多个生产者/消费者情况下不仅节约了网络连接的创建，同时还提升网络连接创建的性能(复用思路)。\n四、其他功能 1. LookupService Lookup机制工作逻辑在 Apache Pulsar源码解析之Lookup机制 这篇文章已经说明了，这里主要一起看看它的创建逻辑。在PulsarClientImpl构造函数中，我们能看到以下代码\nPulsar支持两种Lookup实现，一种是通过http协议去跟服务端通信，另一种是通过二进制方式查询(性能更好)。本次就跟着比较常见的http方式进行分析\n通过构造函数可以看到是通过创建HttpClient对象进行的，这里我一开始以为是用的开源的HttpClient就没继续跟，后来在调试的时候跟进去才发现，这是Pulsar 自定义对象\n跟踪进去看，其他都是赋值、安全检查的工作，核心在161行的httpClient创建。 这里可以看到是创建的AsyncHttpClient对象，这是一个封装Netty的async-http-client-2.12.1.jar的外部包，这是支持异步处理的高性能HTTP工具包\n2. MemoryLimitController 由于MemoryLimitController的内容不多，就看看它的创建以及工作流程\n在PulsarClientImpl的构造函数中可以看到会调用MemoryLimitController构造函数进行创造，从这里进去分析\n构造逻辑很简单，就只有赋值操作，到这里就成功创建MemoryLimitController对象了，那就再看看它是怎么工作的吧。MemoryLimitController的核心工作逻辑是checkTrigger方法\n这里会调用trigger的run方法，这个trigger的逻辑其实就是创建MemoryLimitController的构造方法第三个参数，也就是上面的reduceConsumerReceiverQueueSize方法，那么继续跟踪\n循环调用所有的消费者的reduceCurrentReceiverQueueSize方法，也就是说MemoryLimitController目前只能限制消费者的内存。\n这里的限制逻辑相当于将所有消费者的接收队列的容量减半，避免队列中的数据占用过多内存。由此可见MemoryLimitController目前做的还是比较小范围的内存控制，并且整体逻辑并不复杂。\n3. 线程池 客户端对象初始化时候，会创建三个线程池，现在就来分析下它们的用途\n首先是externalExecutorProvider线程池通过查看调用的方法，可以看到是被消费者使用，里面的工作线程主要是用于消费服务端的消息\n再来看看internalExecutorProvider线程池，可以看到是用来是一些跟偏向Pulsar系统层面的工作\n至于scheduledExecutorProvider相信就更简单了，顾名思义都是处理一些周期性的任务，例如下面的周期性的同步Topic分区信息到客户端\n五、总结 PulsarClient中的EventLoopGroup负责创建TCP连接，ConnectionPool对象负责管理连接，在创建ConnectionPool对象时会通过EventLoopGroup创建连接。 PulsarClient使用Netty来创建TCP连接，并管理一个连接池和两个线程池，所有Producer和Consumer都会复用PulsarClient的连接池和线程池，这样可以避免客户端创建过多的连接和线程。因此通常一个进程中只创建一个PulsarClient，每个Topic可以自己单独创建Producer和Consumer。 由于Broker使用了Reactor模型，单线程只负责转发事件，而数据的读取、解码、处理等都是在工作线程中完成的，也就是服务端所有请求都是异步完成的；客户端Producer/Consumer都是异步的，因此不存在单连接的性能问题。 PulsarClient默认只会与每个Broker建立一个连接，如果觉得不够可以通过配置来调大。 ClientCnx非常重要，这是netty的入队处理逻辑类。后续会专门针对这个类进行解析 ","date":"2024-07-17T10:34:21+08:00","image":"https://sherlock-lin.github.io/p/pulsarclient%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/image-20240715212010651_hu12175125588812162081.png","permalink":"https://sherlock-lin.github.io/p/pulsarclient%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","title":"PulsarClient源码解析"},{"content":"一、介绍 在软件设计中，为了方便能够应对不同的场景，一般在一些容易有差异的环节会考虑允许用户自定义逻辑，拦截器就是其中的一种实现方式，像Spring、Kafka、Pulsar等都支持这种方式。流程简化起来就如下图，客户端跟服务端的写消息请求和接收请求都要先通过一遍拦截器，因此用户都过自定义拦截器逻辑就能以一种无侵入、规范化的方式来改动消息发送以及处理响应的行为。\n二、使用 1. ProducerInterceptor接口 ProducerInterceptor是Pulsar提供的接口，通过实现该接口用户可以在消息发送和发送成功阶段注入自定义的逻辑来扩展Pulsar客户端的能力，进而优雅的解决某些场景的问题。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @InterfaceAudience.Public @InterfaceStability.Stable public interface ProducerInterceptor extends AutoCloseable { void close(); boolean eligible(Message message); Message beforeSend(Producer producer, Message message); void onSendAcknowledgement( Producer producer, Message message, MessageId msgId, Throwable exception); default void onPartitionsChange(String topicName, int partitions) { } } 这里针对这五个方法大概介绍下\nclose：由于该接口实现了AutoCloseable，因此也要定义生产者关闭时要释放的资源，如果没有就空着 eligible：判断拦截器针对那些消息生效，默认false不生效。这个相当于Java8 Stream里的filter，属于职责分离的设计 beforeSend：在每条消息要发送时会调用此方法，因此如果在发送前想做点什么可以考虑在这里实现 onSendAcknowledgement：在每条消息消息发送服务端响应后(无论成功失败)会调用此方法 onPartitionsChange：在分区数有变动的时候会调用这里的逻辑。这是3.2版本新加的逻辑，2.8以及之前的版本没有此接口 2. 实现之统计 这里对生产者累计发送的消息条数进行统计，实现逻辑如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 public class SherlockCountProducerInterceptor implements ProducerInterceptor { private AtomicLong count = new AtomicLong(1); @Override public void close() { } @Override public boolean eligible(Message message) { return true; } @Override public Message beforeSend(Producer producer, Message message) { System.out.println(\u0026#34;累计发送消息条数：\u0026#34;+count.getAndIncrement()); return message; } @Override public void onSendAcknowledgement(Producer producer, Message message, MessageId msgId, Throwable exception) { } } 逻辑比较简单，其实就是通过一个计数器在每次发送时进行加1即可，并且在eligible中返回true也就是对所有发送的消息都生效，每条消息在发送前都会调用一次beforeSend方法进行自增操作并打印出来。\n现在拦截器的逻辑已经定义好了，接下来怎么使用呢，请继续往下看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public static void customInterceptorProducer() throws Exception { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(Schema.STRING) .topic(\u0026#34;sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_3\u0026#34;) .intercept(new SherlockCountProducerInterceptor())\t//拦截器生效逻辑 .create(); for (int i = 0; i \u0026lt; 200; i++) { producer.send(\u0026#34;hello java API pulsar:\u0026#34;+i+\u0026#34;, 当前时间为：\u0026#34;+new Date()); } producer.close(); pulsarClient.close(); } 上述就是使用拦截器的case，通过这种方式就能轻松的定义所需要注入的逻辑。上述代码执行后输出如下\n可以看到我们通过拦截器完成了消息发送的统计功能，可以发散设想一想，像根据不同key进行分组统计、统计某个时间段消息发送失败的条数等功能也同样可以通过拦截器实现。\n3. 实现之二次处理 实现统计感觉还不得劲，再折腾一个。假设咱们的生产者中会发送很多地区的消息，这些消息有些是中国的，有些是新加坡的，有些是巴西的，这个时候它们的时间就有歧义了，因为不同时区的时间是有差异的，那咱们尝试用拦截器来实现一下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 public class SherlockAdapterTimeProducerInterceptor implements ProducerInterceptor { @Override public void close() { } @Override public boolean eligible(Message message) { // if (\u0026#34;V3\u0026#34;.equals(String.valueOf(message.getSchemaVersion()))) { // return true; // } if (\u0026#34;Singapore\u0026#34;.equals(message.getKey())) { System.out.println(\u0026#34;这条消息是新加坡地区的，进行处理！\u0026#34;); return true; } System.out.println(\u0026#34;这条消息是中国地区的，不进行处理！\u0026#34;); return false; } @Override public Message beforeSend(Producer producer, Message message) { System.out.println(\u0026#34;拦截到一条新加坡地区的消息，现在进行处理，消息内容为：\u0026#34;+message.getValue()); return message; } @Override public void onSendAcknowledgement(Producer producer, Message message, MessageId msgId, Throwable exception) { } } 上面就是demo，继续将这个拦截器应用于生产者\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public static void customInterceptorProducer() throws Exception { String serverUrl = \u0026#34;http://localhost:8080\u0026#34;; PulsarClient pulsarClient = PulsarClient.builder().serviceUrl(serverUrl).build(); Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(Schema.STRING) .topic(\u0026#34;sherlock-api-tenant-1/sherlock-namespace-1/partition_partition_topic_3\u0026#34;) .intercept(new SherlockAdapterTimeProducerInterceptor()) .create(); producer.newMessage().key(\u0026#34;China\u0026#34;).value(\u0026#34;下单动作\u0026#34;).send(); producer.newMessage().key(\u0026#34;Singapore\u0026#34;).value(\u0026#34;收藏动作\u0026#34;).send(); producer.newMessage().key(\u0026#34;China\u0026#34;).value(\u0026#34;取消动作\u0026#34;).send(); producer.newMessage().key(\u0026#34;Singapore\u0026#34;).value(\u0026#34;订阅动作\u0026#34;).send(); producer.close(); pulsarClient.close(); } 执行可以看到下面的输出\n通过输出的结果可以分析出来eligible的逻辑是生效的，针对新加坡地区的消息会进行处理，而中国的消息保持不变，所有地区的时间通过此拦截器来统一成东八区的时间。\n4. 小结 通过上述例子可以看到我们可以通过拦截器实现任意的逻辑，但是这里需要注意的是，拦截器里面尽量不要放过多的逻辑，因为这可能会影响生产者发送消息的速度，并且也容易造成处理逻辑的分散。拦截器最好是做一些校验、适配、状态记录等一些需要前置完成并且轻量级的操作。\n三、实现原理 1. 初始化流程 通过使用我们可以看到在创建生产者对象是只要通过.intercept方法传入拦截器对象即可生效，那么我们就先通过这个方法来看看实现逻辑\n1 2 3 4 5 6 7 8 9 10 ProducerBuilder\u0026lt;T\u0026gt; intercept(org.apache.pulsar.client.api.interceptor.ProducerInterceptor... interceptors); public ProducerBuilder\u0026lt;T\u0026gt; intercept(ProducerInterceptor... interceptors) { if (this.interceptorList == null) { this.interceptorList = new ArrayList(); } this.interceptorList.addAll(Arrays.asList(interceptors)); return this; } 通过代码跟踪可以看到ProducerBuilderImpl方法中会先将拦截器对象集合赋值给自己的成员变量，也就是它先保存一份在后面使用。在最终调用create方法来创建Producer时，最终会走到该类的createAsync方法，核心逻辑如下\n1 2 3 4 5 public CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; createAsync() { .... return this.interceptorList != null \u0026amp;\u0026amp; this.interceptorList.size() != 0 ? this.client.createProducerAsync(this.conf, this.schema, new ProducerInterceptors(this.interceptorList)) : this.client.createProducerAsync(this.conf, this.schema, (ProducerInterceptors)null); } } 如果用户通过.intercept方法传入了自定义的拦截器，则会调用PulsarClientImpl带有拦截器对象的构造方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 public \u0026lt;T\u0026gt; CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; createProducerAsync(ProducerConfigurationData conf, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { .... //这个方法的核心逻辑就这一行，继续往下跟踪 return this.createProducerAsync(topic, conf, schema, interceptors); } private \u0026lt;T\u0026gt; CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; createProducerAsync(String topic, ProducerConfigurationData conf, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { .... //同理，核心逻辑就这一行 producer = this.newProducerImpl(topic, -1, conf, schema, interceptors, producerCreatedFuture); } protected \u0026lt;T\u0026gt; ProducerImpl\u0026lt;T\u0026gt; newProducerImpl(....) { return new ProducerImpl(this, topic, conf, producerCreatedFuture, partitionIndex, schema, interceptors); } public ProducerImpl(PulsarClientImpl client, String topic, ProducerConfigurationData conf, CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture, int partitionIndex, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { //只有这里有用到，继续跟踪 super(client, topic, conf, producerCreatedFuture, schema, interceptors); .... } protected ProducerBase(PulsarClientImpl client, String topic, ProducerConfigurationData conf, CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { .... //对父类的成员变量进行赋值 this.interceptors = interceptors; } 通过上面的代码跟踪我们可以知道，当我们通过拦截器创建的Producer对象，它是有在内部维护一个ProducerInterceptors对象来存储我们所指定的拦截器集合的逻辑\n那么我们来看看ProducerInterceptors的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 public class ProducerInterceptors implements Closeable { .... private final List\u0026lt;ProducerInterceptor\u0026gt; interceptors;\t//存储拦截器集合逻辑 .... //在消息发送前进行触发 public Message beforeSend(Producer producer, Message message) { Message interceptorMessage = message; for (ProducerInterceptor interceptor : interceptors) { //调用拦截器的eligible方法来判断是否要对当前这条消息进行拦截处理，这个就是咱们上面实现的eligible接口 if (!interceptor.eligible(message)) { continue; } try { //循环调用拦截器集合里的每个拦截器对这条消息进行处理 interceptorMessage = interceptor.beforeSend(producer, interceptorMessage); } catch (Throwable e) { .... } } return interceptorMessage; } //逻辑跟beforeSend基本一致 public void onSendAcknowledgement(Producer producer, Message message, MessageId msgId, Throwable exception) { for (ProducerInterceptor interceptor : interceptors) { if (!interceptor.eligible(message)) { continue; } try { interceptor.onSendAcknowledgement(producer, message, msgId, exception); } catch (Throwable e) { log.warn(\u0026#34;Error executing interceptor onSendAcknowledgement callback \u0026#34;, e); } } } public void onPartitionsChange(String topicName, int partitions) { for (ProducerInterceptor interceptor : interceptors) { try { interceptor.onPartitionsChange(topicName, partitions); } catch (Throwable e) { log.warn(\u0026#34;Error executing interceptor onPartitionsChange callback \u0026#34;, e); } } } @Override public void close() throws IOException { for (ProducerInterceptor interceptor : interceptors) { try { interceptor.close(); } catch (Throwable e) { log.error(\u0026#34;Fail to close producer interceptor \u0026#34;, e); } } } } 通过上述逻辑可以看到ProducerInterceptors本质上就是个批量管理对象，符合高内聚低耦合的设计，解耦了业务逻辑循环处理的逻辑，将这些循环处理的逻辑都封装在ProducerInterceptors类里面，然后ProducerInterceptors仅对外提供触发某几个动作的api，业务只需要在哪个阶段调用这些api即可。\n2. 生效流程 在生产者消息发送阶段，最终都会走到ProducerImpl类的internalSendAsync方法，可以看到这里会调用拦截器进行处理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CompletableFuture\u0026lt;MessageId\u0026gt; internalSendAsync(Message\u0026lt;?\u0026gt; message) { //核心方法，跟踪进去 MessageImpl\u0026lt;?\u0026gt; interceptorMessage = (MessageImpl) beforeSend(message); .... } protected Message\u0026lt;?\u0026gt; beforeSend(Message\u0026lt;?\u0026gt; message) { if (interceptors != null) { //如果配置了拦截器则调用ProducerInterceptors类的beforeSend方法 return interceptors.beforeSend(this, message); } else { //如果没有配置拦截器则直接返回原消息 return message; } } 这是消息发送的处理逻辑，那如果是再消息发送结束后触发呢？一起来跟踪看下吧，首先还是从ProducerImp类的internalSendAsync方法开始看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @Override CompletableFuture\u0026lt;MessageId\u0026gt; internalSendAsync(Message\u0026lt;?\u0026gt; message) { sendAsync(interceptorMessage, new SendCallback() { .... @Override public void sendComplete(Exception e) { try { if (e != null) { stats.incrementSendFailed(); //从这里跟踪进去看看 onSendAcknowledgement(interceptorMessage, null, e); future.completeExceptionally(e); } else { onSendAcknowledgement(interceptorMessage, interceptorMessage.getMessageId(), null); future.complete(interceptorMessage.getMessageId()); stats.incrementNumAcksReceived(System.nanoTime() - createdAt); } } finally { interceptorMessage.getDataBuffer().release(); } } protected void onSendAcknowledgement(Message\u0026lt;?\u0026gt; message, MessageId msgId, Throwable exception) { if (interceptors != null) { //可以看到最终也是调用的ProducerInterceptors类的onSendAcknowledgement方法 interceptors.onSendAcknowledgement(this, message, msgId, exception); } } 这里的设计是异步回调的方式，将调用拦截器处理逻辑封装成参数传给下一层，在消息发送完成后再调用参数里指定的回调逻辑。那么什么时候触发呢，由于Pulsar客户端跟服务端是通过Netty的TCP通信的，因此直接看看PulsarDecoder的channelRead方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { .... switch (cmd.getType()) { .... case PRODUCER_SUCCESS: //写入消息被Broker处理后会忘生产者客户端通过TCP发送一条PRODUCER_SUCCESS类型的消息也就是这里，跟踪进去看看处理逻辑 checkArgument(cmd.hasProducerSuccess()); handleProducerSuccess(cmd.getProducerSuccess()); break; } } protected void handleProducerSuccess(CommandProducerSuccess success) { .... //生产者会在队列维护每条未被ack的写入请求消息，在Broker ack时会从这个队列中移除并获取回调处理逻辑 CompletableFuture\u0026lt;ProducerResponse\u0026gt; requestFuture = (CompletableFuture\u0026lt;ProducerResponse\u0026gt;) pendingRequests.remove(requestId); if (requestFuture != null) { ProducerResponse pr = new ProducerResponse(success.getProducerName(), success.getLastSequenceId(), success.getSchemaVersion(), success.hasTopicEpoch() ? Optional.of(success.getTopicEpoch()) : Optional.empty()); //调用回调逻辑 requestFuture.complete(pr); } else { .... } } 四、总结 通过使用和跟踪原理，我们对Pulsar生产者拦截器有了进一步的认识，除了生产者拦截器，Pulsar还支持Broker侧以及Bookkeeper侧的拦截器，这些放到后面再跟大家一起学习。\n","date":"2024-04-19T11:54:03+08:00","image":"https://sherlock-lin.github.io/p/%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2%E7%94%9F%E4%BA%A7%E8%80%85%E6%8B%A6%E6%88%AA%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BB%A5%E5%8F%8A%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1/image-20240419110502648_hu11315331606035570821.png","permalink":"https://sherlock-lin.github.io/p/%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2%E7%94%9F%E4%BA%A7%E8%80%85%E6%8B%A6%E6%88%AA%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BB%A5%E5%8F%8A%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1/","title":"深入探索生产者拦截器的使用以及源码设计"},{"content":"一、正文 Namespace下的Topic是分Bundle进行管理的，每个Namespace都是一个哈希环，而Bundle负责管理环上某个范围上的Topic。通过这种方式可以更好的进行Topic的管理。当某个Bundle上负责的Topic越来越多时，会导致负责该Bundle的Broker节点压力变大。因此Pulsar还提供了Bundle分裂的机制，这个机制支持自动触发以及手动触发，今天这篇文章就从源码的角度分析手动触发Bundle分裂时服务端会发生什么\n主动触发Bundle分裂操作方式\n1 2 3 4 5 6 7 8 9 10 bin/pulsar-admin namespaces bundles public/default //输出如下 { \u0026#34;boundaries\u0026#34; : [ \u0026#34;0x00000000\u0026#34;, \u0026#34;0x08000000\u0026#34;, \u0026#34;0x10000000\u0026#34;, \u0026#34;0x20000000\u0026#34;, \u0026#34;0x30000000\u0026#34;, \u0026#34;0x40000000\u0026#34;, \u0026#34;0x50000000\u0026#34;, \u0026#34;0x60000000\u0026#34;, \u0026#34;0x70000000\u0026#34;, \u0026#34;0x80000000\u0026#34;, \u0026#34;0x90000000\u0026#34;, \u0026#34;0xa0000000\u0026#34;, \u0026#34;0xb0000000\u0026#34;, \u0026#34;0xc0000000\u0026#34;, \u0026#34;0xd0000000\u0026#34;, \u0026#34;0xe0000000\u0026#34;, \u0026#34;0xf0000000\u0026#34;, \u0026#34;0xffffffff\u0026#34; ], \u0026#34;numBundles\u0026#34; : 17 } //指定某个bundle进行分裂 bin/pulsar-admin namespaces split-bundle --bundle 0x00000000_0x10000000 public/default 二、源码解析 Pulsar管理流相关的操作都是通过HTTP的方式，因为需要支持多种客户端类型(http、client、cli)。服务端处理这些操作都在admin模块下，如下图，本次要聊的bundle分裂就在Namespaces方法中\n首先从Namespaces的splitNamespaceBundle进行跟踪\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public void splitNamespaceBundle( .... @QueryParam(\u0026#34;splitAlgorithmName\u0026#34;) String splitAlgorithmName, //指定bundle分裂算法 @ApiParam(\u0026#34;splitBoundaries\u0026#34;) List\u0026lt;Long\u0026gt; splitBoundaries) { //校验参数格式以及是否存在对应的namespace validateNamespaceName(tenant, namespace); //异步进行分裂操作 internalSplitNamespaceBundleAsync(bundleRange, authoritative, unload, splitAlgorithmName, splitBoundaries) .thenAccept(__ -\u0026gt; { .... }) .exceptionally(ex -\u0026gt; { .... }); } 可以看到最外层只是做些参数校验，那么就继续跟踪internalSplitNamespaceBundleAsync方法，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 protected CompletableFuture\u0026lt;Void\u0026gt; internalSplitNamespaceBundleAsync(String bundleName, boolean authoritative, boolean unload, String splitAlgorithmName, List\u0026lt;Long\u0026gt; splitBoundaries) { return validateSuperUserAccessAsync() //权限校验 .thenAccept(__ -\u0026gt; { checkNotNull(bundleName, \u0026#34;BundleRange should not be null\u0026#34;); log.info(\u0026#34;[{}] Split namespace bundle {}/{}\u0026#34;, clientAppId(), namespaceName, bundleName); //获取当前集群所支持的bundle分裂算法，这里是硬编码的固定四种 List\u0026lt;String\u0026gt; supportedNamespaceBundleSplitAlgorithms = pulsar().getConfig().getSupportedNamespaceBundleSplitAlgorithms(); //此处省去参数检查逻辑 .... } }) .thenCompose(__ -\u0026gt; { //此处省去参数检查逻辑 .... }) .thenCompose(__ -\u0026gt; validatePoliciesReadOnlyAccessAsync()) //权限校验 .thenCompose(__ -\u0026gt; getBundleRangeAsync(bundleName)) //获取要分裂的bundle的范围 .thenCompose(bundleRange -\u0026gt; { return getNamespacePoliciesAsync(namespaceName) .thenCompose(policies -\u0026gt; //1. 校验Bundle的范围是否有效 //2.判断当前Broker节点是否负责这个bundle的管理，如果不是则重定向 validateNamespaceBundleOwnershipAsync(namespaceName, policies.bundles, bundleRange,authoritative, false)) //核心方法就是这里的 NamespaceService.splitAndOwnBundle .thenCompose(nsBundle -\u0026gt; pulsar().getNamespaceService().splitAndOwnBundle(nsBundle, unload, pulsar().getNamespaceService() .getNamespaceBundleSplitAlgorithmByName(splitAlgorithmName), splitBoundaries)); }); } 接下来就是进入NamespaceService类的splitAndOwnBundle方法，NamespaceService也是Pulsar比较重要的一个类，这里先继续跟踪分割bundle的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 public CompletableFuture\u0026lt;Void\u0026gt; splitAndOwnBundle(NamespaceBundle bundle, boolean unload, NamespaceBundleSplitAlgorithm splitAlgorithm, List\u0026lt;Long\u0026gt; boundaries) { //如果实现了自定义的分割逻辑则使用自定义的 if (ExtensibleLoadManagerImpl.isLoadManagerExtensionEnabled(config)) { return ExtensibleLoadManagerImpl.get(loadManager.get()) .splitNamespaceBundleAsync(bundle, splitAlgorithm, boundaries); } final CompletableFuture\u0026lt;Void\u0026gt; unloadFuture = new CompletableFuture\u0026lt;\u0026gt;(); final AtomicInteger counter = new AtomicInteger(BUNDLE_SPLIT_RETRY_LIMIT); //核心流程流程 splitAndOwnBundleOnceAndRetry(bundle, unload, counter, unloadFuture, splitAlgorithm, boundaries); return unloadFuture; } void splitAndOwnBundleOnceAndRetry(NamespaceBundle bundle, boolean unload, AtomicInteger counter, CompletableFuture\u0026lt;Void\u0026gt; completionFuture, NamespaceBundleSplitAlgorithm splitAlgorithm, List\u0026lt;Long\u0026gt; boundaries) { //获取bundle分割配置 BundleSplitOption bundleSplitOption = getBundleSplitOption(bundle, boundaries, config); //根据配置来选择对应的分割算法进行分割 splitAlgorithm.getSplitBoundary(bundleSplitOption).whenComplete((splitBoundaries, ex) -\u0026gt; { CompletableFuture\u0026lt;List\u0026lt;NamespaceBundle\u0026gt;\u0026gt; updateFuture = new CompletableFuture\u0026lt;\u0026gt;(); if (ex == null) { .... try { //进行bundle分割操作 bundleFactory.splitBundles(bundle, splitBoundaries.size() + 1, splitBoundaries) .thenAccept(splitBundles -\u0026gt; { .... Objects.requireNonNull(splitBundles.getLeft()); Objects.requireNonNull(splitBundles.getRight()); checkArgument(splitBundles.getRight().size() == splitBoundaries.size() + 1, \u0026#34;bundle has to be split in \u0026#34; + (splitBoundaries.size() + 1) + \u0026#34; bundles\u0026#34;); NamespaceName nsname = bundle.getNamespaceObject(); .... try { // 检查确保每个Bundle都有对应的Broker负责 for (NamespaceBundle sBundle : splitBundles.getRight()) { Objects.requireNonNull(ownershipCache.tryAcquiringOwnership(sBundle)); } //更新Bundle信息，毕竟Bundle已经分裂好了，相关的一些元数据要同步更新 updateNamespaceBundles(nsname, splitBundles.getLeft()).thenCompose(__ -\u0026gt; updateNamespaceBundlesForPolicies(nsname, splitBundles.getLeft())) .thenRun(() -\u0026gt; { bundleFactory.invalidateBundleCache(bundle.getNamespaceObject()); updateFuture.complete(splitBundles.getRight()); }).exceptionally(ex1 -\u0026gt; { .... }); } catch (Exception e) { .... } }); } catch (Exception e) { .... } } else { updateFuture.completeExceptionally(ex); } updateFuture.whenCompleteAsync((r, t)-\u0026gt; { if (t != null) { // 失败则重试几次 if ((t.getCause() instanceof MetadataStoreException.BadVersionException) \u0026amp;\u0026amp; (counter.decrementAndGet() \u0026gt;= 0)) { pulsar.getExecutor().schedule(() -\u0026gt; pulsar.getOrderedExecutor() .execute(() -\u0026gt; splitAndOwnBundleOnceAndRetry( bundle, unload, counter, completionFuture, splitAlgorithm, boundaries)), 100, MILLISECONDS); } else if (t instanceof IllegalArgumentException) { completionFuture.completeExceptionally(t); } else { // Retry enough, or meet other exception String msg2 = format(\u0026#34; %s not success update nsBundles, counter %d, reason %s\u0026#34;, bundle.toString(), counter.get(), t.getMessage()); LOG.warn(msg2); completionFuture.completeExceptionally(new ServiceUnitNotReadyException(msg2)); } return; } //更新bundle的状态 getOwnershipCache().updateBundleState(bundle, false) .thenRun(() -\u0026gt; { // update bundled_topic cache for load-report-generation pulsar.getBrokerService().refreshTopicToStatsMaps(bundle); loadManager.get().setLoadReportForceUpdateFlag(); // release old bundle from ownership cache pulsar.getNamespaceService().getOwnershipCache().removeOwnership(bundle); completionFuture.complete(null); if (unload) { // Unload new split bundles, in background. This will not // affect the split operation which is already safely completed r.forEach(this::unloadNamespaceBundle); } onNamespaceBundleSplit(bundle); }) .exceptionally(e -\u0026gt; { .... }); }, pulsar.getOrderedExecutor()); }); } 上面这个方法的逻辑比较丰富，但核心的分割流程实际上是调用的NamespaceBundleFactory的splitBundles进行的，这里就继续跟踪NamespaceBundleFactory的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 public CompletableFuture\u0026lt;Pair\u0026lt;NamespaceBundles, List\u0026lt;NamespaceBundle\u0026gt;\u0026gt;\u0026gt; splitBundles( NamespaceBundle targetBundle, int argNumBundles, List\u0026lt;Long\u0026gt; splitBoundaries) { //判断当前bundle是否支持分裂 checkArgument(canSplitBundle(targetBundle), \u0026#34;%s bundle can\u0026#39;t be split further since range not larger than 1\u0026#34;, targetBundle); .... NamespaceName nsname = targetBundle.getNamespaceObject(); final int numBundles = argNumBundles; return bundlesCache.get(nsname).thenApply(sourceBundle -\u0026gt; { final int lastIndex = sourceBundle.partitions.length - 1; //重新创建数组保存哈希环的节点，因为bundle分裂后环上的节点会增多 final long[] partitions = new long[sourceBundle.partitions.length + (numBundles - 1)]; int pos = 0; int splitPartition = -1; final Range\u0026lt;Long\u0026gt; range = targetBundle.getKeyRange(); for (int i = 0; i \u0026lt; lastIndex; i++) { //遍历当前Namespace的所有Bundle来找到需要进行分裂的目标Bundle if (sourceBundle.partitions[i] == range.lowerEndpoint() \u0026amp;\u0026amp; (range.upperEndpoint() == sourceBundle.partitions[i + 1])) { splitPartition = i; long minVal = sourceBundle.partitions[i]; partitions[pos++] = minVal; if (splitBoundaries == null || splitBoundaries.size() == 0) { long maxVal = sourceBundle.partitions[i + 1]; //numBundles就是要分割成的份数，这里相当于将原先Bundle负责的范围平均分给多个新的Bundle long segSize = (maxVal - minVal) / numBundles; long curPartition = minVal + segSize; for (int j = 0; j \u0026lt; numBundles - 1; j++) { partitions[pos++] = curPartition; curPartition += segSize; } } else { for (long splitBoundary : splitBoundaries) { partitions[pos++] = splitBoundary; } } } else { partitions[pos++] = sourceBundle.partitions[i]; } } partitions[pos] = sourceBundle.partitions[lastIndex]; if (splitPartition != -1) { // keep version of sourceBundle //根据上面旧的Bundle范围划分来分裂出多个新的bundle NamespaceBundles splitNsBundles = new NamespaceBundles(nsname, this, sourceBundle.getLocalPolicies(), partitions); List\u0026lt;NamespaceBundle\u0026gt; splitBundles = splitNsBundles.getBundles().subList(splitPartition, (splitPartition + numBundles)); return new ImmutablePair\u0026lt;\u0026gt;(splitNsBundles, splitBundles); } return null; }); } 到这里基本就结束，这条链路主要是Broker将原先的哈希环中的某一个范围拆分成多个范围的逻辑，这里保留几个问题给读者思考\nBundle分裂后是否会涉及到数据的迁移？ Bundle分裂算法有四种，区别是什么？ 三、参考文献 官方文档 ","date":"2024-04-16T10:34:57+08:00","image":"https://sherlock-lin.github.io/p/%E4%B8%BB%E5%8A%A8%E8%A7%A6%E5%8F%91bundle%E5%88%86%E8%A3%82%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/image-20240416181912380_hu8808141309202124509.png","permalink":"https://sherlock-lin.github.io/p/%E4%B8%BB%E5%8A%A8%E8%A7%A6%E5%8F%91bundle%E5%88%86%E8%A3%82%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","title":"主动触发Bundle分裂服务端流程源码解析"},{"content":"引言 在看了 你真的了解Pulsar的消息保留、积压、TTL策略吗 后，相信有不少对技术充满热情的小伙伴会疑惑，Pulsar的TTL又是怎么去失信的呢？今天就让我们一起来看看吧\n在看下面的文章时我们要带着以下几个问题\nTTL是什么时候触发的 TTL机制被触发后会发生什么 过期的消息会立马被删除吗 整体流程 在跟踪源码前先看看这张图，每个Broker内部都会有一个周期定时线程任务，每隔一段时间都会触发TTL任务。TTL任务会轮询当前Broker所管理的所有Topic中的所有订阅者，因为每个订阅者都会在Broker维护一个消费游标，因此Broker会根据用户配置的过期时间到轮询检查游标，看看有哪些消息是没被消费但是已经过了TTL的并会将游标移动到其的左侧(从实现的层面相当于表示它已经被消费了)，从而达到这些过了TTL的消息允许被删除的效果，最终再将游标的信息持久化到Bookkeeper中进行保存。\n触发TTL流程 那么直接来跟踪下触发TTL的流程吧，先从Broker的启动方法start开始看。可以看到启动的时候还会开启一系列定时任务，这里只看TTL相关的，跟踪进去startMessageExpiryMonitor方法可以看到messageExpiryMonitor其实是JDK提供的线程池ScheduledExecutorService类，在这里通过scheduleAtFixedRate方法周期性的执行过期任务检测，间隔从Broker配置中进行读取的，默认是每隔5分钟一次\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public void start() throws Exception { .... this.startInactivityMonitor(); //启动定时消息过期检测任务(TTL) this.startMessageExpiryMonitor(); this.startCompactionMonitor(); this.startConsumedLedgersMonitor(); this.startBacklogQuotaChecker(); this.updateBrokerPublisherThrottlingMaxRate(); this.updateBrokerDispatchThrottlingMaxRate(); this.startCheckReplicationPolicies(); this.startDeduplicationSnapshotMonitor(); } protected void startMessageExpiryMonitor() { int interval = pulsar().getConfiguration().getMessageExpiryCheckIntervalInMinutes(); messageExpiryMonitor.scheduleAtFixedRate(this::checkMessageExpiry, interval, interval, TimeUnit.MINUTES); } TTL启动流程 通过上一节可以看到Pulsar的TTL触发就是通过JDK的定时线程池来实现的，Broker会周期性的调用checkMessageExpiry方法进行处理，因此现在就从这个方法进行跟踪，可以看到最终会调用到Topic的checkMessageExpiry方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 public void checkMessageExpiry() { //进去看看forEachTopic的实现 forEachTopic(Topic::checkMessageExpiry); } public void forEachTopic(Consumer\u0026lt;Topic\u0026gt; consumer) { //topics是Broker维护的一个Map结构，用于记录当前Broker所维护的Topic信息 //这里使用了Java8的Consumer，相当于闭包的设计，让内部的所有Topic都执行checkMessageExpiry方法 topics.forEach((n, t) -\u0026gt; { Optional\u0026lt;Topic\u0026gt; topic = extractTopic(t); topic.ifPresent(consumer::accept); }); } 由于Topic只是接口，因此我们看它最常用的实现类也就是我们熟悉的PersistentTopic类的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 public void checkMessageExpiry() { //从topic配置中获取消息的TTL配置，这个跟上面的配置是有区别的，这里代表的是消息的有效期 int messageTtlInSeconds = topicPolicies.getMessageTTLInSeconds().get(); //通过这里的逻辑可以知道如果配置成0就不会过期 if (messageTtlInSeconds != 0) { //循环调用当前Topic订阅者检测过期 subscriptions.forEach((__, sub) -\u0026gt; { if (!isCompactionSubscription(sub.getName())) { //进行具体的检测动作，从这里继续跟踪 sub.expireMessages(messageTtlInSeconds); } }); } } public boolean expireMessages(int messageTTLInSeconds) { //获取当前积压消息的条数(指的就是未被消费的消息条数) long backlog = getNumberOfEntriesInBacklog(false); //如果没有消息积压就代表所有消息都被成功消费并且游标此时已经处于最左端，因此没必要再做TTL的检测了 if (backlog == 0 || (dispatcher != null \u0026amp;\u0026amp; dispatcher.isConsumerConnected() \u0026amp;\u0026amp; backlog \u0026lt; MINIMUM_BACKLOG_FOR_EXPIRY_CHECK \u0026amp;\u0026amp; !topic.isOldestMessageExpired(cursor, messageTTLInSeconds))) { // don\u0026#39;t do anything for almost caught-up connected subscriptions return false; } this.lastExpireTimestamp = System.currentTimeMillis(); //更新过期消息的下标，从这里继续跟踪 return expiryMonitor.expireMessages(messageTTLInSeconds); } public boolean expireMessages(int messageTTLInSeconds) { .... // 这里进去进行过期检查 checkExpiryByLedgerClosureTime(cursor, messageTTLInSeconds); .... } private void checkExpiryByLedgerClosureTime(ManagedCursor cursor, int messageTTLInSeconds) { //参数校验，可是这里为什么又做了小于0的判断，而外层只判断了等于0，可否用统一的参数校验工具进行校验呢？ if (messageTTLInSeconds \u0026lt;= 0) { return; } if (cursor instanceof ManagedCursorImpl managedCursor) { ManagedLedgerImpl managedLedger = (ManagedLedgerImpl) managedCursor.getManagedLedger(); //获得游标当前标记的可删除位置 Position deletedPosition = managedCursor.getMarkDeletedPosition(); //获取当前未被成功消费的积压日志信息，按Ledger进行排序 SortedMap\u0026lt;Long, MLDataFormats.ManagedLedgerInfo.LedgerInfo\u0026gt; ledgerInfoSortedMap = managedLedger.getLedgersInfo().subMap(deletedPosition.getLedgerId(), true, managedLedger.getLedgersInfo().lastKey(), true); MLDataFormats.ManagedLedgerInfo.LedgerInfo info = null; // 查询最接近现在的第一个未过期Ledger，那么其上一个Ledger一定是过期的并且其之前的都是过期的 for (MLDataFormats.ManagedLedgerInfo.LedgerInfo ledgerInfo : ledgerInfoSortedMap.values()) { if (!ledgerInfo.hasTimestamp() || !MessageImpl.isEntryExpired(messageTTLInSeconds, ledgerInfo.getTimestamp())) { break; } info = ledgerInfo; } //如果info不为空说明一定存在已经处于过期的Ledger也就是过期的消息集合体 if (info != null \u0026amp;\u0026amp; info.getLedgerId() \u0026gt; -1) { //获取具体过期的位置 PositionImpl position = PositionImpl.get(info.getLedgerId(), info.getEntries() - 1); if (((PositionImpl) managedLedger.getLastConfirmedEntry()).compareTo(position) \u0026lt; 0) { findEntryComplete(managedLedger.getLastConfirmedEntry(), null); } else { //这里进去检查过期位置 findEntryComplete(position, null); } } } } 通过上面的代码跟踪可以看到在获取到过期的消息位置后，最终调用了PersistentMessageExpiryMonitor类的findEntryComplete方法，那么咱们接下来跟着进去看看都发生了哪些有意思的事情吧\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 public void findEntryComplete(Position position, Object ctx) { ....\t//通过方法命名可以推测是通过标记游标的删除位置到达到TTL的效果，继续跟踪进去看看 cursor.asyncMarkDelete(position, cursor.getProperties(), markDeleteCallback, cursor.getNumberOfEntriesInBacklog(false)); .... } public void asyncMarkDelete(final Position position, Map\u0026lt;String, Long\u0026gt; properties, final MarkDeleteCallback callback, final Object ctx) { .... //异步标记删除位置 internalAsyncMarkDelete(newPosition, properties, callback, ctx); } protected void internalAsyncMarkDelete(final PositionImpl newPosition, Map\u0026lt;String, Long\u0026gt; properties, final MarkDeleteCallback callback, final Object ctx) { .... synchronized (pendingMarkDeleteOps) { switch (STATE_UPDATER.get(this)) { .... case Open: if (PENDING_READ_OPS_UPDATER.get(this) \u0026gt; 0) { pendingMarkDeleteOps.add(mdEntry); } else { //进行标记删除 internalMarkDelete(mdEntry); } break; .... } } } void internalMarkDelete(final MarkDeleteEntry mdEntry) { .... //持久化游标信息到Bookkeeper persistPositionToLedger(cursorLedger, mdEntry, cb); } void persistPositionToLedger(final LedgerHandle lh, MarkDeleteEntry mdEntry, final VoidCallback callback) { PositionImpl position = mdEntry.newPosition; PositionInfo pi = PositionInfo.newBuilder().setLedgerId(position.getLedgerId()) .setEntryId(position.getEntryId()) .addAllIndividualDeletedMessages(buildIndividualDeletedMessageRanges()) .addAllBatchedEntryDeletionIndexInfo(buildBatchEntryDeletionIndexInfoList()) .addAllProperties(buildPropertiesMap(mdEntry.properties)).build(); .... requireNonNull(lh); byte[] data = pi.toByteArray(); //调用Bookkeeper客户端将游标信息写入 lh.asyncAddEntry(data, (rc, lh1, entryId, ctx) -\u0026gt; { .... }, null); } 通过上面的跟踪可以看得到最终是通过Bookkeeper的客户端对象LedgerHandle的asyncAddEntry方法将游标信息持久化到了Bookkeeper，这里就不继续跟踪下去了，因为已经到了TTL左右范围的尽头了。\n总结 以上就是Pulsar中TTL的实现源码流程，我在这个过程中尽量省略了一些非必要的逻辑，主要是跟踪了主线，像一些地方也是值得跟踪的，感兴趣的伙伴可以自行跟踪。同时在这里我们可以看到，TTL过程只移动了游标位置并不涉及到数据的删除，说明在Pulsar的设计中将两者进行了分离，这是一种很好的职责设计思路，各自负责自己的一那部分。一方面在程序维护时可以避免改动引起一些相关性不那么大的影响，另一方面对于程序执行的整体性能来说也是比较高效的。\n","date":"2024-04-02T10:34:56+08:00","image":"https://sherlock-lin.github.io/p/pulsar%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8Bttl/image-20240331111412370_hu135517140155982200.png","permalink":"https://sherlock-lin.github.io/p/pulsar%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8Bttl/","title":"Pulsar源码解析之TTL"},{"content":"引言 任何东西都有生命周期，就像沙丁鱼罐头🥫也会过期一样，咱们的消息本身也是有生命周期的，因此像Pulsar这样的流平台/消息队列也提供了Retention、Backlog和TTL机制。\n默认清理机制 任何机制的出现都是有背景的，因此我们要先了解这三个机制出现之前的情况，才能分析出它们具体分别解决的事什么问题。首先看下图，生产者往Broker不断往Broker中写入消息，这些消息在Broker中会按照顺序从左到右进行存储的，新写入的消息是不断的在左边进行新增。消费者也是从右往左进行消费的，在Broker中会维持一个游标记录消费的情况，通过此游标Broker才可以对消息进去区分，哪些消息是可清理回收的，哪些消息是目前还不能清理回收的。这个默认清理机制符合我们使用消息队列的一部分业务场景\nRetention机制 在咱们理解了默认清理机制时，我们会有个疑问🤔️，如果消息一被成功消费就被删除，那么如果Broker的下游处理有问题需要从头消费或者从指定的某个位置消费的话，目前的清理机制是完成没办法支持的。而Pulsar设计的目标是一个大一统的流平台/消息队列，肯定是不允许这种情况的出现，因此引入了Retention机制。\n通过下图可以看到消息发布订阅过程，在默认的清理机制中游标右侧的消息是允许被删除的，但是配置Retention的情况则会按照机制进行保存一段时间。而Cursor游标左边的这些消息表示还没被消费者消费，因此即便没有配置 Retention机制这些消息也不会被删除。Retention 支持数据大小维度以及时间维度两种方式进行配置，不过它目前仅支持针对Namespace级别进行生效。\n在设置Retentio机制时可以通过defaultRetentionSizeInMB和defaultRetentionTimeInMinutes同时进行配置，此时会有以下几种情况\n时间维度 文件大小为度 消息保留效果 -1 -1 永久保存 -1 \u0026gt;0 基于数据大小进行保留 \u0026gt;0 -1 基于时间进行保留 0 0 默认配置，当消息被消费后不会被保留 0 \u0026gt;0 无效配置 \u0026gt;0 0 无效配置 \u0026gt;0 \u0026gt;0 当消息已经被消费或者没有消费者订阅时，满足其中一个条件的则不被保留 Backlog 限额机制 Retention机制解决了消息“提前”删除的问题，那么可以高枕无忧了吗？让我们来想想下面这种场景，就是未被成功消费的消息是会一直保留在Pulsar中的，那么如果写入速度一直远大于消费速度，是不是就相当于一个蓄水池入水速度远大于出水速度，最终这个池子会满一样，Pulsar的磁盘也会被打满从而影响服务的稳定性。\nBacklog的原理其实不复杂，相当于在蓄水池中标记一个水位线，当蓄水池的高度到达这条水位线时则触发报警，工作人员根据这个报警来做出相对应的处理。Pulsar其实也是一样，假如我们配置Backlog限额的大小是两条消息的大小，那么如下图，此时如果已经有两条消息未被消费，再有一条新的消息进行尝试写入，就会触发Pulsar的报警策略进行处理。\n积压警报策略有以下三种\nproducer_request_hold：生产者会暂时等待一段时间，并在之后重新进行消息发送 producer_exception：向生产者抛出异常，让生产者进行处理如停止或暂停往Pulsar进行消息的写入 consumer_backlog_eviction：Broker会清理一些积压的数据 Backlog 机制是针对Namespace级别限额，同样是支持通过数据大小以及时间两种维度配置。\nTime to live (TTL) 看起来Retention和Backlog机制已经基本满足我们的使用了，那么为啥还要加一个TTL呢？是社区闲着没事干整那么多东西吗，答案显然不是的。我们再来想想生活中的某些场景，如果在我们的业务场景中消息是有时效性的，例如股票最新的价格，如果这个价格信息是通过Pulsar进行传递的，那么如果这条消息及时没有被消费，在10分钟后它的价值理论上就没有那么高了，因为还会有源源不断的最新价格信息写入Pulsar，而用户更加关心的是最新的价格。因此根据业务场景给消息配置上TTL，可以更有利于Pulsar进行消息的回收以及资源的释放。通过下图我们可以看到，每一条消息都有一个“蜡烛”标记它的生命周期\n当这条消息上面的蜡烛燃尽时，即便这条消息还没有被消费，游标依然会移动到它左边将其标记为允许被删除，因为此时对于业务来说这条消息已经属于没有价值，没有在Pulsar继续保留的必要。可以看到这种方式相比Backlog的方式更加稳妥，因为这不依赖于消费者的消费情况\n综合 对于Pulsar存储而言，Backlog和TTL机制可以防止磁盘被耗尽；而Retention机制会占用磁盘来保留未来可能还会用到的消息。 对于影响范围而言，Retention机制仅针对已经成功消费的消息，Backlog和TTL仅针对未被成功消费的消息。 整体流程就是，在过了Retention配置的时间，已被成功消费的消息就会被删除；如果Pulsar的消息超过了Backlog限额则Pulsar会停止接收来自生产者的消息直到有更多可用的空间，因此针对消息数据配置TTL是可以非常好的保护Backlog限额的。 Pulsar物理存储的大小应该满足Retention和Backlog的总和，在设计集群时应该把消息需要保留多久、允许多少积压等考虑进去。 除此之外Pulsar支持分层存储，会将冷数据迁移到更加廉价的外部系统中存储，此时配置的策略会依然有效，因此应该考虑全面。 以上就是Pulsar Retention、Backlog和TTL机制的核心，可以满足流平台/消息队列的使用场景\n参考资料 官方文档\nUnderstanding Pulsar Message TTL, Backlog, and Retention\nApache Pulsar 之 TTL 与 Retention 策略\n","date":"2024-04-01T10:34:57+08:00","image":"https://sherlock-lin.github.io/p/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3pulsar%E7%9A%84%E6%B6%88%E6%81%AF%E4%BF%9D%E7%95%99%E7%A7%AF%E5%8E%8Bttl%E7%AD%96%E7%95%A5%E5%90%97/image-20240331085922332_hu13280254660255972346.png","permalink":"https://sherlock-lin.github.io/p/%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BA%86%E8%A7%A3pulsar%E7%9A%84%E6%B6%88%E6%81%AF%E4%BF%9D%E7%95%99%E7%A7%AF%E5%8E%8Bttl%E7%AD%96%E7%95%A5%E5%90%97/","title":"你真的了解Pulsar的消息保留、积压、TTL策略吗"},{"content":"引言 今天一起来看看Pulsar服务端消息相关最重要的一个类PersistentTopic，看看它都负责哪些事情以及是如何设计的\n正文 在Topic被创建时，Pulsar集群中会有一台Broker维护这个Topic，在实现上就是维护一个PersistentTopic对象，这个PersistentTopic对象处理针对该Topic相关的一切操作，具体负责的相关操作如下\n处理订阅以及下线订阅 新增生产者对象以及下线生产者对象 处理消息写入 进行跨集群复制 记录Topic度量状态以及做Topic限流 检查消息的TTL、积压、压缩、配置策略更新 绘成表格的形式如下图\n本篇文章不会深入讲解每一项，主要是大概过下这个类都做了哪些事情以及大致逻辑，因为服务端的代码会经常跟这个类打交道，因此专门弄清楚这个类的相关知识还是很有必要的。接下来就让我们带着以下三个疑问去看下面的内容\nPersistentTopic在什么时候会被创建？都有哪些重要的成员变量？ PersistentTopic的创建流程都会发生什么？ 上面那些相关操作大概是什么实现的？ 一、何时创建 以下列出创建的时机，基本上流程都是发起创建Topic的时候会通过一致性哈希计算出这个Topic所归属的Bundle，然后去zookeeper获取这个Bundle所归属的Broker机器，最后请求这台Broker节点创建对应的PersistentTopic对象\n管理流创建 cli管理命令行 多语言Client Http方式 写入流创建 生产者写入流 消费者写入流 接下来看看这个类的主要成员变量，重要的一些已经加上注释解释了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 public class PersistentTopic extends AbstractTopic implements Topic, AddEntryCallback { // 管理Bookkeeper的Ledger，在做消息读取或者写入时会通过该对象 protected final ManagedLedger ledger; // 存储订阅当前Topic的所有订阅对象，key是订阅名，value是订阅对象 private final ConcurrentOpenHashMap\u0026lt;String, PersistentSubscription\u0026gt; subscriptions; // 管理对端集群，负责做跨集群数据复制 private final ConcurrentOpenHashMap\u0026lt;String/*RemoteCluster*/, Replicator\u0026gt; replicators; //跟replicators类似 private final ConcurrentOpenHashMap\u0026lt;String/*ShadowTopic*/, Replicator\u0026gt; shadowReplicators; @Getter private volatile List\u0026lt;String\u0026gt; shadowTopics; private final TopicName shadowSourceTopic; //调度限流器 private Optional\u0026lt;DispatchRateLimiter\u0026gt; dispatchRateLimiter = Optional.empty(); //调度限流锁 private final Object dispatchRateLimiterLock = new Object(); //订阅限流器 private Optional\u0026lt;SubscribeRateLimiter\u0026gt; subscribeRateLimiter = Optional.empty(); //积压游标阈值条数 private final long backloggedCursorThresholdEntries; public static final int MESSAGE_RATE_BACKOFF_MS = 1000; //处理消息重复情况 protected final MessageDeduplication messageDeduplication; //处理消息压缩服务 private TopicCompactionService topicCompactionService; // 在对外开放压缩策略配置时，根据用户配置创建对应的压缩策略 private static Map\u0026lt;String, TopicCompactionStrategy\u0026gt; strategicCompactionMap = Map.of( ServiceUnitStateChannelImpl.TOPIC, new ServiceUnitStateCompactionStrategy()); //未知 private CompletableFuture\u0026lt;MessageIdImpl\u0026gt; currentOffload = CompletableFuture.completedFuture( (MessageIdImpl) MessageId.earliest); //负责跨集群复制时订阅相关事项 private volatile Optional\u0026lt;ReplicatedSubscriptionsController\u0026gt; replicatedSubscriptionsController = Optional.empty(); //记录Topic度量相关信息，如这个Topic的写入速率、消费速率等 private static final FastThreadLocal\u0026lt;TopicStatsHelper\u0026gt; threadLocalTopicStats = new FastThreadLocal\u0026lt;TopicStatsHelper\u0026gt;() { @Override protected TopicStatsHelper initialValue() { return new TopicStatsHelper(); } }; } 二、创建流程 创建流程主要分为构建和初始化两个阶段，一般是先通过构造函数创建PersistentTopic，创建好后在调用其initialize方法进行初始化\n1. 构造函数 构造函数主要做三件事：1. 更新Broker级别策略 2. 创建发布限流 3. 初始化容器。具体代码实现如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 public AbstractTopic(String topic, BrokerService brokerService) { //基本都是赋值操作 this.topic = topic; this.brokerService = brokerService; this.producers = new ConcurrentHashMap\u0026lt;\u0026gt;(); this.isFenced = false; ServiceConfiguration config = brokerService.pulsar().getConfiguration(); this.replicatorPrefix = config.getReplicatorPrefix(); topicPolicies = new HierarchyTopicPolicies(); updateTopicPolicyByBrokerConfig(); this.lastActive = System.nanoTime(); this.preciseTopicPublishRateLimitingEnable = config.isPreciseTopicPublishRateLimiterEnable(); //创建发布限流 topicPublishRateLimiter = new PublishRateLimiterImpl(brokerService.getPulsar().getMonotonicSnapshotClock()); //更新限流策略 updateActiveRateLimiters(); } public PersistentTopic(String topic, ManagedLedger ledger, BrokerService brokerService) { super(topic, brokerService); //赋值操作 this.orderedExecutor = brokerService.getTopicOrderedExecutor() != null ? brokerService.getTopicOrderedExecutor().chooseThread(topic) : null; this.ledger = ledger; //初始化容器，分别是维护当前Topic的订阅情况、跨集群副本情况 this.subscriptions = ConcurrentOpenHashMap.\u0026lt;String, PersistentSubscription\u0026gt;newBuilder() .expectedItems(16) .concurrencyLevel(1) .build(); this.replicators = ConcurrentOpenHashMap.\u0026lt;String, Replicator\u0026gt;newBuilder() .expectedItems(16) .concurrencyLevel(1) .build(); this.shadowReplicators = ConcurrentOpenHashMap.\u0026lt;String, Replicator\u0026gt;newBuilder() .expectedItems(16) .concurrencyLevel(1) .build(); this.backloggedCursorThresholdEntries = brokerService.pulsar().getConfiguration().getManagedLedgerCursorBackloggedThreshold(); .... } 2. initialize方法 初始化方法的逻辑相比下要丰富写，归纳起来有以下四点\n获取压缩服务\n通过双重判断获取单例TwoPhaseCompactor进行压缩，有专门的线程池进行处理压缩动作\n遍历游标恢复订阅的状态\n当Topic重新分配后，在新的Broker中要根据游标恢复订阅、跨集群复制的状态，这样才能让客户端“无感”\n跨集群复制\n遍历这个Topic的游标，如果游标中存在跨集群复制游标则创建对应的GeoPersistentReplicator对象进行消息的复制\n更新配置\n更新Topic命名空间的配置策略 初始化调度限流DispatchRateLimiter 更新订阅/发布/资源组限流 更新Topic级别的配置 具体代码实现如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 public CompletableFuture\u0026lt;Void\u0026gt; initialize() { List\u0026lt;CompletableFuture\u0026lt;Void\u0026gt;\u0026gt; futures = new ArrayList\u0026lt;\u0026gt;(); //获取Broker压缩对象 futures.add(brokerService.getPulsar().newTopicCompactionService(topic) .thenAccept(service -\u0026gt; { PersistentTopic.this.topicCompactionService = service; //遍历这个Topic的游标恢复订阅中的对象 this.createPersistentSubscriptions(); })); //遍历这个Topic的游标，如果游标中存在跨集群复制游标则创建对应的GeoPersistentReplicator对象进行消息的复制 for (ManagedCursor cursor : ledger.getCursors()) { if (cursor.getName().startsWith(replicatorPrefix)) { String localCluster = brokerService.pulsar().getConfiguration().getClusterName(); String remoteCluster = PersistentReplicator.getRemoteCluster(cursor.getName()); futures.add(addReplicationCluster(remoteCluster, cursor, localCluster)); } } return FutureUtil.waitForAll(futures).thenCompose(__ -\u0026gt; brokerService.pulsar().getPulsarResources().getNamespaceResources() .getPoliciesAsync(TopicName.get(topic).getNamespaceObject()) .thenAcceptAsync(optPolicies -\u0026gt; { if (!optPolicies.isPresent()) { isEncryptionRequired = false; updatePublishDispatcher(); //进行资源组级别的隔离 updateResourceGroupLimiter(new Policies()); initializeDispatchRateLimiterIfNeeded(); updateSubscribeRateLimiter(); return; } Policies policies = optPolicies.get(); //更新命名空间级别的策略 this.updateTopicPolicyByNamespacePolicy(policies); //初始化调度限流 initializeDispatchRateLimiterIfNeeded(); //更新订阅限流 updateSubscribeRateLimiter(); //更新发布限流 updatePublishDispatcher(); //更新资源组限流 updateResourceGroupLimiter(policies); this.isEncryptionRequired = policies.encryption_required; isAllowAutoUpdateSchema = policies.is_allow_auto_update_schema; }, getOrderedExecutor()) .thenCompose(ignore -\u0026gt; initTopicPolicy()) .exceptionally(ex -\u0026gt; { .... })); } 三、负责内容 1、生产者相关 新增 生产者客户端启动时会跟Topic归属的Broker建立TCP连接并请求Broker端ServerCnx的handleProducer方法进行处理，这个方法最终会调用PersistentTopic的addProducer方法进行创建，咱们来看看实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public CompletableFuture\u0026lt;Optional\u0026lt;Long\u0026gt;\u0026gt; addProducer(Producer producer, CompletableFuture\u0026lt;Void\u0026gt; producerQueuedFuture) { //进去看看父类的实现 return super.addProducer(producer, producerQueuedFuture).thenCompose(topicEpoch -\u0026gt; { messageDeduplication.producerAdded(producer.getProducerName()); // Start replication producers if not already return startReplProducers().thenApply(__ -\u0026gt; topicEpoch); }); } public CompletableFuture\u0026lt;Optional\u0026lt;Long\u0026gt;\u0026gt; addProducer(Producer producer, CompletableFuture\u0026lt;Void\u0026gt; producerQueuedFuture) { .... //继续跟踪 return internalAddProducer(producer); .... } protected CompletableFuture\u0026lt;Void\u0026gt; internalAddProducer(Producer producer) { .... //producers是ConcurrentHashMap结构，相当于会在PersistentTopic中维护服务端的生产者对象 Producer existProducer = producers.putIfAbsent(producer.getProducerName(), producer); .... return CompletableFuture.completedFuture(null); } 删除 删除的逻辑很简单，也是通过TCP接收到客户端断开连接的请求后，会调用Producer的close方法进行资源释放，同时从维护的producers中移除此对象\n2. 订阅相关 消费者客户端启动时会跟Topic归属的Broker建立TCP连接并请求Broker端，请求由ServerCnx的handleSubscribe方法进行处理，这个方法最终会调用PersistentTopic的internalSubscribe方法进行创建，咱们来看看实现\n新增 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 private CompletableFuture\u0026lt;Consumer\u0026gt; internalSubscribe(final TransportCnx cnx, String subscriptionName, long consumerId, SubType subType, int priorityLevel, String consumerName, boolean isDurable, MessageId startMessageId, Map\u0026lt;String, String\u0026gt; metadata, boolean readCompacted, InitialPosition initialPosition, long startMessageRollbackDurationSec, boolean replicatedSubscriptionStateArg, KeySharedMeta keySharedMeta, Map\u0026lt;String, String\u0026gt; subscriptionProperties, long consumerEpoch, SchemaType schemaType) { .... //创建对应的客户端对象 Consumer consumer = new Consumer(subscription, subType, topic, consumerId, priorityLevel, consumerName, isDurable, cnx, cnx.getAuthRole(), metadata, readCompacted, keySharedMeta, startMessageId, consumerEpoch, schemaType); .... //将消费者对象加到订阅里 return addConsumerToSubscription(subscription, consumer); } protected CompletableFuture\u0026lt;Void\u0026gt; addConsumerToSubscription(Subscription subscription, Consumer consumer) { .... //最终将将创建的消费者放到Dispatcher中进行管理以及加到subscriptions这个Map中进行维护 return subscription.addConsumer(consumer); } 删除 删除的逻辑也是比较简单的，就是从subscriptions中进行移除\n3. 处理消息写入 消息写入的流程相对比较复杂，后面再单独分析，暂时跳过\n4. 跨集群复制 跨集群复制这里实现逻辑其实不难，相当于在Broker上去读取Bookkeeper获取数据并通过启动生产者往其他集群进行数据写入，具体细节后面也单独写文章进行分析\n5. 消息清理 Pulsar的消息TTL、积压、压缩都是在Broker启动时往线程池中加入定时任务轮询触发的\nTTL TTL会循环扫描每个Topic过期的消息位置，并通过改变游标进行标记，而清理是由专门的线程根据游标进行处理的\n积压 消息积压也是会定期检测，分两种情况，一种是消息超过限制额度，另一种是消息时间超过额度。这两种情况都会触发积压策略的处理，具体的策略在BacklogQuota.RetentionPolicy中定义\n压缩 跟上面一样，轮询检测进行处理\n配置更新 在PersistentTopic启动的初始化initialize方法中会调用onUpdate进行策略更新，那Pulsar又是如何做的支持配置动态更新的呢？这个问题保留给大家\n四、总结 本文主要强调PersistentTopic类的重要性、聊聊它都负责哪些事情以及大概实现，具体每个细节展开都能单独写一篇文章，这个敬请期待～\n","date":"2024-03-27T10:34:55+08:00","image":"https://sherlock-lin.github.io/image-20240331085922332.png","permalink":"https://sherlock-lin.github.io/p/pulsar%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8Bpersistenttopic%E7%B1%BB/","title":"Pulsar源码解析之PersistentTopic类"},{"content":"一、引言 无论你是刚接触Pulsar还是使用Pulsar多年，相信你对下面这段代码都很熟悉，这就是生产者端最常写的代码没有之一，其中最核心的其实就三行代码，分别用红色数字标识出来了，其中对应的就是1、客户端对象创建 2、生产者对象创建 3、消息发送。今天就分别针对这三个步骤进行深入的探索。\n二、创建客户端对象 无论是写生产者还是消费者端代码，第一步都是要创建客户端对象，那么客户端对象都做了些什么事情呢？这里将客户端对象创建的步骤绘制成以下图\n客户端对象的创建以下几个东西，其中最重要的是前两个\nLookup服务 连接池 线程池 内存控制器MemoryLimitController 创建客户端计数器HashedWheelTimer 1. Lookup服务 Lookup服务负责获取Topic的归属Broker地址以及Schema信息等，非常重要。默认有HTTP和二进制传输两种实现，如果创建客户端对象时.serviceUrl方法传入的地址是http开头则使用HTTP实现，否则就是二进制传输实现。\nLookup服务的创建如下\n1 2 3 4 5 6 7 //初始化LookupService服务 if (conf.getServiceUrl().startsWith(\u0026#34;http\u0026#34;)) { lookup = new HttpLookupService(conf, this.eventLoopGroup); } else { lookup = new BinaryProtoLookupService(this, conf.getServiceUrl(), conf.getListenerName(), conf.isUseTls(), this.scheduledExecutorProvider.getExecutor()); } 进入看看HttpLookupService的构造方法可以看到它是内部套了一个HttpClient对象来对外进行HTTP通信，在继续看HttpClient的构造函数可以它内部实际上是调用的DefaultAsyncHttpClient构造函数来创建，这个对象是外部包 async-http-client-2.12.1.jar的实现，跟了一下代码，底层也是基于Netty来进行实现的HTTP长连接通信\n1 2 3 4 5 6 7 8 9 10 11 public HttpLookupService(ClientConfigurationData conf, EventLoopGroup eventLoopGroup) throws PulsarClientException { this.httpClient = new HttpClient(conf, eventLoopGroup); this.useTls = conf.isUseTls(); this.listenerName = conf.getListenerName(); } protected HttpClient(ClientConfigurationData conf, EventLoopGroup eventLoopGroup) throws PulsarClientException { .... httpClient = new DefaultAsyncHttpClient(config); } 跟到这里就差不多了，我们知道创建客户端对象的时候会初始化Lookup服务，而Lookup服务初始化的时候会创建跟外部进行通信的异步HTTP客户端，用于在创建生产者时去查询Topic的归属Broker的IP地址，这样生产者才知道具体去跟哪台Broker创建TCP连接\n2. 连接池ConnectionPool 连接池是池化技术在网络连接这个场景的使用，使用连接池可以避免重复创建关闭TCP连接造成资源浪费以及提升性能。在创建客户端对象的构造方法中，ConnectionPool的创建如下，可以是通过new方式进行创建，因此我们看下它的构造方法\n1 2 connectionPoolReference = connectionPool != null ? connectionPool : new ConnectionPool(conf, this.eventLoopGroup); ConnectionPool的构造方法如下，可以看到核心逻辑其实就是通过Bootstrap创建Netty客户端对象，通过 pool = new ConcurrentHashMap\u0026lt;\u0026gt;(); 这行代码也很重要，这个HashMap就是存储咱们的网络连接\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public ConnectionPool(ClientConfigurationData conf, EventLoopGroup eventLoopGroup, Supplier\u0026lt;ClientCnx\u0026gt; clientCnxSupplier, Optional\u0026lt;AddressResolver\u0026lt;InetSocketAddress\u0026gt;\u0026gt; addressResolver) throws PulsarClientException { .... //启动Netty客户端 pool = new ConcurrentHashMap\u0026lt;\u0026gt;(); bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup); bootstrap.channel(EventLoopUtil.getClientSocketChannelClass(eventLoopGroup)); //Netty相关配置 bootstrap.option(ChannelOption.CONNECT_TIMEOUT_MILLIS, conf.getConnectionTimeoutMs()); bootstrap.option(ChannelOption.TCP_NODELAY, conf.isUseTcpNoDelay()); bootstrap.option(ChannelOption.ALLOCATOR, PulsarByteBufAllocator.DEFAULT); try { channelInitializerHandler = new PulsarChannelInitializer(conf, clientCnxSupplier); bootstrap.handler(channelInitializerHandler); } catch (Exception e) { log.error(\u0026#34;Failed to create channel initializer\u0026#34;); throw new PulsarClientException(e); } .... } 三、生产者对象创建 看完了客户端对象创建，再来看看生产者对象的创建，从这条语句进行切入 Producer\u0026lt;String\u0026gt; producer = pulsarClient.newProducer(...).create(); 在创建生产者对象时会进行一下几步\n1. 指定路由策略 这个主要就是根据创建生产者对象时指定的，如果没有配置的话默认使用的轮询路由策略。setMessageRoutingMode 这个方法就是指定路由策略的，然后指定完后下面的代码可以看到，如果有配置拦截器的话在创建生产者对象时也会把它给拦截逻辑加载进去。\n1 2 3 4 5 6 7 8 9 10 11 12 public CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; createAsync() { .... try { setMessageRoutingMode(); } catch (PulsarClientException pce) { return FutureUtil.failedFuture(pce); } return interceptorList == null || interceptorList.size() == 0 ? client.createProducerAsync(conf, schema, null) : client.createProducerAsync(conf, schema, new ProducerInterceptors(interceptorList)); } setMessageRoutingMode 逻辑如下，默认用轮询路由，如果配置其他的就用其他的，其次就是做路由规则的校验，看看用户是否配置的信息有冲突的提前感知并抛出去。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 private void setMessageRoutingMode() throws PulsarClientException { if (conf.getMessageRoutingMode() == null \u0026amp;\u0026amp; conf.getCustomMessageRouter() == null) { messageRoutingMode(MessageRoutingMode.RoundRobinPartition); } else if (conf.getMessageRoutingMode() == null \u0026amp;\u0026amp; conf.getCustomMessageRouter() != null) { messageRoutingMode(MessageRoutingMode.CustomPartition); } else if (conf.getMessageRoutingMode() == MessageRoutingMode.CustomPartition \u0026amp;\u0026amp; conf.getCustomMessageRouter() == null) { throw new PulsarClientException(\u0026#34;When \u0026#39;messageRoutingMode\u0026#39; is \u0026#34; + MessageRoutingMode.CustomPartition + \u0026#34;, \u0026#39;messageRouter\u0026#39; should be set\u0026#34;); } else if (conf.getMessageRoutingMode() != MessageRoutingMode.CustomPartition \u0026amp;\u0026amp; conf.getCustomMessageRouter() != null) { throw new PulsarClientException(\u0026#34;When \u0026#39;messageRouter\u0026#39; is set, \u0026#39;messageRoutingMode\u0026#39; \u0026#34; + \u0026#34;should be set as \u0026#34; + MessageRoutingMode.CustomPartition); } } 2. 获取Topic分区数和Schema 通过Lookup机制去Broker中查询这个Topic的Schema信息，可以看到如果服务端没有配置Schema信息的话则默认用 Schema.BYTES\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 return lookup.getSchema(TopicName.get(conf.getTopicName())) .thenCompose(schemaInfoOptional -\u0026gt; { if (schemaInfoOptional.isPresent()) { SchemaInfo schemaInfo = schemaInfoOptional.get(); if (schemaInfo.getType() == SchemaType.PROTOBUF) { autoProduceBytesSchema.setSchema(new GenericAvroSchema(schemaInfo)); } else { autoProduceBytesSchema.setSchema(Schema.getSchema(schemaInfo)); } } else { autoProduceBytesSchema.setSchema(Schema.BYTES); } return createProducerAsync(topic, conf, schema, interceptors); }); 咱们进去看看 getSchema 的实现，可以看到构造的是HTTP地址并通过GET方式请求Broker进行获取\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public CompletableFuture\u0026lt;Optional\u0026lt;SchemaInfo\u0026gt;\u0026gt; getSchema(TopicName topicName, byte[] version) { CompletableFuture\u0026lt;Optional\u0026lt;SchemaInfo\u0026gt;\u0026gt; future = new CompletableFuture\u0026lt;\u0026gt;(); String schemaName = topicName.getSchemaName(); String path = String.format(\u0026#34;admin/v2/schemas/%s/schema\u0026#34;, schemaName); if (version != null) { if (version.length == 0) { future.completeExceptionally(new SchemaSerializationException(\u0026#34;Empty schema version\u0026#34;)); return future; } path = String.format(\u0026#34;admin/v2/schemas/%s/schema/%s\u0026#34;, schemaName, ByteBuffer.wrap(version).getLong()); } httpClient.get(path, GetSchemaResponse.class).thenAccept(response -\u0026gt; { if (response.getType() == SchemaType.KEY_VALUE) { SchemaData data = SchemaData .builder() .data(SchemaUtils.convertKeyValueDataStringToSchemaInfoSchema( response.getData().getBytes(StandardCharsets.UTF_8))) .type(response.getType()) .props(response.getProperties()) .build(); future.complete(Optional.of(SchemaInfoUtil.newSchemaInfo(schemaName, data))); } else { future.complete(Optional.of(SchemaInfoUtil.newSchemaInfo(schemaName, response))); } }).exceptionally(ex -\u0026gt; { .... }); return future; } 除了读取Schema，还会读取Topic的分区信息，从下面创建的代码可以看到，如果存在分区则创建PartitionedProducerImpl对象，不存在分区则创建ProducerImpl对象。获取分区的方法是 getPartitionedTopicMetadata，咱们进去看看它的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 private \u0026lt;T\u0026gt; CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; createProducerAsync(String topic, ProducerConfigurationData conf, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture = new CompletableFuture\u0026lt;\u0026gt;(); getPartitionedTopicMetadata(topic).thenAccept(metadata -\u0026gt; { .... ProducerBase\u0026lt;T\u0026gt; producer; if (metadata.partitions \u0026gt; 0) { producer = newPartitionedProducerImpl(topic, conf, schema, interceptors, producerCreatedFuture, metadata); } else { producer = newProducerImpl(topic, -1, conf, schema, interceptors, producerCreatedFuture, Optional.empty()); } producers.add(producer); }).exceptionally(ex -\u0026gt; { .... }); return producerCreatedFuture; } getPartitionedTopicMetadata方法的核心逻辑如下，通过一路跟踪下去发现也是通过Lookup服务进行HTTP进行查询读取分区信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 public CompletableFuture\u0026lt;PartitionedTopicMetadata\u0026gt; getPartitionedTopicMetadata(String topic) { CompletableFuture\u0026lt;PartitionedTopicMetadata\u0026gt; metadataFuture = new CompletableFuture\u0026lt;\u0026gt;(); try { TopicName topicName = TopicName.get(topic); AtomicLong opTimeoutMs = new AtomicLong(conf.getLookupTimeoutMs()); .... getPartitionedTopicMetadata(topicName, backoff, opTimeoutMs, metadataFuture, new ArrayList\u0026lt;\u0026gt;()); } catch (IllegalArgumentException e) { .... } return metadataFuture; } private void getPartitionedTopicMetadata(TopicName topicName, Backoff backoff, AtomicLong remainingTime, CompletableFuture\u0026lt;PartitionedTopicMetadata\u0026gt; future, List\u0026lt;Throwable\u0026gt; previousExceptions) { long startTime = System.nanoTime(); lookup.getPartitionedTopicMetadata(topicName).thenAccept(future::complete).exceptionally(e -\u0026gt; { remainingTime.addAndGet(-1 * TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTime)); long nextDelay = Math.min(backoff.next(), remainingTime.get()); // skip retry scheduler when set lookup throttle in client or server side which will lead to // `TooManyRequestsException` boolean isLookupThrottling = !PulsarClientException.isRetriableError(e.getCause()) || e.getCause() instanceof PulsarClientException.AuthenticationException; if (nextDelay \u0026lt;= 0 || isLookupThrottling) { PulsarClientException.setPreviousExceptions(e, previousExceptions); future.completeExceptionally(e); return null; } .... }); } public CompletableFuture\u0026lt;PartitionedTopicMetadata\u0026gt; getPartitionedTopicMetadata(TopicName topicName) { String format = topicName.isV2() ? \u0026#34;admin/v2/%s/partitions\u0026#34; : \u0026#34;admin/%s/partitions\u0026#34;; return httpClient.get(String.format(format, topicName.getLookupName()) + \u0026#34;?checkAllowAutoCreation=true\u0026#34;, PartitionedTopicMetadata.class); } 3. 创建生产者对象 在上一小结可以看到已经查到分区信息，在有分区的情况是会调用 newPartitionedProducerImpl 方法进行生产者对象的初始化，现在就从这里开始进行跟踪，可以看到这个方法只是封装了new PartitionedProducerImpl对象的操作，于是继续看PartitionedProducerImpl的构造函数的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 protected \u0026lt;T\u0026gt; PartitionedProducerImpl\u0026lt;T\u0026gt; newPartitionedProducerImpl(String topic, ProducerConfigurationData conf, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors, CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture, PartitionedTopicMetadata metadata) { return new PartitionedProducerImpl\u0026lt;\u0026gt;(PulsarClientImpl.this, topic, conf, metadata.partitions, producerCreatedFuture, schema, interceptors); } public PartitionedProducerImpl(PulsarClientImpl client, String topic, ProducerConfigurationData conf, int numPartitions, CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors) { super(client, topic, conf, producerCreatedFuture, schema, interceptors); this.producers = ConcurrentOpenHashMap.\u0026lt;Integer, ProducerImpl\u0026lt;T\u0026gt;\u0026gt;newBuilder().build(); this.topicMetadata = new TopicMetadataImpl(numPartitions); //配置路由策略 this.routerPolicy = getMessageRouter(); stats = client.getConfiguration().getStatsIntervalSeconds() \u0026gt; 0 ? new PartitionedTopicProducerStatsRecorderImpl() : null; //配置最大同时等待的消息数 int maxPendingMessages = Math.min(conf.getMaxPendingMessages(), conf.getMaxPendingMessagesAcrossPartitions() / numPartitions); conf.setMaxPendingMessages(maxPendingMessages); final List\u0026lt;Integer\u0026gt; indexList; if (conf.isLazyStartPartitionedProducers() \u0026amp;\u0026amp; conf.getAccessMode() == ProducerAccessMode.Shared) { // try to create producer at least one partition indexList = Collections.singletonList(routerPolicy .choosePartition(((TypedMessageBuilderImpl\u0026lt;T\u0026gt;) newMessage()).getMessage(), topicMetadata)); } else { // try to create producer for all partitions indexList = IntStream.range(0, topicMetadata.numPartitions()).boxed().collect(Collectors.toList()); } firstPartitionIndex = indexList.get(0); //这里是核心逻辑，从这里进去 start(indexList); // start track and auto subscribe partition increasement if (conf.isAutoUpdatePartitions()) { topicsPartitionChangedListener = new TopicsPartitionChangedListener(); partitionsAutoUpdateTimeout = client.timer() .newTimeout(partitionsAutoUpdateTimerTask, conf.getAutoUpdatePartitionsIntervalSeconds(), TimeUnit.SECONDS); } } private void start(List\u0026lt;Integer\u0026gt; indexList) { .... final ProducerImpl\u0026lt;T\u0026gt; firstProducer = createProducer(indexList.get(0)); firstProducer.producerCreatedFuture().handle((prod, createException) -\u0026gt; { .... }).thenApply(name -\u0026gt; { for (int i = 1; i \u0026lt; indexList.size(); i++) { //循环分区数创建与之对应的ProducerImpl对象 createProducer(indexList.get(i), name).producerCreatedFuture().handle((prod, createException) -\u0026gt; { afterCreatingProducer.accept(false, createException); return null; }); } return null; }); } 继续从 createProducer 方法进行跟踪\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 private ProducerImpl\u0026lt;T\u0026gt; createProducer(final int partitionIndex, final Optional\u0026lt;String\u0026gt; overrideProducerName) { //创建ProducerImpl后会统一放到producers这个Map中，key是分区号，value就是ProducerImpl对象 return producers.computeIfAbsent(partitionIndex, (idx) -\u0026gt; { String partitionName = TopicName.get(topic).getPartition(idx).toString(); //继续进入看看ProducerImpl创建的逻辑 return client.newProducerImpl(partitionName, idx, conf, schema, interceptors, new CompletableFuture\u0026lt;\u0026gt;(), overrideProducerName); }); } public ProducerImpl(PulsarClientImpl client, String topic, ProducerConfigurationData conf, CompletableFuture\u0026lt;Producer\u0026lt;T\u0026gt;\u0026gt; producerCreatedFuture, int partitionIndex, Schema\u0026lt;T\u0026gt; schema, ProducerInterceptors interceptors, Optional\u0026lt;String\u0026gt; overrideProducerName) { .... this.compressor = CompressionCodecProvider.getCompressionCodec(conf.getCompressionType()); .... //这里是核心逻辑，用于创建ProducerImpl对象跟对应的Broker的TCP网络连接 grabCnx(); } 4. 创建连接 从上一小节可以看到代码逻辑跟到了grabCnx 方法，继续一路跟踪下去，可以看到最后都会调用 state.client.getConnection, 继续往里面看看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 void grabCnx() { this.connectionHandler.grabCnx(); } protected void grabCnx() { grabCnx(Optional.empty()); } protected void grabCnx(Optional\u0026lt;URI\u0026gt; hostURI) { .... cnxFuture = state.client.getConnection(address, address, randomKeyForSelectConnection); .... } public CompletableFuture\u0026lt;ClientCnx\u0026gt; getConnection(final InetSocketAddress logicalAddress, final InetSocketAddress physicalAddress, final int randomKeyForSelectConnection) { //可以看到调用连接池进行网络连接的获取，继续进去看看实现细节 return cnxPool.getConnection(logicalAddress, physicalAddress, randomKeyForSelectConnection); } public CompletableFuture\u0026lt;ClientCnx\u0026gt; getConnection(InetSocketAddress logicalAddress, InetSocketAddress physicalAddress, final int randomKey) { if (maxConnectionsPerHosts == 0) { // 如果配置没开启连接池，则每一次都重新新建一个TCP连接 return createConnection(logicalAddress, physicalAddress, -1); } final ConcurrentMap\u0026lt;Integer, CompletableFuture\u0026lt;ClientCnx\u0026gt;\u0026gt; innerPool = pool.computeIfAbsent(logicalAddress, a -\u0026gt; new ConcurrentHashMap\u0026lt;\u0026gt;()); // 新建TCP网络连接并放在连接池中以备之后的复用,继续进去看createConnection的逻辑 CompletableFuture\u0026lt;ClientCnx\u0026gt; completableFuture = innerPool .computeIfAbsent(randomKey, k -\u0026gt; createConnection(logicalAddress, physicalAddress, randomKey)); .... } private CompletableFuture\u0026lt;ClientCnx\u0026gt; createConnection(InetSocketAddress logicalAddress, InetSocketAddress physicalAddress, int connectionKey) { .... // 继续进去看createConnection的逻辑 createConnection(logicalAddress, physicalAddress).thenAccept(channel -\u0026gt; { .... }).exceptionally(exception -\u0026gt; { .... }); return cnxFuture; } private CompletableFuture\u0026lt;Channel\u0026gt; createConnection(InetSocketAddress logicalAddress, InetSocketAddress unresolvedPhysicalAddress) { CompletableFuture\u0026lt;List\u0026lt;InetSocketAddress\u0026gt;\u0026gt; resolvedAddress; try { .... return resolvedAddress.thenCompose( inetAddresses -\u0026gt; connectToResolvedAddresses( logicalAddress, unresolvedPhysicalAddress, inetAddresses.iterator(), isSniProxy ? unresolvedPhysicalAddress : null) ); } catch (URISyntaxException e) { .... } } private CompletableFuture\u0026lt;Channel\u0026gt; connectToResolvedAddresses(...) { CompletableFuture\u0026lt;Channel\u0026gt; future = new CompletableFuture\u0026lt;\u0026gt;(); // 继续往下跟踪 connectToAddress(logicalAddress, resolvedPhysicalAddress.next(), unresolvedPhysicalAddress, sniHost) .... return null; }); return future; } private CompletableFuture\u0026lt;Channel\u0026gt; connectToAddress(InetSocketAddress logicalAddress, InetSocketAddress physicalAddress, InetSocketAddress unresolvedPhysicalAddress, InetSocketAddress sniHost) { .... //终于跟踪到了，就是bootstrap.register() 这个方法，可以尝试往里面看看实现，从这里开始就是Netty的代码了 return toCompletableFuture(bootstrap.register()) .thenCompose(channelInitializerHandler::initSocks5IfConfig) .thenCompose(ch -\u0026gt; channelInitializerHandler.initializeClientCnx(ch, logicalAddress, unresolvedPhysicalAddress)) .thenCompose(channel -\u0026gt; toCompletableFuture(channel.connect(physicalAddress))); } public ChannelFuture register() { validate(); //继续往下 return initAndRegister(); } final ChannelFuture initAndRegister() { Channel channel = null; try { //新建一条网络通道到Broker channel = channelFactory.newChannel(); init(channel); } catch (Throwable t) { .... } .... return regFuture; } 从上面的代码跟踪可以看到，生产者对象在创建的时候会通过Netty客户端跟Topic所在的Broker建立TCP网络连接，方便后续的通信\n四、消息发送 消息发送流程大致如下图\n在选择消息发送是，生产者对象会根据路由策略来决定用目标分区所对应的ProducerImpl对象进行处理 发送前会按照顺序对数据进行拦截器链逻辑处理(如果有配置的话)，然后进行压缩最后再进行序列化操作(消息传输/存储必须序列化) 消息发送前会放到待确认队列中进行维护，每个分区都有一个对应的确认队列，在消息写入成功后会从对应的确认队列中将自己删除，否则这条消息不算写入成功。 将消息发送操作封装成一个任务交给线程池中的一个线程进行最后的发送操作 Broker将数据写入成功后向客户端返回ack，客户端通过ack中携带的消息信息到待确认队列中进行消息的删除 那么老规矩，继续一起看下代码实现吧\n1. 发送流程前置操作 就从常见的这条语句进行切入 producer.sendAsync(\u0026quot;hello java API pulsar:\u0026quot;+i+\u0026quot;, 当前时间为：\u0026quot;+new Date()); 。通过代码中可以看到sendAsync 方法是从其父类ProducerBase中继承的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public CompletableFuture\u0026lt;MessageId\u0026gt; sendAsync(Message\u0026lt;?\u0026gt; message) { //继续跟踪进去 return internalSendAsync(message); } //这是一个抽象方法，有分区生产者和非分区生产者两种实现，跟踪分区生产者的实现逻辑 abstract CompletableFuture\u0026lt;MessageId\u0026gt; internalSendAsync(Message\u0026lt;?\u0026gt; message); CompletableFuture\u0026lt;MessageId\u0026gt; internalSendAsync(Message\u0026lt;?\u0026gt; message) { //继续往里面跟踪 return internalSendWithTxnAsync(message, null); } CompletableFuture\u0026lt;MessageId\u0026gt; internalSendWithTxnAsync(Message\u0026lt;?\u0026gt; message, Transaction txn) { .... //还有印象吗，在生产者创建时会维护一个以分区号为key，ProducerImpl为value的Map，现在从里面获取相对应的对象进行处理 return producers.get(partition).internalSendWithTxnAsync(message, txn); } 2. 消息处理以及包装 又回到ProducerImpl对象的逻辑了，相当于分区生产者对象只是个壳，无论分区还是非分区最终都是调用的ProducerImpl进行消息发送的真正逻辑，不废话继续往下看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 CompletableFuture\u0026lt;MessageId\u0026gt; internalSendWithTxnAsync(Message\u0026lt;?\u0026gt; message, Transaction txn) { .... return internalSendAsync(message); } CompletableFuture\u0026lt;MessageId\u0026gt; internalSendAsync(Message\u0026lt;?\u0026gt; message) { .... //拦截器的逻辑就是在这里生效 MessageImpl\u0026lt;?\u0026gt; interceptorMessage = (MessageImpl) beforeSend(message); .... //核心逻辑，继续切入 sendAsync(interceptorMessage, new SendCallback() { .... }); return future; } public void sendAsync(Message\u0026lt;?\u0026gt; message, SendCallback callback) { checkArgument(message instanceof MessageImpl); .... //核心逻辑，从名字可以看到就是对消息进行序列化并进行发送逻辑，继续跟进去 serializeAndSendMessage(msg, payload, sequenceId, uuid, chunkId, totalChunks, readStartIndex, payloadChunkSize, compressedPayload, compressed, compressedPayload.readableBytes(), callback, chunkedMessageCtx, messageId); } catch (PulsarClientException e) { .... } } private void serializeAndSendMessage(....) throws IOException { //核心逻辑，op是OpSendMsg对象，封装了要发送的消息内容，继续往下跟 processOpSendMsg(op); } } 3. 消息发送 数据已经处理包装得差不多了，接下来就是发送的逻辑，咱们顺着 processOpSendMsg 方法继续往下看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 protected void processOpSendMsg(OpSendMsg op) { .... //将消息加到OpSendMsgQueue队列中，这就是“等待确认队列” pendingMessages.add(op); .... //ClientCnx对象维护着Netty实现跟Broker的TCP连接 final ClientCnx cnx = getCnxIfReady(); if (cnx != null) { .... //WriteInEventLoopCallback方法的run方法执行会将数据发送出去，然后队列中维护消息的状态 cnx.ctx().channel().eventLoop().execute(WriteInEventLoopCallback.create(this, cnx, op)); stats.updateNumMsgsSent(op.numMessagesInBatch, op.batchSizeByte); } else { .... } } catch (Throwable t) { .... } } //WriteInEventLoopCallback是一个线程类，被放到线程池里面执行，因此直接看它的run方法 private static final class WriteInEventLoopCallback implements Runnable { public void run() { .... try { //熟悉Netty的朋友相信对writeAndFlush方法不默认，就是通过之间建立好的TCP连接将数据发送到Broker去 cnx.ctx().writeAndFlush(cmd, cnx.ctx().voidPromise()); op.updateSentTimestamp(); } finally { recycle(); } } } 跟踪到这里基本就结束了，对Netty感兴趣的朋友可以再继续往下跟，这里可以简单说一下，其实它内部是对JDK的NIO做了包装和优化，最底层也是通过Java的Socket连接网络端口进行的数据发送。不知道有没有小伙伴发现，咦，怎么没有对返回结果进行处理的逻辑呢？这就是所谓的异步设计，Netty不就是一个异步通信框架吗，客户端发送的逻辑到这里就是彻底结束了，而消息的处理结束就是要等服务端Broker向客户端发起消息ack请求了，想知道Pulsar怎么实现的吗？那就跟着我一起瞅瞅吧～\n4. 消息确认 消息确认流程是由Broker端发起的，那么生产者对象肯定是通过Netty客户端接收的，所以直接看Pulsar实现的ChannelInboundHandlerAdapter类的channelRead的逻辑即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public abstract class PulsarDecoder extends ChannelInboundHandlerAdapter { public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { .... switch (cmd.getType()) { .... //可以看得到消息有写成功的后续处理动作，那就从这里看看 case PRODUCER_SUCCESS: checkArgument(cmd.hasProducerSuccess()); handleProducerSuccess(cmd.getProducerSuccess()); break; case UNSUBSCRIBE: checkArgument(cmd.hasUnsubscribe()); safeInterceptCommand(cmd); handleUnsubscribe(cmd.getUnsubscribe()); break; } } } handleProducerSuccess这个方法的是由ClientCnx对象进行实现的，那就跟进来看看吧\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 protected void handleProducerSuccess(CommandProducerSuccess success) { checkArgument(state == State.Ready); if (log.isDebugEnabled()) { log.debug(\u0026#34;{} Received producer success response from server: {} - producer-name: {}\u0026#34;, ctx.channel(), success.getRequestId(), success.getProducerName()); } long requestId = success.getRequestId(); if (!success.isProducerReady()) { //还记得pendingRequests 这个“待确认队列”吗，现在会从里面查询对应的消息 TimedCompletableFuture\u0026lt;?\u0026gt; requestFuture = pendingRequests.get(requestId); if (requestFuture != null) { //日志进行打印 log.info(\u0026#34;{} Producer {} has been queued up at broker. request: {}\u0026#34;, ctx.channel(), success.getProducerName(), requestId); //标记为成功 requestFuture.markAsResponded(); } return; } //客户端处理的主要逻辑就是从队列中移除此消息 CompletableFuture\u0026lt;ProducerResponse\u0026gt; requestFuture = (CompletableFuture\u0026lt;ProducerResponse\u0026gt;) pendingRequests.remove(requestId); .... } 五、总结 生产者写数据的流程到这里基本就结束了，怎么样，是不是没想象中那么可怕？也许你对1、客户端对象创建 2、生产者对象创建 3、消息发送 这三步之间的关系还有点迷迷糊糊，那就请允许我给你举个例子，1、客户端对象创建相当于一座新城市的创建，打好城市的地基，2、生产者对象创建 相当于在这个地基的基础上建起了城市，发电厂等等，最重要的是修建其通往其他各个城市的交通要道，最后的3、消息发送 相当于这个新城市的居民们乘坐高铁去其他的城市。这么说相信你一定已经明白了～如果还有其他疑问欢迎在下面的评论区一起讨论\n","date":"2024-03-21T10:34:57+08:00","image":"https://sherlock-lin.github.io/p/%E4%B8%80%E6%96%87%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82producer%E7%AB%AF%E6%B5%81%E7%A8%8B%E4%BB%A5%E5%8F%8A%E5%8E%9F%E7%90%86/image-20240321153410330_hu2501804792718174333.png","permalink":"https://sherlock-lin.github.io/p/%E4%B8%80%E6%96%87%E5%BD%BB%E5%BA%95%E6%90%9E%E6%87%82producer%E7%AB%AF%E6%B5%81%E7%A8%8B%E4%BB%A5%E5%8F%8A%E5%8E%9F%E7%90%86/","title":"一文彻底搞懂Producer端流程以及原理"},{"content":"一、引言 系统学习Pulsar的大纲\n二、正文 下图是我绘制的Pulsar大纲 (由于时间缘故花的比较粗糙，这张图会不定期更新)\n三、学习大纲 一、Pulsar Client 二、生产者 Pulsar消息路由深入剖析 三、消费者 四、Topic pulsar原来是这样操作topic的 五、Function Pulsar3.2 Function的介绍与使用 Pulsar IO实战 Worker调度管理器原理解析 六、Schema Registry Pulsar Schema使用原理介绍 七、存储管理 详解bookkeeper AutoRecovery机制 八、综合 九、高可用 四、学习资料 官方文档\nPulsar三本书籍\n《Mastering Apache Pulsar》：个人认为是Pulsar系统资料最好的，喜欢Pulsar的伙伴务必阅读此经典\n《深入解析Apace Pulsar》：林琳大佬的作品，兼备使用、调优以及原理的介绍\n《Apache Pulsar in Action》：目录布局不太好，但针对个别知识点讲得比较精细，可作为补充读物\nPulsar官方整理资料大全：收录了大量Pulsar使用、原理、业务场景等精彩的文章资料\n五、想对本系列文章读者说的话 坦白了说，现如今的网络环境不像过去那么友好(这个观点不做任何讨论，如果你觉得不对，那么你是对的)，如今还能坚持在网络上输出高质量的作者都是值得尊敬的。我之所以写这系列文章的目的有以下三点：1. 接触技术这些年，受到不少大佬文章的熏陶，技术和思维都有了不少的提升，因此也想做些回馈于技术社区的事情 2. 我知道一定有不少对技术充满热情的小白，我希望能够以Pulsar为切入点给你带来技术的乐趣 3. 通过输出文章来倒推自己持续阅读、提升自己提炼抽象能力。\n我能保证的是以下几点\n这个系列的文章永不收费(降低大家阅读成本) 输出的内容都是经过思考的，拒绝复制张贴以及低价值的东西 尽可能以图、精简的话来协助大家伙对某个知识点的理解 不做模凌两可的解释，宁可偏激的给个错误的答案，我相信有一定依据支撑的错误答案价值大于模糊不清的概念 除此之外，由于本人的知识量以及认知有限，如果有某个知识表述不清楚或者表达有误的地方，恳请指出，大家一起学习共同进步～\n","date":"2024-03-16T10:34:56+08:00","image":"https://sherlock-lin.github.io/p/pulsar%E4%BB%8E%E5%85%A5%E8%BF%B7%E5%88%B0%E5%85%A5%E9%AD%94%E4%B9%8B%E8%B7%AF/image-20240316103554735_hu8487334652089443369.png","permalink":"https://sherlock-lin.github.io/p/pulsar%E4%BB%8E%E5%85%A5%E8%BF%B7%E5%88%B0%E5%85%A5%E9%AD%94%E4%B9%8B%E8%B7%AF/","title":"Pulsar从入迷到入魔之路"},{"content":"一、引言 今天跟着 官方文档 基于docker玩一把Pulsar IO吧\n二、概要 在用户能够轻松的将消息队列跟其他系统(数据库、其他消息系统)一起使用时，消息队列的作用才是最强大的。而Pulsar IO connectors可以让你很轻松的创建、部署以及管理这些跟外部系统的连接，例如mysql、kafka、cassandra等。\nPulsar connector分为Source和Sink两种，Source connector会将数据从外部系统喂给Pulsar，而Sink connector负责将数据从Pulsar喂给外部系统。\nPulsar connector是一种特殊的Function，只不过这个Function持有其他系统的客户端作为pulsar与其他系统的桥梁，它在处理保证上跟Function是一致的，分别是最多一次、至少一次、精准一次。处理保证不仅依靠Pulsar，还跟外部系统相关以及实现逻辑相关。\n最多一次：发给connector的消息最多处理一次或者不做处理 至少一次：发给connector的消息处理一次或者多次 精准一次：发给connector的消息只处理一次 三、实战 1.安装connector 在 这里 下载对应的connector，先选择对应的版本，在点进 connectors 目录选择对应的source或者sink\n将下载的nar文件放到pulsar安装地址的connectors 目录下(没有需要创建)\n启动Pulsar\n通过指令查看服务connector信息，先输出下面这样的信息就说明connector已经注册到Pulsar上面了\n1 curl -s http://localhost:8080/admin/v2/functions/connectors 2. 安装Cassandra 基于 brew install --cask --appdir=/Applications docker 安装docker(仅针对mac环境)\n基于docker运行 cassandra，成功运行后通过 docker ps可以看到Cassandra服务已经起来了\n1 docker run -d --rm --name=cassandra -p 9042:9042 cassandra:3.11 通过 docker exec -ti cassandra cqlsh localhost 进入Cassandra服务的容器，并通过以下指令进行库表的初始化\n1 2 3 4 5 CREATE KEYSPACE pulsar_test_keyspace WITH replication = {\u0026#39;class\u0026#39;:\u0026#39;SimpleStrategy\u0026#39;, \u0026#39;replication_factor\u0026#39;:1}; USE pulsar_test_keyspace; CREATE TABLE pulsar_test_table (key text PRIMARY KEY, col text); 先查询该表确保没有数据 select * from pulsar_test_table;\n3. 功能验证 写配置文件cassandra-sink.yml\n1 2 3 4 5 6 configs: roots: \u0026#34;localhost:9042\u0026#34; keyspace: \u0026#34;pulsar_test_keyspace\u0026#34; columnFamily: \u0026#34;pulsar_test_table\u0026#34; keyname: \u0026#34;key\u0026#34; columnName: \u0026#34;col\u0026#34; 启动写Cassandra的sink，启动后通过指令查看显示sink已经正常启动\n1 2 3 4 5 6 7 pulsar-admin sinks create \\ --tenant public \\ --namespace default \\ --name cassandra-test-sink \\ --sink-type cassandra \\ --sink-config-file examples/cassandra-sink.yml \\ --inputs test_cassandra 执行命令批量往pulsar中写入数据，看是否会正常输出到Cassandra中\n1 for i in {0..9}; do pulsar-client produce -m \u0026#34;key-$i\u0026#34; -n 1 test_cassandra; done 由于上面的操作是有延迟的，所以不断的查询Cassandra的表是可以看到数据在逐步的增加，并最终写满十条数据\n四、总结 纸上得来终觉浅，绝知此事要躬行。 学习不能仅仅停留在纸面上或者理论，脱离使用去探讨设计或者源码都是不切实际的。因此今天一起体验了一把Pulsar IO，除此之外Pulsar还提供了非常丰富的跟其他系统交互的Connector，详细可以看上面发的下载地址并尝试使用自己感兴趣的Connector感受下实操的快乐～\n","date":"2024-03-13T10:34:55+08:00","image":"https://sherlock-lin.github.io/p/pulsario%E5%AE%9E%E6%88%98/image-20240313191618099_hu9489298534787853151.png","permalink":"https://sherlock-lin.github.io/p/pulsario%E5%AE%9E%E6%88%98/","title":"PulsarIO实战"},{"content":"引言 Pulsar Function中最重要的角色是Worker，而Worker中最核心的类就是调度管理器SchedulerManager，本篇博客就专门对它进行剖析\n一、正文 SchedulerManager职责有以下两点\n任务调度 重平衡 任务调度是SchedulerManager最重要的逻辑，咱们通过 pulsar-admin functions create xxx 启动一个function实例时，Pulsar 如何决定在哪台worker所在节点启动这个function实例，这就是任务调度要做的事情，除此之外任务调度还包含function更新、删除以及任务的生命周期管理。而重平衡是稳定性相关的，当咱们的function实例分布不均衡时，可能会导致涝的涝死，旱的旱死，因此需要咱们做function实例的重新分配，这也是SchedulerManager要做的事情\n二、任务调度 在开始讲解任务调度之前，咱们先看看下面这张图\n图中的上下方分别是两个worker进程实例，在咱们通过 bin/pulsar functions-worker 启动worker进程时服务内部会启动consumer并以failover方式来在Broker进行消息订阅消费，failover机制保证同一时间内只有一个Consumer能进行订阅，因此Worker通过这个机制来进行选主，成功订阅的worker晋升为leader负责任务的派发，未成功订阅的worker变为follwer并通过Reader监听Broker中的任务。其中Leader和Follwer的主要逻辑分别如下\nworker-leader 通过Consumer-failver机制成为leader后，会立马启动一个Producer负责往Broker中发送任务消息，每当有新建启动function请求进来，就会往Broker中写入一条对应的任务消息。除此之外Leader还会通过Consumer消费Broker中保存的元数据，并以Map数据结构存在内存中作为Leader的元数据管理的数据来源，这里的数据有Function信息、Instance信息等，在接受到我不请求是也会同步更新这里，同时也会定期的根据元数据检查来排查出异常的Function、Instance等。\nworker-follwer 在worker成为follower后，会启动Reader监听Broker中的任务，当监听到有新任务时会调用任务管理器进行处理，例如在有新增启动Function时，如果是配置的线程级别则以线程方式启动Instance，如果是配置的进程级别则会拉起一个独立的Instance进程进行Function任务的执行。\n源码解析 调度逻辑的主要入口在SchedulerManager的invokeScheduler方法\n获取所有Function 以及Instance 删除所有不存在的Function/Instance的分配器，同时更新分配器的信息 如果元数据管理器中存有Function，但是这个Function对应的InstanceID不存在于Runtime管理器的Runtime列表 则从Master元数据中删除，并通过内部Topic发送一条清除这个Instance给Worker 通过元数据比较获取需要新增的Instance并包装成Assignment，通过内部Topic发送这条新增Instance给Worker 获取还未启动的Instance实例，并通过RoundRobinScheduler的schedule方法派发到对应的Worker节点进行启动 循环通过FunctionRuntimeManager.processAssignment启动Instance 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 void invokeScheduler() { long startTime = System.nanoTime(); //获取当前worker集群中可用的worker节点信息 Set\u0026lt;String\u0026gt; availableWorkers = getCurrentAvailableWorkers(); //获取所有Function List\u0026lt;FunctionMetaData\u0026gt; allFunctions = functionMetaDataManager.getAllFunctionMetaData(); //获取所有Instance Map\u0026lt;String, Function.Instance\u0026gt; allInstances = computeAllInstances(allFunctions, functionRuntimeManager.getRuntimeFactory().externallyManaged()); //获取当前每个 workerId-\u0026gt;(InstanceId—\u0026gt;任务) Map\u0026lt;String, Map\u0026lt;String, Assignment\u0026gt;\u0026gt; workerIdToAssignments = functionRuntimeManager .getCurrentAssignments(); // 初始化调度状态记录器 SchedulerStats schedulerStats = new SchedulerStats(workerIdToAssignments, availableWorkers); //delete assignments of functions and instances that don\u0026#39;t exist anymore Iterator\u0026lt;Map.Entry\u0026lt;String, Map\u0026lt;String, Assignment\u0026gt;\u0026gt;\u0026gt; it = workerIdToAssignments.entrySet().iterator(); while (it.hasNext()) { Map.Entry\u0026lt;String, Map\u0026lt;String, Assignment\u0026gt;\u0026gt; workerIdToAssignmentEntry = it.next(); Map\u0026lt;String, Assignment\u0026gt; functionMap = workerIdToAssignmentEntry.getValue(); // remove instances that don\u0026#39;t exist anymore functionMap.entrySet().removeIf(entry -\u0026gt; { String fullyQualifiedInstanceId = entry.getKey(); boolean deleted = !allInstances.containsKey(fullyQualifiedInstanceId); if (deleted) { Assignment assignment = entry.getValue(); MessageId messageId = publishNewAssignment(assignment.toBuilder().build(), true); // Directly update in memory assignment cache since I am leader log.info(\u0026#34;Deleting assignment: {}\u0026#34;, assignment); functionRuntimeManager.deleteAssignment(fullyQualifiedInstanceId); // update message id associated with current view of assignments map lastMessageProduced = messageId; // 更新状态 schedulerStats.removedAssignment(assignment); } return deleted; }); // update assignment instances in case attributes of a function gets updated for (Map.Entry\u0026lt;String, Assignment\u0026gt; entry : functionMap.entrySet()) { String fullyQualifiedInstanceId = entry.getKey(); Assignment assignment = entry.getValue(); Function.Instance instance = allInstances.get(fullyQualifiedInstanceId); if (!assignment.getInstance().equals(instance)) { functionMap.put(fullyQualifiedInstanceId, assignment.toBuilder().setInstance(instance).build()); Assignment newAssignment = assignment.toBuilder().setInstance(instance).build().toBuilder().build(); MessageId messageId = publishNewAssignment(newAssignment, false); // Directly update in memory assignment cache since I am leader log.info(\u0026#34;Updating assignment: {}\u0026#34;, newAssignment); functionRuntimeManager.processAssignment(newAssignment); // update message id associated with current view of assignments map lastMessageProduced = messageId; //update stats schedulerStats.updatedAssignment(newAssignment); } if (functionMap.isEmpty()) { it.remove(); } } } List\u0026lt;Assignment\u0026gt; currentAssignments = workerIdToAssignments .entrySet() .stream() .filter(workerIdToAssignmentEntry -\u0026gt; { String workerId = workerIdToAssignmentEntry.getKey(); // remove assignments to workers that don\u0026#39;t exist / died for now. // wait for failure detector to unassign them in the future for re-scheduling return availableWorkers.contains(workerId); }) .flatMap(stringMapEntry -\u0026gt; stringMapEntry.getValue().values().stream()) .collect(Collectors.toList()); //获取未分配的Function任务 Pair\u0026lt;List\u0026lt;Function.Instance\u0026gt;, List\u0026lt;Assignment\u0026gt;\u0026gt; unassignedInstances = getUnassignedFunctionInstances(workerIdToAssignments, allInstances); workerStatsManager.scheduleStrategyExecTimeStartStart(); //触发任务真正调度派发的入口 List\u0026lt;Assignment\u0026gt; assignments = scheduler.schedule(unassignedInstances.getLeft(), currentAssignments, availableWorkers); workerStatsManager.scheduleStrategyExecTimeStartEnd(); assignments.addAll(unassignedInstances.getRight()); if (log.isDebugEnabled()) { log.debug(\u0026#34;New assignments computed: {}\u0026#34;, assignments); } isCompactionNeeded.set(!assignments.isEmpty()); //更新到Leader内存中的元数据 for (Assignment assignment : assignments) { MessageId messageId = publishNewAssignment(assignment, false); // Directly update in memory assignment cache since I am leader log.info(\u0026#34;Adding assignment: {}\u0026#34;, assignment); functionRuntimeManager.processAssignment(assignment); // update message id associated with current view of assignments map lastMessageProduced = messageId; // update stats schedulerStats.newAssignment(assignment); } log.info(\u0026#34;Schedule summary - execution time: {} sec | total unassigned: {} | stats: {}\\n{}\u0026#34;, (System.nanoTime() - startTime) / Math.pow(10, 9), unassignedInstances.getLeft().size(), schedulerStats.getSummary(), schedulerStats); } 三、总结 以上就是Pulsar中Worker的调度过程，这里主要是以Worker独立部署的方式进行讲解的，基于Broker启动的Worker感兴趣的朋友可以自行跟踪下代码。\n","date":"2024-03-05T10:34:57+08:00","image":"https://sherlock-lin.github.io/p/worker%E8%B0%83%E5%BA%A6%E7%AE%A1%E7%90%86%E5%99%A8%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/image-20240305140605827-9618768_hu15216224640484705908.png","permalink":"https://sherlock-lin.github.io/p/worker%E8%B0%83%E5%BA%A6%E7%AE%A1%E7%90%86%E5%99%A8%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/","title":"Worker调度管理器原理解析"},{"content":"引言小故事 张三在一家小型互联网公司上班，由于公司实行的996，因此经常有同事“不辞而别”，为了工作的正常推进，团队内达成了某种默契，这种默契就是通过某个规则来选出一个同事，这个同事除了工作之余还有额外看看每天是否有同事“不辞而别”，当发现有同事李四离职时，就会去把李四负责的工作的内容进行拆分给其他的同事进行处理。整个过程大致如下图\n由上图可以看到这个公司通过一个签到本和工作进度表来完成整个流程，每个同事上班时都要在签到本上进行签到，每天下班前要在工作进度表上同步今天的工作进展；例如今天李四“不辞而别”溜了，张三在签到本上看到李四没有签到记录，就判定这家伙不干了同时在工作进度表中把李四的任务进行拆分给大狗和二狗来做\u0026hellip;\n通过上面的故事会发现有几个问题\n张三是通过什么规则被选成“监督者”的？ 如果张三也不辞而别呢？ 为啥要通过签到本的方式，而不是张三直接去挨个挨个看？ \u0026hellip;. 咱们可以带着这些种种心里的疑惑看下面的文章，这个故事其实是一个分布式存储组件的雏形，刚刚所讨论的那些问题也是这些组件所会遇到的且大部分都是有解法的，所以咱们接下来就来看看bookkeeper这个分布式存储组件是如何解决上述问题的\nbookkeeper基础 “硬件无法保证不故障”，在这个大前提下，所有运行在硬件上的存储组件都一定会做一件很重要的事情，这件事就是数据恢复，要么是在组件内部来做，要么是在组件外部来做。\nbk是一个具有容错的分布式存储组件，同一份数据会有多个副本，分别存在多个bookie中来提供容错保证，那么当一台bookie不可用时，其上面保存的数据都少了一个副本，如果不进行数据恢复/复制的话再有其他的bookie不可用就很容易造成数据的丢失。因此bk自身内部提供了数据恢复的机制，今天通篇大论都是围绕bk的这个机制进行展开的\n数据恢复一般分为手动和自动，bk同时支持这两种方式，接下来就看看具体怎么操作的\n手动恢复\n1 2 bin/bookkeeper shell recover 192.168.1.10:3181 指定bookie机器来恢复 bin/bookkeeper shell recover 192.168.1.10:3181 --ledger ledgerID 指定bookie机器上的某个ledgerId进行恢复 在执行手动恢复时，会发生以下四个步骤\n客户端从zookeeper读取Ledger信息 根据Ledger信息确定需要做数据复制的Ledger(根据Ledger中存有被哪些bookie存储的元信息来确定) 在客户端启动一个做数据恢复的进程，针对需要做数据复制的Ledger进行数据恢复 一旦所有Ledger被标记为全副本了，则恢复动作完成 自动恢复\n1 2 3 bin/bookkeeper autorecovery bookie 集群开启自动恢复机制 bin/bookkeeper shell autorecovery -disable 关闭自动恢复机制 bin/bookkeeper shell autorecovery -enable 关闭恢复后再重新开启 除了通过指令的方式启动，bk还支持配置的方式，只需要在bookie节点配置autoRecoveryDaemonEnabled为false，这个bookie节点在启动的时候也同样会启动autorecovery服务\nautorecovery机制 上一章节讲了怎么使用，本章节主要讲明autorecovery这个机制\n自动恢复机制中有两个角色Auditor和replication worker，在启动自动恢复机制后，会在每个bookie实例中启动这两个角色\nAuditor\nbk集群中的Auditor们会通过zookeeper选举产生一位leader，这个leader负责监听 zookeeper /ledgers/available 节点变化情况来判定是否要做数据恢复动作，因为所有节点启动都会注册在上面，如果有服务不可用由于zookeeper的临时目录机制，会自动删除在此目录下自己节点的信息，因此leader通过watch机制可以轻松感知到有节点不可用，当Auditor leader感知到有节点不可用时，会将此bookie所负责的所有Ledger加在zookeeper /ledgers/underreplicated 路径下，通过这种方式通知replication worker做数据恢复过程\nreplication worker\n每个replication worker都会监听 /ledgers/underreplicated 地址，在监听到有数据恢复任务时，会在 /ledgers/underreplication/locks下添加锁从而避免并发问题；如果在开始恢复前发下当前Ledger的Fragment还处于写入中的状态，replication worker会先尝试等待它写完再做数据恢复动作，但如果等了一段时间还没写完会通过Fence机制处理再做复制，同时开启一个新的Fragment给客户端做数据的继续写入\n启动工作流程\n参照上图，在服务器节点上执行bin/bookkeeper autorecovery bookie后会发生以下步骤\n通过exec shell指令调用操作系统拉起AutoRecoveryMain 这个Java进程 AutoRecoveryMain进程启动时会同时启动Auditor线程和ReplicationWorker线程，由于环境中可能会启动多个AutoRecoveryMain进程来做HA高可用，因此多个Auditor线程会通过zookeeper选举来产生一个Auditor Leader 由于bookie集群用zookeeper来做集群感知，因此Auditor Leader只需要通过watch监听zookeeper上 bookie所注册的地址就能感知到是否有bookie节点不可用；当bookie节点不可用时一般就不会上报心跳给zookeeper，zookeeper就会将该节点创建的临时目录进行删除并告知添加watch的Auditor Leader Auditor Leader收到通知后会去zookeeper查询该不可用bookie所负责的Ledger列表，理论上这些Ledger都是需要做数据恢复的，因此会将它们放在zookeeper的/ledgers/underreplicated 目录下来通知ReplicationWorker ReplicationWorker通过watch监听到此目录有需要做数据恢复的Ledger后，会先在zk加锁再进行数据恢复逻辑；通过将Ledger划分为多个Fragment来轮训进行数据恢复，通过读取其他正常bookie上该Ledger的数据并写到其他没有该数据的bookie的节点上从而保证每份数据都有多个副本，直到将/ledgers/underreplicated 下的所有Ledger进行复制完，本次 autorecovery就算完成了。而Auditor线程和ReplicationWorker线程会不停的监听zookeeper直到下一个bookie节点不可用 通过此机制给bookkeeper提高了稳定性以及高可用能力，在有个别节点挂掉的时候依然能自动做到数据完备不丢，这种设计是一个成熟的组件该具备的能力\nautorecovery启动源码 源码主要分 启动流程以及工作流程进行讲解，同时在这里给需要阅读的朋友提供一个可能会用上的“词典”\n1 2 3 4 5 6 7 8 9 10 AutoRecoveryMain核心类, 主要负责启动AutoRecovery服务 AutoRecoveryService核心类，主要负责AutoRecovery相关的服务 LedgerManager 对外提供一个管理ledger的api，对内负责如何将ledger的元数据存储在kv存储上。提供增删、读写、注册/注销六个核心接口 AbstractZkLedgerManager 抽象类 LedgerIdGenerator：基于zk实现全局唯一递增的ledgerId ZkLedgerUnderreplicationManager：管理未完成复制的Ledger ZkLedgerAuditorManager：管理Auditor ReplicationWorker：负责从ZkLedgerUnderreplicationManager中获取未完成复制的Ledger并进行复制，每隔rwRereplicateBackoffMs触发一次 LedgerFragment：组成Ledger的单元，也是恢复复制的单元 EmbeddedServer：启动bk实例的节点 从现在开始跟踪启动的源码，在客户端执行 bin/bookkeeper autorecovery bookie 后会走到 bookkeeper/bin/bookkeeper 这个脚本下面的这行逻辑\n1 2 if [ ${COMMAND} == \u0026#34;autorecovery\u0026#34; ]; then exec \u0026#34;${JAVA}\u0026#34; ${OPTS} ${JMX_ARGS} org.apache.bookkeeper.replication.AutoRecoveryMain --conf ${BOOKIE_CONF} $@ 逻辑非常清晰，其实就是通过shell启动AutoRecovery 这样一个独立的Java进程，专门负责做故障数据恢复。JVM会从启动类的main方法进行引导执行，因此咱们接下来从AutoRecoveryMain的main方法作为入口来看看后面会发生哪些事情\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 public static void main(String[] args) { //调用真正执行的方法，开源项目中真正执行某个操作会以do前缀来进行修饰 int retCode = doMain(args); .... } static int doMain(String[] args) { ServerConfiguration conf; try { //根据shell启动命令中指定的配置地址加载成配置对象 conf = parseArgs(args); } catch (IllegalArgumentException iae) { .... } LifecycleComponent server; try { //构建AutoRecoveryServer对象，比较重要的方法 server = buildAutoRecoveryServer(new BookieConfiguration(conf)); } catch (Exception e) { .... } try { //启动AutoRecoveryServer对象 ComponentStarter.startComponent(server).get(); } catch (InterruptedException ie) { .... } return ExitCode.OK; } 通过这里可以发现AutoRecoveryMain的main方法只是做一个引导的动作，最终启动的是AutoRecoveryServer对象。因此让我们深入看看这个服务的构造以及启动的流程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public static LifecycleComponentStack buildAutoRecoveryServer(BookieConfiguration conf) throws Exception { LifecycleComponentStack.Builder serverBuilder = LifecycleComponentStack.newBuilder() .withName(\u0026#34;autorecovery-server\u0026#34;); // 1. 创建StatsProviderService对象，主要用来记录AutoRecovery服务的各项指标状态 StatsProviderService statsProviderService = new StatsProviderService(conf); .... // 2. 通过构造函数的方式创建AutoRecoveryService对象，这是核心的代码 AutoRecoveryService autoRecoveryService = new AutoRecoveryService(conf, rootStatsLogger); .... // 3. 创建BKHttpServiceProvider对象，主要用来对外提供http服务，支持通过http方式读取内部状态信息等 if (conf.getServerConf().isHttpServerEnabled()) { BKHttpServiceProvider provider = new BKHttpServiceProvider.Builder() .setAutoRecovery(autoRecoveryService.getAutoRecoveryServer()) .setServerConfiguration(conf.getServerConf()) .setStatsProvider(statsProviderService.getStatsProvider()).build(); HttpService httpService = new HttpService(provider, conf, rootStatsLogger); .... } return serverBuilder.build(); } 再看AutoRecoveryService的构造函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 public AutoRecoveryService(BookieConfiguration conf, StatsLogger statsLogger) throws Exception { super(NAME, conf, statsLogger); //通过构造函数创建AutoRecoveryMain，AutoRecoveryMain是AutoRecoveryService的成员变量 //进入看看它的实现 this.main = new AutoRecoveryMain( conf.getServerConf(), statsLogger); } public AutoRecoveryMain(ServerConfiguration conf, StatsLogger statsLogger) throws IOException, InterruptedException, KeeperException, UnavailableException, CompatibilityException { .... //创建AuditorElector对象，负责选举产生Auditor Leader auditorElector = new AuditorElector( BookieImpl.getBookieId(conf).toString(), conf, bkc, statsLogger.scope(AUDITOR_SCOPE), false); //创建ReplicationWorker对象，负责做数据的拷贝工作 replicationWorker = new ReplicationWorker( conf, bkc, false, statsLogger.scope(REPLICATION_WORKER_SCOPE)); deathWatcher = new AutoRecoveryDeathWatcher(this); } 服务构造的逻辑差不多就跟到这了，我们知道最终是为了创建AuditorElector和ReplicationWorker这两个对象就够了。服务启动这块从上面的 ComponentStarter.startComponent(server).get(); 进行跟踪\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public static CompletableFuture\u0026lt;Void\u0026gt; startComponent(LifecycleComponent component) { .... //调用start方法，这里涉及上采用了模版方法设计模式以及闭包，本质上就是就是调用创建的 //StatsProviderService、\tAutoRecoveryService、HttpService这三个服务的doStart方法 component.start(); .... } protected void doStart() { //还是调的AutoRecoveryMain方法的start方法 this.main.start(); } public void start() { //启动auditorElector服务 auditorElector.start(); //启动replicationWorker服务 replicationWorker.start(); .... deathWatcher.start(); } 结合上面的可以发现AutoRecovery的启动本质上就是启动AuditorElector和ReplicationWorker这两个服务，因此接下来咱们就来看看这两个服务的start过程，先来看看AuditorElector\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public Future\u0026lt;?\u0026gt; start() { running.set(true); //提交选举任务 return submitElectionTask(); } Future\u0026lt;?\u0026gt; submitElectionTask() { Runnable r = new Runnable() { @Override public void run() { .... //创建一个Auditor对象并进行启动 auditor = new Auditor(bookieId, conf, bkc, false, statsLogger); auditor.start(); } }; try { //异步执行以上逻辑 return executor.submit(r); } catch (RejectedExecutionException e) { .... } } 在这里其实就是对Auditor对象进行初始化以及启动，再进一步跟踪\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public Auditor(final String bookieIdentifier, ServerConfiguration conf, BookKeeper bkc, boolean ownBkc, BookKeeperAdmin admin, boolean ownAdmin, StatsLogger statsLogger) throws UnavailableException { .... //调用初始化Auditor对象逻辑 initialize(conf, bkc); .... } private void initialize(ServerConfiguration conf, BookKeeper bkc) throws UnavailableException { try { LedgerManagerFactory ledgerManagerFactory = bkc.getLedgerManagerFactory(); ledgerManager = ledgerManagerFactory.newLedgerManager(); this.bookieLedgerIndexer = new BookieLedgerIndexer(ledgerManager); this.ledgerUnderreplicationManager = ledgerManagerFactory .newLedgerUnderreplicationManager(); .... lostBookieRecoveryDelayBeforeChange = this.ledgerUnderreplicationManager.getLostBookieRecoveryDelay(); } catch (CompatibilityException ce) { .... } } 看完了初始化逻辑，再继续看下Auditor的启动逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 public void start() { LOG.info(\u0026#34;I\u0026#39;m starting as Auditor Bookie. ID: {}\u0026#34;, bookieIdentifier); synchronized (this) { .... try { //1. 监听bookie变更事件，本质上就是在zk /ledgers/available 目录下增加watch监听节点的变动 //这里还会监听 只读bookie 节点的变动 watchBookieChanges(); //从zk获取处于可用的bk节点列表 knownBookies = getAvailableBookies(); } catch (BKException bke) { .... } try { //1. 在感知到有bookie节点不可用时回调LostBookieRecoveryDelayChangedCb进行逻辑处理 this.ledgerUnderreplicationManager .notifyLostBookieRecoveryDelayChanged(new LostBookieRecoveryDelayChangedCb()); } catch (UnavailableException ue) { .... } try { //1. 感知到有Ledger的副本少时触发，跟上面一样也是通过回调方式进行处理 this.ledgerUnderreplicationManager.notifyUnderReplicationLedgerChanged( new UnderReplicatedLedgersChangedCb()); } catch (UnavailableException ue) { .... } scheduleBookieCheckTask(); //启动一个线程检查Ledger的状态 scheduleCheckAllLedgersTask(); schedulePlacementPolicyCheckTask(); scheduleReplicasCheckTask(); } } 这些就是Auditor启动的逻辑，接下来再看看ReplicationWorker的启动逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 public void start() { //workerThread实际上就是一个BookieThread对象 this.workerThread.start(); } public void run() { workerRunning = true; while (workerRunning) { try { //核心逻辑就是循环调用rereplicate方法 if (!rereplicate()) { LOG.warn(\u0026#34;failed while replicating fragments\u0026#34;); waitBackOffTime(rwRereplicateBackoffMs); } } catch (InterruptedException e) { .... } } LOG.info(\u0026#34;ReplicationWorker exited loop!\u0026#34;); } private boolean rereplicate() throws InterruptedException, BKException, UnavailableException { //获取需要做数据恢复的Ledger long ledgerIdToReplicate = underreplicationManager .getLedgerToRereplicate(); Stopwatch stopwatch = Stopwatch.createStarted(); boolean success = false; try { //进行数据恢复 success = rereplicate(ledgerIdToReplicate); } finally { .... } return success; } autorecovery工作源码 这块由于逻辑相对较多，因此针对autorecovery工作流程单独开一章。经过上面我们可以清晰的知道在经过启动后都发生了哪些事情，接下来咱们看看autorecovery真正工作的逻辑。在Auditor start的时候，会通过监听zookeeper来感知数据的动态变化\n1 2 3 4 5 6 7 8 public void start() { //感知bookie节点下线，将这些bookie上管理的Ledger标记为需要备份放到zookeeper上 this.ledgerUnderreplicationManager .notifyLostBookieRecoveryDelayChanged(new LostBookieRecoveryDelayChangedCb()); //感知Ledger副本变动，统计到指标里 this.ledgerUnderreplicationManager.notifyUnderReplicationLedgerChanged( new UnderReplicatedLedgersChangedCb()); } 上述两个唤醒方法主要是通过watch感知zookeeper事件，所以咱们主要看回调类里面的处理逻辑，先看下LostBookieRecoveryDelayChangedCb类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 private class LostBookieRecoveryDelayChangedCb implements GenericCallback\u0026lt;Void\u0026gt; { @Override public void operationComplete(int rc, Void result) { .... Auditor.this.ledgerUnderreplicationManager .notifyLostBookieRecoveryDelayChanged(LostBookieRecoveryDelayChangedCb.this); .... //提交事件变动处理任务，进去看看 Auditor.this.submitLostBookieRecoveryDelayChangedEvent(); } } synchronized Future\u0026lt;?\u0026gt; submitLostBookieRecoveryDelayChangedEvent() { return executor.submit(() -\u0026gt; { int lostBookieRecoveryDelay = -1; try { waitIfLedgerReplicationDisabled(); lostBookieRecoveryDelay = Auditor.this.ledgerUnderreplicationManager .getLostBookieRecoveryDelay(); .... //核心逻辑,进去看看都做了些什么 auditorBookieCheckTask.startAudit(false); } else if (auditTask != null) { LOG.info(\u0026#34;lostBookieRecoveryDelay has been set to {}, so rescheduling AuditTask accordingly\u0026#34;, lostBookieRecoveryDelay); auditTask = executor.schedule(() -\u0026gt; { auditorBookieCheckTask.startAudit(false); auditTask = null; bookiesToBeAudited.clear(); }, lostBookieRecoveryDelay, TimeUnit.SECONDS); auditorStats.getNumBookieAuditsDelayed().inc(); } } catch (InterruptedException ie) { .... } finally { if (lostBookieRecoveryDelay != -1) { lostBookieRecoveryDelayBeforeChange = lostBookieRecoveryDelay; } } }); } void startAudit(boolean shutDownTask) { try { //看起来是开始做Auditor的主要任务了，继续往下 auditBookies(); shutDownTask = false; } catch (BKException bke) { .... } } void auditBookies() throws ReplicationException.BKAuditException, InterruptedException, BKException { .... List\u0026lt;String\u0026gt; availableBookies = getAvailableBookies(); // find lost bookies Set\u0026lt;String\u0026gt; knownBookies = ledgerDetails.keySet(); //通过之前内存中存的bookie集合减去 zk当前bookie集合即可得出都有哪些bookie节点不可用了 Collection\u0026lt;String\u0026gt; lostBookies = CollectionUtils.subtract(knownBookies, availableBookies); .... //如果本次变动涉及到bookie节点不可用，则调用handleLostBookiesAsync方法处理不可用的节点 if (lostBookies.size() \u0026gt; 0) { try { FutureUtils.result( handleLostBookiesAsync(lostBookies, ledgerDetails), ReplicationException.EXCEPTION_HANDLER); } catch (ReplicationException e) { .... } .... } .... } private CompletableFuture\u0026lt;?\u0026gt; handleLostBookiesAsync(Collection\u0026lt;String\u0026gt; lostBookies, Map\u0026lt;String, Set\u0026lt;Long\u0026gt;\u0026gt; ledgerDetails) { LOG.info(\u0026#34;Following are the failed bookies: {},\u0026#34; + \u0026#34; and searching its ledgers for re-replication\u0026#34;, lostBookies); return FutureUtils.processList( Lists.newArrayList(lostBookies), //看方法名大概能猜得出来计算这些bookie上的Ledger，并针对这些Ledger进行数据恢复 //由于之前有存节点跟Ledger的映射关系，因此直接通过ledgerDetails映射表来获取这些不可用节点所负责的Ledger bookieIP -\u0026gt; publishSuspectedLedgersAsync( Lists.newArrayList(bookieIP), ledgerDetails.get(bookieIP)), null ); } protected CompletableFuture\u0026lt;?\u0026gt; publishSuspectedLedgersAsync(Collection\u0026lt;String\u0026gt; missingBookies, Set\u0026lt;Long\u0026gt; ledgers) { .... LongAdder underReplicatedSize = new LongAdder(); FutureUtils.processList( Lists.newArrayList(ledgers), ledgerId -\u0026gt; //通过读取这些Ledger的元数据，方便后续的数据恢复动作 ledgerManager.readLedgerMetadata(ledgerId).whenComplete((metadata, exception) -\u0026gt; { if (exception == null) { underReplicatedSize.add(metadata.getValue().getLength()); } }), null).whenComplete((res, e) -\u0026gt; { .... }); return FutureUtils.processList( Lists.newArrayList(ledgers), //主流程，继续往下 ledgerId -\u0026gt; ledgerUnderreplicationManager.markLedgerUnderreplicatedAsync(ledgerId, missingBookies), null ); } public CompletableFuture\u0026lt;Void\u0026gt; markLedgerUnderreplicatedAsync(long ledgerId, Collection\u0026lt;String\u0026gt; missingReplicas) { .... final String znode = getUrLedgerZnode(ledgerId); //标记需要做备份的Ledger tryMarkLedgerUnderreplicatedAsync(znode, missingReplicas, zkAcls, createFuture); return createFuture; } private void tryMarkLedgerUnderreplicatedAsync(final String znode, final Collection\u0026lt;String\u0026gt; missingReplicas, final List\u0026lt;ACL\u0026gt; zkAcls, final CompletableFuture\u0026lt;Void\u0026gt; finalFuture) { .... //将需要做数据恢复的副本进行进行proto编码 missingReplicas.forEach(builder::addReplica); .... ZkUtils.asyncCreateFullPathOptimistic( zkc, znode, urLedgerData, zkAcls, CreateMode.PERSISTENT, (rc, path, ctx, name) -\u0026gt; { if (Code.OK.intValue() == rc) { FutureUtils.complete(finalFuture, null); } else if (Code.NODEEXISTS.intValue() == rc) { //要在zookeeper将这些Ledger标记为需要做数据恢复 handleLedgerUnderreplicatedAlreadyMarked(znode, missingReplicas, zkAcls, finalFuture); } else { FutureUtils.completeExceptionally(finalFuture, KeeperException.create(Code.get(rc))); } }, null); } private void handleLedgerUnderreplicatedAlreadyMarked(final String znode, final Collection\u0026lt;String\u0026gt; missingReplicas, final List\u0026lt;ACL\u0026gt; zkAcls, final CompletableFuture\u0026lt;Void\u0026gt; finalFuture) { // get the existing underreplicated ledger data zkc.getData(znode, false, (getRc, getPath, getCtx, existingUrLedgerData, getStat) -\u0026gt; { if (Code.OK.intValue() == getRc) { // deserialize existing underreplicated ledger data final UnderreplicatedLedgerFormat.Builder builder = UnderreplicatedLedgerFormat.newBuilder(); try { TextFormat.merge(new String(existingUrLedgerData, UTF_8), builder); } catch (ParseException e) { .... } UnderreplicatedLedgerFormat existingUrLedgerFormat = builder.build(); boolean replicaAdded = false; for (String missingReplica : missingReplicas) { if (existingUrLedgerFormat.getReplicaList().contains(missingReplica)) { continue; } else { builder.addReplica(missingReplica); replicaAdded = true; } } .... //盲猜这里在zk将Ledger标志为需要做数据同步 zkc.setData(znode, newUrLedgerData, getStat.getVersion(), (setRc, setPath, setCtx, setStat) -\u0026gt; { if (Code.OK.intValue() == setRc) { FutureUtils.complete(finalFuture, null); } else if (Code.NONODE.intValue() == setRc) { tryMarkLedgerUnderreplicatedAsync(znode, missingReplicas, zkAcls, finalFuture); } else if (Code.BADVERSION.intValue() == setRc) { handleLedgerUnderreplicatedAlreadyMarked(znode, missingReplicas, zkAcls, finalFuture); } else { FutureUtils.completeExceptionally(finalFuture, KeeperException.create(Code.get(setRc))); } }, null); } else if (Code.NONODE.intValue() == getRc) { tryMarkLedgerUnderreplicatedAsync(znode, missingReplicas, zkAcls, finalFuture); } else { FutureUtils.completeExceptionally(finalFuture, KeeperException.create(Code.get(getRc))); } }, null); } 从ReplicationWorker的rereplicate方法开始就是真正做数据恢复的过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 private boolean rereplicate(long ledgerIdToReplicate) throws InterruptedException, BKException, UnavailableException { .... //获取需要做数据恢复的Ledger的处理对象LedgerHandle try (LedgerHandle lh = admin.openLedgerNoRecovery(ledgerIdToReplicate)) { //通过对Ledger进行分解成更小数据恢复单位LedgerFragment，后续分别对LedgerFragment进行数据恢复 Set\u0026lt;LedgerFragment\u0026gt; fragments = getUnderreplicatedFragments(lh, conf.getAuditorLedgerVerificationPercentage()); .... for (LedgerFragment ledgerFragment : fragments) { .... try { //对LedgerFragment进行数据恢复 admin.replicateLedgerFragment(lh, ledgerFragment, onReadEntryFailureCallback); numFragsReplicated++; if (ledgerFragment.getReplicateType() == LedgerFragment .ReplicateType.DATA_NOT_ADHERING_PLACEMENT) { numNotAdheringPlacementFragsReplicated++; } } catch (BKException.BKBookieHandleNotAvailableException e) { .... } } .... fragments = getUnderreplicatedFragments(lh, conf.getAuditorLedgerVerificationPercentage()); .... } catch (BKNoSuchLedgerExistsOnMetadataServerException e) { .... } finally { .... } } 继续进一步看admin.replicateLedgerFragment的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 public void replicateLedgerFragment(LedgerHandle lh, final LedgerFragment ledgerFragment, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException, BKException { .... //继续往下跟踪 replicateLedgerFragment(lh, ledgerFragment, targetBookieAddresses, onReadEntryFailureCallback); } private void replicateLedgerFragment(LedgerHandle lh, final LedgerFragment ledgerFragment, final Map\u0026lt;Integer, BookieId\u0026gt; targetBookieAddresses, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException, BKException { .... //在这里看到这个恢复其实是异步处理的过程，继续往下 asyncRecoverLedgerFragment(lh, ledgerFragment, cb, targetBookieSet, onReadEntryFailureCallback); .... } private void asyncRecoverLedgerFragment(final LedgerHandle lh, final LedgerFragment ledgerFragment, final AsyncCallback.VoidCallback ledgerFragmentMcb, final Set\u0026lt;BookieId\u0026gt; newBookies, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException { //发现会调用LedgerFragmentReplicator对象进行数据恢复，继续往下 lfr.replicate(lh, ledgerFragment, ledgerFragmentMcb, newBookies, onReadEntryFailureCallback); } void replicate(final LedgerHandle lh, final LedgerFragment lf, final AsyncCallback.VoidCallback ledgerFragmentMcb, final Set\u0026lt;BookieId\u0026gt; targetBookieAddresses, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException { Set\u0026lt;LedgerFragment\u0026gt; partionedFragments = splitIntoSubFragments(lh, lf, bkc.getConf().getRereplicationEntryBatchSize()); .... //继续往下看实现 replicateNextBatch(lh, partionedFragments.iterator(), ledgerFragmentMcb, targetBookieAddresses, onReadEntryFailureCallback); } private void replicateNextBatch(final LedgerHandle lh, final Iterator\u0026lt;LedgerFragment\u0026gt; fragments, final AsyncCallback.VoidCallback ledgerFragmentMcb, final Set\u0026lt;BookieId\u0026gt; targetBookieAddresses, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) { if (fragments.hasNext()) { try { //来了，一般有Internal的地方都会有实现的干货，继续进去 replicateFragmentInternal(lh, fragments.next(), new AsyncCallback.VoidCallback() { @Override public void processResult(int rc, String v, Object ctx) { if (rc != BKException.Code.OK) { ledgerFragmentMcb.processResult(rc, null, null); } else { replicateNextBatch(lh, fragments, ledgerFragmentMcb, targetBookieAddresses, onReadEntryFailureCallback); } } }, targetBookieAddresses, onReadEntryFailureCallback); } catch (InterruptedException e) { ..... } } else { ledgerFragmentMcb.processResult(BKException.Code.OK, null, null); } } private void replicateFragmentInternal(final LedgerHandle lh, final LedgerFragment lf, final AsyncCallback.VoidCallback ledgerFragmentMcb, final Set\u0026lt;BookieId\u0026gt; newBookies, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException { .... //针对每个Entry对象循环做数据恢复，Entry是BK里最小的数据单元 for (final Long entryId : entriesToReplicate) { recoverLedgerFragmentEntry(entryId, lh, ledgerFragmentEntryMcb, newBookies, onReadEntryFailureCallback); } } private void recoverLedgerFragmentEntry(final Long entryId, final LedgerHandle lh, final AsyncCallback.VoidCallback ledgerFragmentEntryMcb, final Set\u0026lt;BookieId\u0026gt; newBookies, final BiConsumer\u0026lt;Long, Long\u0026gt; onReadEntryFailureCallback) throws InterruptedException { .... long startReadEntryTime = MathUtils.nowInNano(); /* * Read the ledger entry using the LedgerHandle. This will allow us to * read the entry from one of the other replicated bookies other than * the dead one. */ //到了真正读取Entry的逻辑，继续往下 lh.asyncReadEntries(entryId, entryId, new ReadCallback() { .... } }, null); } public void asyncReadEntries(long firstEntry, long lastEntry, ReadCallback cb, Object ctx) { .... //调用异步读取逻辑 asyncReadEntriesInternal(firstEntry, lastEntry, cb, ctx, false); } void asyncReadEntriesInternal(long firstEntry, long lastEntry, ReadCallback cb, Object ctx, boolean isRecoveryRead) { if (!clientCtx.isClientClosed()) { //继续往下跟踪 readEntriesInternalAsync(firstEntry, lastEntry, isRecoveryRead) .whenCompleteAsync(new FutureEventListener\u0026lt;LedgerEntries\u0026gt;() { .... }, clientCtx.getMainWorkerPool().chooseThread(ledgerId)); } else { cb.readComplete(Code.ClientClosedException, LedgerHandle.this, null, ctx); } } CompletableFuture\u0026lt;LedgerEntries\u0026gt; readEntriesInternalAsync(long firstEntry, long lastEntry, boolean isRecoveryRead) { //构造读数据的对象 PendingReadOp op = new PendingReadOp(this, clientCtx, firstEntry, lastEntry, isRecoveryRead); //运行起来，跟进去瞅瞅 op.run(); return op.future(); } public void run() { //无他，继续往下 initiate(); } void initiate() { .... do { //决定是串行读取数据还是并行读取数据 if (parallelRead) { entry = new ParallelReadRequest(ensemble, lh.ledgerId, i); } else { entry = new SequenceReadRequest(ensemble, lh.ledgerId, i); } seq.add(entry); i++; } while (i \u0026lt;= endEntryId); // read the entries. for (LedgerEntryRequest entry : seq) { //核心逻辑，这里进行数据读取操作 entry.read(); } } void read() { //继续往下看 sendNextRead(); } synchronized BookieId sendNextRead() { .... try { BookieId to = ensemble.get(bookieIndex); //发送读取请求的操作 sendReadTo(bookieIndex, to, this); .... } catch (InterruptedException ie) { .... } } void sendReadTo(int bookieIndex, BookieId to, LedgerEntryRequest entry) throws InterruptedException if (isRecoveryRead) { .... } else { //调用BK客户端进行数据的读取 clientCtx.getBookieClient().readEntry(to, lh.ledgerId, entry.eId, this, new ReadContext(bookieIndex, to, entry), BookieProtocol.FLAG_NONE); } } default void readEntry(BookieId address, long ledgerId, long entryId, ReadEntryCallback cb, Object ctx, int flags) { //继续往下 readEntry(address, ledgerId, entryId, cb, ctx, flags, null); } default void readEntry(BookieId address, long ledgerId, long entryId, ReadEntryCallback cb, Object ctx, int flags, byte[] masterKey) { //继续往下 readEntry(address, ledgerId, entryId, cb, ctx, flags, masterKey, false); } public void readEntry(final BookieId addr, final long ledgerId, final long entryId, final ReadEntryCallback cb, final Object ctx, int flags, byte[] masterKey, final boolean allowFastFail) { //获取要访问的客户端对象 final PerChannelBookieClientPool client = lookupClient(addr); .... client.obtain((rc, pcbc) -\u0026gt; { if (rc != BKException.Code.OK) { completeRead(rc, ledgerId, entryId, null, cb, ctx); } else { //调用读取逻辑 pcbc.readEntry(ledgerId, entryId, cb, ctx, flags, masterKey, allowFastFail); } }, ledgerId); } public void readEntry(final long ledgerId, final long entryId, ReadEntryCallback cb, Object ctx, int flags, byte[] masterKey, boolean allowFastFail) { //看到Internal就知道要有东西了，继续往下 readEntryInternal(ledgerId, entryId, null, null, false, cb, ctx, (short) flags, masterKey, allowFastFail); } private void readEntryInternal(final long ledgerId, final long entryId, final Long previousLAC, final Long timeOutInMillis, final boolean piggyBackEntry, final ReadEntryCallback cb, final Object ctx, int flags, byte[] masterKey, boolean allowFastFail) { .... //构造请求对象 ReadRequest.Builder readBuilder = ReadRequest.newBuilder() .setLedgerId(ledgerId) .setEntryId(entryId); .... request = withRequestContext(Request.newBuilder()) .setHeader(headerBuilder) .setReadRequest(readBuilder) .build(); .... //继续往下 writeAndFlush(channel, completionKey, request, allowFastFail); } private void writeAndFlush(final Channel channel, final CompletionKey key, final Object request, final boolean allowFastFail) { .... try { .... //跟到这里就知道最终调用了Netty的客户端来发起请求 channel.writeAndFlush(request, promise); } catch (Throwable e) { .... } } 到这里数据就发出去了，我们也能知道AutoRecovery进程是通过Netty向BK的服务端进行数据读取，那么服务端在接收到请求后又是怎么处理的呢，这里咱们从服务端接收请求的逻辑开始跟，由于BK本身也是通过Netty实例进行网络请求处理的，因此可以轻松找到BookieRequestHandler的channelRead方法监听外部网络请求\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { .... //职责分离做得很好，BookieRequestHandler只负责接收请求，逻辑处理相关的全权交给requestProcessor对象 requestProcessor.processRequest(msg, this); } public void processRequest(Object msg, BookieRequestHandler requestHandler) { Channel channel = requestHandler.ctx().channel(); if (msg instanceof BookkeeperProtocol.Request) { BookkeeperProtocol.Request r = (BookkeeperProtocol.Request) msg; restoreMdcContextFromRequest(r); try { BookkeeperProtocol.BKPacketHeader header = r.getHeader(); //非常好的一种设计，kafka的KafkaApis类里也是这样设计，服务端支持的操作在这里就能很清晰的看到 switch (header.getOperation()) { case ADD_ENTRY: processAddRequestV3(r, requestHandler); break; case READ_ENTRY: //在这里可以处理的读取请求，从这里进去看看 processReadRequestV3(r, requestHandler); break; case FORCE_LEDGER: processForceLedgerRequestV3(r, requestHandler); break; .... case WRITE_LAC: processWriteLacRequestV3(r, requestHandler); break; case READ_LAC: processReadLacRequestV3(r, requestHandler); break; case GET_BOOKIE_INFO: processGetBookieInfoRequestV3(r, requestHandler); break; case START_TLS: processStartTLSRequestV3(r, requestHandler); break; case GET_LIST_OF_ENTRIES_OF_LEDGER: processGetListOfEntriesOfLedgerProcessorV3(r, requestHandler); break; default: .... break; } } finally { MDC.clear(); } } else { .... } } private void processReadRequestV3(final BookkeeperProtocol.Request r, final BookieRequestHandler requestHandler) { //能看到BK也同时支持长轮询的方式读取数据 if (RequestUtils.isLongPollReadRequest(r.getReadRequest())) { .... read = new LongPollReadEntryProcessorV3(r, requestHandler, this, fenceThread, lpThread, requestTimer); } else { read = new ReadEntryProcessorV3(r, requestHandler, this, fenceThread); .... } if (null == threadPool) { //跟进去看看实现 read.run(); } else { .... } } public void run() { .... //执行读取操作 executeOp(); } protected void executeOp() { //这里感觉设计得不是很清晰，应该先读取数据出来再构造返回对象的，你们觉得呢？ ReadResponse readResponse = getReadResponse(); if (null != readResponse) { sendResponse(readResponse); } } protected ReadResponse getReadResponse() { // 读取Entry数据的地方，在此处深入探索下 return readEntry(readResponse, entryId, startTimeSw); } catch (Bookie.NoLedgerException e) { .... } } protected ReadResponse readEntry(ReadResponse.Builder readResponseBuilder, long entryId, Stopwatch startTimeSw) throws IOException, BookieException { //继续深入 return readEntry(readResponseBuilder, entryId, false, startTimeSw); } protected ReadResponse readEntry(ReadResponse.Builder readResponseBuilder, long entryId, boolean readLACPiggyBack, Stopwatch startTimeSw) throws IOException, BookieException { //调用Bookie的readEntry来读取数据 ByteBuf entryBody = requestProcessor.getBookie().readEntry(ledgerId, entryId); .... } public ByteBuf readEntry(long ledgerId, long entryId) throws IOException, NoLedgerException, BookieException { .... try { LedgerDescriptor handle = handles.getReadOnlyHandle(ledgerId); .... //调用真正读数据的逻辑，因为在这里能看到获取的值entry也是对外返回的 //这里调用的是LedgerDescriptor类的readEntry方法 ByteBuf entry = handle.readEntry(entryId); .... return entry; } finally { .... } } ByteBuf readEntry(long entryId) throws IOException, BookieException { //调用LedgerStorage接口，SingleDirectoryDbLedgerStorage实现类来读取Entry return ledgerStorage.getEntry(ledgerId, entryId); } public ByteBuf getEntry(long ledgerId, long entryId) throws IOException, BookieException { long startTime = MathUtils.nowInNano(); try { //继续往下跟踪 ByteBuf entry = doGetEntry(ledgerId, entryId); recordSuccessfulEvent(dbLedgerStorageStats.getReadEntryStats(), startTime); return entry; } catch (IOException e) { .... } } private ByteBuf doGetEntry(long ledgerId, long entryId) throws IOException, BookieException { .... //尝试从BK本地缓存中读取数据 ByteBuf entry = localWriteCache.get(ledgerId, entryId); //尝试从本地缓存flush中进行数据命中数据 entry = localWriteCacheBeingFlushed.get(ledgerId, entryId); // 尝试从读缓存中进行数据读取 entry = readCache.get(ledgerId, entryId); //从磁盘文件中进行数据读取, 调用的是EntryLogger接口，DefaultEntryLogger对象的readEntry方法 entry = entryLogger.readEntry(ledgerId, entryId, entryLocation); //写到读缓存中 readCache.put(ledgerId, entryId, entry); .... return entry; } public ByteBuf readEntry(long ledgerId, long entryId, long entryLocation) throws IOException, Bookie.NoEntryException { //再进一步探索 return internalReadEntry(ledgerId, entryId, entryLocation, true /* validateEntry */); } private ByteBuf internalReadEntry(long ledgerId, long entryId, long location, boolean validateEntry) throws IOException, Bookie.NoEntryException { //获取entry所在的LogId long entryLogId = logIdForOffset(location); long pos = posForOffset(location); BufferedReadChannel fc = null; int entrySize = -1; try { fc = getFCForEntryInternal(ledgerId, entryId, entryLogId, pos); ByteBuf sizeBuff = readEntrySize(ledgerId, entryId, entryLogId, pos, fc); entrySize = sizeBuff.getInt(0); if (validateEntry) { validateEntry(ledgerId, entryId, entryLogId, pos, sizeBuff); } } catch (EntryLookupException e) { .... } ByteBuf data = allocator.buffer(entrySize, entrySize); //进行数据读取 int rc = readFromLogChannel(entryLogId, fc, data, pos); .... data.writerIndex(entrySize); return data; } private int readFromLogChannel(long entryLogId, BufferedReadChannel channel, ByteBuf buff, long pos) throws IOException { BufferedLogChannel bc = entryLogManager.getCurrentLogIfPresent(entryLogId); .... //继续往下 return channel.read(buff, pos); } public int read(ByteBuf dest, long pos) throws IOException { //继续往下 return read(dest, pos, dest.writableBytes()); } public synchronized int read(ByteBuf dest, long pos, int length) throws IOException { .... while (length \u0026gt; 0) { // Check if the data is in the buffer, if so, copy it. if (readBufferStartPosition \u0026lt;= currentPosition \u0026amp;\u0026amp; currentPosition \u0026lt; readBufferStartPosition + readBuffer.readableBytes()) { int posInBuffer = (int) (currentPosition - readBufferStartPosition); int bytesToCopy = Math.min(length, readBuffer.readableBytes() - posInBuffer); dest.writeBytes(readBuffer, posInBuffer, bytesToCopy); currentPosition += bytesToCopy; length -= bytesToCopy; cacheHitCount++; } else { // We don\u0026#39;t have it in the buffer, so put necessary data in the buffer readBufferStartPosition = currentPosition; int readBytes = 0; //从磁盘读取数据到readBuffer中，再将readBuffer的数据写到 dest中作为返回值 //这里调用的是Java NIO FileChannel类的read方法来从磁盘进行数据的读取 if ((readBytes = validateAndGetFileChannel().read(readBuffer.internalNioBuffer(0, readCapacity), currentPosition)) \u0026lt;= 0) { throw new IOException(\u0026#34;Reading from filechannel returned a non-positive value. Short read.\u0026#34;); } readBuffer.writerIndex(readBytes); } } return (int) (currentPosition - pos); } 总结 在这里解答下引言小故事\n张三是通过什么规则被选成“监督者”的？\n张三是通过zookeeper的Paxos算法选举产生的\n如果张三也不辞而别呢？\n大狗和二狗也会通过zookeeper监听张三的状态，如果张三不辞而别的话，大狗二狗会通过zookeeper选举成为新的“监督者”\n为啥要通过签到本的方式，而不是张三直接去挨个挨个看？\n通过签到本的方式比较节约张三的时间，否则当员工比较多的时候并且对感知时间比较快的时候，张三就要每隔几分钟就要跑去挨个挨个看，这样没多久张三也要“不辞而别”了。通过签到本如果某个同事不签到了张三就能很轻松感知到并做相对应的处理了\n参考资料 https://bookkeeper.apache.org/docs/admin/autorecovery/ bk项目 site3/website/docs/admin/* 指令使用说明 ","date":"2024-01-04T12:00:10+08:00","image":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3bookkeeper%E4%B9%8Bautorecovery%E6%9C%BA%E5%88%B6/image-20231215102118251_hu13366877165211677337.png","permalink":"https://sherlock-lin.github.io/p/%E8%AF%A6%E8%A7%A3bookkeeper%E4%B9%8Bautorecovery%E6%9C%BA%E5%88%B6/","title":"详解bookkeeper之AutoRecovery机制"},{"content":"一、简析 schema是pulsar重要的功能之一，现在就一起从源码的视角看下管理流创建schema时客户端和服务端的表现\n客户端 客户端主要经历以下四个步骤\n创建Schema实例\n根据数据类型创建相对应的实例，例如Avro创建AvroSchema、JSON创建JSONSchema等\n获取处理Schema的对象\n管理流PulsarAdmin对象获取SchemasImpl对象，这个对象是专门处理所有schema相关的操作。除此之外PulsarAdmin对象还维护着Clusters、Brokers、Tenants等等管理维护集群的重要对象，通过这些对象可以很好的管理维护Pulsar集群\n构造SchemaInfo\n通过Schema实例创建其对应的SchemaInfo信息，里面就包括这个schema的名字、schema的结构化信息、schema类型等等，最后SchemaInfo这个对象会转成字符串发到服务端\n发送HTTP请求\n通过post请求将数据发到服务端，这里是通过Java Rest库javax.ws.rs-api进行处理的\n服务端 服务端主要经历以下四个步骤\n参数格式校验\n校验租户、命名空间的是否有效(判空、是否有特殊字符) 从缓存中根据Topic获取其对应的TopicName对象 权限校验\n判断是否是Topic的owner 判断当前用户是否有操作当前Topic的权限 SchemaRegistry注册schema\nSchemaRegistryService是服务端处理所有schema相关的对象，而schema相关的读写操作是依赖它的成员SchemaStorage进行处理的，SchemaStorage的最终是通过Bookkeeper客户端对象发送写请求\n写Bookkeeper\n通过LedgerHandle对象向Bookkeeper服务端发送写请求\n小结 schame新建流程概括起来就是，客户端构造schema信息，服务端负责schema校验，bookkeeper负责schema的存储\n二、客户端源码解析 源码跟踪 下面是通过管理流创建schema的样例代码，核心就是通过PulsarAdmin.schemas获取schema对象，这个schema对象负责所有客户端跟schema相关的操作，包括schema的增删改查等。通过方法的第二个参数可以看到是通过Schema接口提供的静态方法AVRO来构造Avro格式的schema对象，除此之外Schema接口还提供了诸如JSON、KeyValue、PROTOBUF等静态方法提供对应数据格式的schema对象，这里如果将这块构造schema对象逻辑抽成简单工厂模式可能会更合适些\n接下来就进入createSchema方法，顾名思义可以知道这个方法就是用于创建schema的，第一个参数是topic，第二个参数是SchemaInfo对象，这个对象包含了所有要新建的schema信息，这里会将它转换为PostSchemaPayload对象传递给下一个方法。PostSchemaPayload是用来请求到服务端的参数\n这个方法并不会有返回值，sync方法是处理异步结果对象，它在正常写成功情况下不会做任何操作，但如果有什么错误会往外抛出异常。这里核心逻辑是在createSchemaAsync方法\n可以看到这个方法的返回值是个异步对象，146行这里会获取当前topic对应的TopicName对象，并通过schemaPath方法构造WebTarget对象，这个对象中就包含着要请求的HTTP地址，主要是根据当前Topic的版本来决定请求服务端哪个版本的处理方法。除此之外还可以看到有通过Entity.json方法将PostSchemaPayload对象转换为HTTP请求的参数对象，转换逻辑是javax.ws.rs-api这个网络库封装的，就不进行跟踪了\n这里就是客户端最后发送的地方，request方法中还会发送前的安全相关检查，async方法基本上就说明本次HTTP请求是异步的，而post方法也能看得出，这是一个POST类型的HTTP请求，再往后就是将请求发送出去了\n不知是否有人好奇参数WebTarget长什么样子，通过通过调试可以看到值为\n/admin/v2/schemas/public/test-namespace-jytixthzgatgirem/test-multi-version-schema-one/schema\n此值仅供学习参考，具体这个值的构造逻辑如下\n小结 简单归纳如下\n通过Schema接口构造对应数据格式的schema对象，由此对象可得到schema相关的元信息SchemaInfo 构造请求目标的HTTP地址 通过javax.ws.rs-api提供的库发送异步HTTP请求到服务端 三、服务端源码解析 源码跟踪 服务端的接收逻辑在SchemasResource类，这个类在org.apache.pulsar.broker.admin包下，这个包下全是处理管理流相关的操作，如果有做pulsar平台化需求的，这个包下的相关逻辑值得一读。\n再来看看postSchema方法，首先是validateTopicName方法，这个方法就是对入参进行判空、是否有特殊字符做检查；接下来就是核心方法postSchemaAsync，通过方法名可以推断出这是个异步处理schema写请求的方法\npostSchemaAsync方法看似复杂，实际上核心的就是133行，其余的方法大概说一下，validateOwnershipAndOperationAsync方法主要检查当前用户是否有新建schema的操作权限，getSchemaCompatibilityStrategyAsyncWithoutAuth方法相对复杂一些，放到后面详细讲解。那么再看回133行，其中getSchemaRegistryService方法获取的是SchemaRegistryServiceImpl对象，顾名思义可以知道Pulsar的SchemaResistry相关的功能都是由它进行处理，现在先看它的putSchemaIfAbsent方法\nSchemaStorage对象是SchemaRegistryServiceImpl的核心成员，负责schema存储相关的操作。在新建schema时会调用它的put方法进行创建；这里有个trimDeletedSchemaAndGetList方法，如果put方法在创建schema时有任何异常，则此方法会去删除该新建的schema，避免写\u0026quot;一半\u0026quot;的情况发生，某种意义上这也是一种回滚的设计。\n这里的getAll方法很重要，会根据schema的id来查询是否已经存在当前schema，有的话则将版本号加1。处理完之后就调用put方法\n这里没什么逻辑，继续往下跟踪\ngetSchemaLocator方法会构造LocatorEntry对象，调用putSchema\n由于是初次创建schema，因此直接走到337行；如果这个topic已经创建过schema则会读取之前的schema信息再新增，同时把版本号自增\n在这里可以看得到构造IndexEntry对象，这是消息的索引对象，后续用来加速查询schema\n这个方法的内容就很眼熟了(bookkeeper相关内容)，createLedger方法会先创建这个Ledger\n在576行可以看到最终调用bookkeeper创建这个Ledger\n再来看看addEntry方法，这里核心也是调用bookkeeper的ledgerHandle进行数据写入\n这个方法是属于Bookkeeper客户端的逻辑了，通过方法注释可以看到，这个方法负责将数据异步写入到一个打开的Ledger。Bookkeeper相关的逻辑后续在单独写post进行讲解\n小结 简单归纳如下\n参数格式校验、操作权限校验 查询当前Topic是否已经创建过schema，有则以插入时版本号自增 如果是初次创建Schema，则调用bookkeeper创建Ledger 往这个Schema对应的Ledger内插入schema元数据信息 四、其他 序列化对象创建流程 现在再专门来看看序列化对象的创建过程，回到开头管理流创建schema的地方，Schema.AVRO方法是咱们本次要看的\n通过注释可以看到，此静态方法是创建一个Avro类型的schema对象，getDefaultImplementation方法是获取实现类(饿汉单例设计模式)，而newAvroSchema方法才是本次要看的\n继续往下跟踪\n获取对应处理的类加载器，并通过对应的类加载器创建AvroSchema实例\n54行是核心，其他的都是赋值操作\nsuper调用父类构造函数做赋值操作，还是继续看\n继续跟踪parse逻辑\nFACTORY.createParser方法是jackson的方法，用于创建JsonParser对象的；因此继续跟踪parse方法\n1471行可以看到返回了我们想要的Schema对象，那么Schema.parse方法就是重中之重\n这个方法是核心，本身会递归的进行解析赋值给schema对象\n相信读者读到这里也好奇schema长什么样，因此提供下图让读者感受下，能大概推测得出来这里已经涵盖了schema的结构信息了\ngetSchemaCompatibilityStrategyAsyncWithoutAuth方法 AdminResource#getSchemaCompatibilityStrategyAsyncWithoutAuth方法是在服务端处理schema创建请求阶段会调用的方法，现在就一起跟踪看看\n731行和739行分别是获取Topic级别和Namespace级别的schema兼容策略，如果没有定义则默认自动更新。例如Topic A之前已经创建过schema1，那么如果此时再发起schema2创建请求，则服务端会继续保存并且生效schema2，只不过它的版本号会进行累加，当然，也可以配置为不支持schema策略不支持更新，一旦确定了后就不允许再变更\n五、总结 相信大家对schema创建的流程已经很清楚了，再次简单归纳下\n客户端根据用户定义的结构信息创建对应的Schema对象，并将结构信息以HTTP请求发给服务端 服务端检测并根据Schema兼容策略做相对应的处理，一般情况下会调用Bookkeeper创建Ledger以及Entry Bookkeeper将此Schema数据持久化到磁盘，相当于Schema信息会被Bookkeeper当作一条消息进行存储 这基本上就是全部内容，当然细节感兴趣的小伙伴可以自行跟踪代码，相信你会有更多收获～\n","date":"2021-08-01T10:02:55+08:00","image":"https://sherlock-lin.github.io/p/%E7%AE%A1%E7%90%86%E6%B5%81%E5%88%9B%E5%BB%BAschema%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/image-20240801111546500_hu11517481045871944850.png","permalink":"https://sherlock-lin.github.io/p/%E7%AE%A1%E7%90%86%E6%B5%81%E5%88%9B%E5%BB%BAschema%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","title":"管理流创建schema流程源码解析"}]